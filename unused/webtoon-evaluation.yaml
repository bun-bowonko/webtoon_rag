# PIPELINE DEFINITION
# Name: inference
# Inputs:
#    base_model_name_or_path: str [Default: 'gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250417-merged']
#    batch_size: int [Default: 1.0]
#    output_table_name: str [Default: 'llama_sft_250417_evaluation_parsed']
#    project_id: str [Default: 'prod-ai-project']
components:
  comp-inference:
    executorLabel: exec-inference
    inputDefinitions:
      parameters:
        base_model_name_or_path:
          defaultValue: gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250417-merged
          isOptional: true
          parameterType: STRING
        batch_size:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        output_table_name:
          defaultValue: llama_sft_250417_evaluation_parsed
          isOptional: true
          parameterType: STRING
        project_id:
          defaultValue: prod-ai-project
          isOptional: true
          parameterType: STRING
deploymentSpec:
  executors:
    exec-inference:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - inference
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.10.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'transformers==4.51.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef inference(\n        project_id: str = \"prod-ai-project\",\n\
          \        base_model_name_or_path: str = \"gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250417-merged\"\
          ,\n        batch_size: int = 1,\n        output_table_name: str = \"llama_sft_250417_evaluation_parsed\"\
          ,\n    ):\n\n    from transformers import pipeline\n    from transformers\
          \ import BitsAndBytesConfig\n    from datasets import load_dataset, Dataset\n\
          \    from transformers.pipelines.pt_utils import KeyDataset\n    from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\n    import torch\n    import\
          \ subprocess\n    from google.cloud import bigquery\n    import os\n   \
          \ import shutil\n    import re\n    from tqdm import tqdm\n    tqdm.pandas()\n\
          \    import logging\n    client = bigquery.Client(project=project_id)\n\
          \    os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\
          \n    logging.basicConfig(format='%(asctime)s, %(levelname)-8s [%(pathname)s:%(lineno)d]\
          \ %(message)s',\n                        datefmt='%Y-%m-%d:%H:%M:%S',\n\
          \                        level=logging.WARNING)\n\n    if os.path.exists('base-model'):\n\
          \        shutil.rmtree('base-model')\n    os.mkdir('base-model')\n    #\
          \ base model\uC774 GCS\uC5D0 \uC800\uC7A5\uB418\uC5B4 \uC788\uB294 \uACBD\
          \uC6B0\n    if base_model_name_or_path.startswith('gs://'):\n        subprocess.run(f\"\
          gsutil -m cp {base_model_name_or_path}/* ./base-model/\", shell=True)\n\
          \    # base model\uC774 huggingface hub\uC5D0 \uC788\uB294 \uACBD\uC6B0\n\
          \    else:\n        subprocess.run(\"huggingface-cli login --token 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\"\
          , shell=True)\n        subprocess.run(\n            f'huggingface-cli download\
          \ {base_model_name_or_path} --local-dir-use-symlinks=False --local-dir=base-model\
          \ --include \"*.safetensors\" \"*.json\" $$',\n            shell=True)\n\
          \    print('** Finsihed Downloading Model Checkpoint ** ')\n\n    def parse_prediction(prompt,\
          \ prediction):\n        prompt_nums = []\n        for line in prompt.split('\\\
          n'):\n            line = line.strip()\n            if not re.findall('^[0-9]{3}',\
          \ line):\n                continue\n            prompt_nums.append(int(re.findall('^[0-9]{3}',\
          \ line)[0]))\n\n        prompt_max_num = max(prompt_nums)\n\n        eng_dict\
          \ = {i: '' for i in range(prompt_max_num + 1)}\n\n        for line in prediction.split('\\\
          n'):\n            line = line.strip()\n            if not re.findall('^[0-9]{3}',\
          \ line):\n                continue\n            key = int(re.findall('^[0-9]{3}',\
          \ line)[0])\n            value = re.sub('^[0-9]{3}', '', line).strip()\n\
          \            # value = re.sub(r\"\\[.*?\\]\", \"\", re.sub('^[0-9]{3}',\
          \ '', line).strip()).strip()\n            if key in eng_dict:\n        \
          \        eng_dict[key] = value\n\n        return list(eng_dict.values())\n\
          \n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n \
          \       bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n\
          \        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_storage=torch.bfloat16,\n\
          \    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        'base-model',\n\
          \        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n   \
          \     quantization_config=bnb_config,\n        attn_implementation=\"flash_attention_2\"\
          ,\n    )\n    model.eval()\n\n    tokenizer = AutoTokenizer.from_pretrained(\"\
          meta-llama/Llama-4-Scout-17B-16E-Instruct\", trust_remote_code=True)\n \
          \   tokenizer.pad_token = tokenizer.eos_token\n    # A decoder-only architecture\
          \ is being used, but right-padding was detected! For correct generation\
          \ results, please set `padding_side='left'` when initializing the tokenizer.\n\
          \    tokenizer.padding_side = \"left\"\n\n    def instruct_structure(prompt,\
          \ system_prompt=\"\"\"You're an expert translator who translates Korean\
          \ webtoon in English. Make sure the number of target sentences matches the\
          \ number of source sentences. The result should be TSV formatted. \n   \
          \ \u2022 Find a balance between staying true to the Korean meaning and keeping\
          \ a natural flow. Don't be afraid to add to the text. Embellish it. \n \
          \   \u2022 Avoid translating word-for-word. Keep the general feeling and\
          \ translate the text accordingly. \n    \u2022 Translate with an American\
          \ audience in mind. This means easy-to-read, conversational English.\"\"\
          \"):\n        input_text, output_text = prompt.split('### target')\n   \
          \     input_text = input_text.replace('### glossaries', '### glossary').replace('\\\
          n* ', '\\n\u2022 ')\n        input_text = re.sub(r\"\\[[^\\]]+\\] \", \"\
          none \", input_text)\n        return f\"\"\"<|begin_of_text|><|header_start|>system<|header_end|>\n\
          \n{system_prompt}<|eot|><|header_start|>user<|header_end|>\n\n{input_text.strip()}<|eot|><|header_start|>assistant<|header_end|>\"\
          \"\"\n\n    sql = \"\"\"\n          select series_id, episode_id, org_input_text,\
          \ org_output_text, prompt \n          from webtoon_translation.structured_240820_ep_line\n\
          \          where data_split = 'romance_valid'\n          \"\"\"\n    df\
          \ = client.query(sql).result().to_dataframe()\n    df['prompt'] = df.prompt.progress_apply(lambda\
          \ x: instruct_structure(x))\n    infer_dataset = Dataset.from_pandas(df[['prompt']],\
          \ split=\"test\")\n\n    # p00 parameter\uC640 \uB3D9\uC77C\uD568\n    pipe00\
          \ = pipeline(\n        \"text-generation\",\n        model=model,\n    \
          \    tokenizer=tokenizer,\n        max_new_tokens=4096,\n        do_sample=True,\n\
          \        temperature=0.1,\n        top_p=0.9,\n        torch_dtype=torch.bfloat16,\n\
          \        device_map=\"auto\",\n        top_k=30,\n        repetition_penalty=1.2,\n\
          \    )\n\n    predictions = []\n    cnt = 0\n    for result in tqdm(pipe00(KeyDataset(infer_dataset,\
          \ \"prompt\"), batch_size=batch_size, return_full_text=False)):\n      \
          \  logging.info(cnt)\n        pred = result[0]['generated_text'].lower()\n\
          \        predictions.append(pred)\n        if cnt == 0:\n            logging.info(pred)\n\
          \        cnt += 1\n\n    df['prediction'] = predictions\n    df['source']\
          \ = 'llama_p00'\n    df['model_path'] = base_model_name_or_path\n\n    df['parsed_prediction']\
          \ = df.progress_apply(lambda x: parse_prediction(x['prompt'].split('###\
          \ target')[0], x['prediction']), axis=1)\n\n    table_id = f'{project_id}.webtoon_translation.{output_table_name}'\n\
          \    job_config = bigquery.LoadJobConfig(write_disposition='WRITE_TRUNCATE')\n\
          \    job = client.load_table_from_dataframe(\n        df, table_id, job_config=job_config\n\
          \    )\n    job.result()\n\n"
        image: asia-northeast3-docker.pkg.dev/prod-ai-project/tmp-h100/llama3.1-base@sha256:8125b2d27a8382d644041b37610ca65f4fa7bf61d5461f5665b34e6f190ab966
pipelineInfo:
  name: inference
root:
  dag:
    tasks:
      inference:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-inference
        inputs:
          parameters:
            base_model_name_or_path:
              componentInputParameter: base_model_name_or_path
            batch_size:
              componentInputParameter: batch_size
            output_table_name:
              componentInputParameter: output_table_name
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: inference
  inputDefinitions:
    parameters:
      base_model_name_or_path:
        defaultValue: gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250417-merged
        isOptional: true
        parameterType: STRING
      batch_size:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      output_table_name:
        defaultValue: llama_sft_250417_evaluation_parsed
        isOptional: true
        parameterType: STRING
      project_id:
        defaultValue: prod-ai-project
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.10.1
