# PIPELINE DEFINITION
# Name: inference
# Inputs:
#    base_model_name_or_path: str [Default: 'meta-llama/Llama-4-Scout-17B-16E-Instruct']
#    lora_adapter_path: str [Default: 'gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250414']
#    output_table_name: str [Default: 'llama4_109b_sft_250414_evaluation_parsed']
#    project_id: str [Default: 'prod-ai-project']
components:
  comp-inference:
    executorLabel: exec-inference
    inputDefinitions:
      parameters:
        base_model_name_or_path:
          defaultValue: meta-llama/Llama-4-Scout-17B-16E-Instruct
          isOptional: true
          parameterType: STRING
        lora_adapter_path:
          defaultValue: gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250414
          isOptional: true
          parameterType: STRING
        output_table_name:
          defaultValue: llama4_109b_sft_250414_evaluation_parsed
          isOptional: true
          parameterType: STRING
        project_id:
          defaultValue: prod-ai-project
          isOptional: true
          parameterType: STRING
deploymentSpec:
  executors:
    exec-inference:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - inference
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.10.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'transformers==4.51.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef inference(\n        project_id: str = \"prod-ai-project\",\n\
          \        base_model_name_or_path: str = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\
          ,\n        lora_adapter_path : str = \"gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250414\"\
          ,\n        output_table_name: str = \"llama4_109b_sft_250414_evaluation_parsed\"\
          ,\n    ):\n\n    from transformers import pipeline\n    from transformers\
          \ import BitsAndBytesConfig\n    from datasets import load_dataset, Dataset\n\
          \    from transformers.pipelines.pt_utils import KeyDataset\n    from transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\n    from peft import PeftModel\n\
          \    import torch\n    import subprocess\n    from google.cloud import bigquery\n\
          \    import os\n    import shutil\n    import re\n    from tqdm import tqdm\n\
          \    tqdm.pandas()\n    import logging\n    client = bigquery.Client(project=project_id)\n\
          \    os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\
          \n    logging.basicConfig(format='%(asctime)s, %(levelname)-8s [%(pathname)s:%(lineno)d]\
          \ %(message)s',\n                        datefmt='%Y-%m-%d:%H:%M:%S',\n\
          \                        level=logging.WARNING)\n\n    if os.path.exists('base-model'):\n\
          \        shutil.rmtree('base-model')\n    os.mkdir('base-model')\n    #\
          \ base model\uC774 GCS\uC5D0 \uC800\uC7A5\uB418\uC5B4 \uC788\uB294 \uACBD\
          \uC6B0\n    if base_model_name_or_path.startswith('gs://'):\n        subprocess.run(f\"\
          gsutil -m cp {base_model_name_or_path}/* ./base-model/\", shell=True)\n\
          \    # base model\uC774 huggingface hub\uC5D0 \uC788\uB294 \uACBD\uC6B0\n\
          \    else:\n        subprocess.run(\"huggingface-cli login --token 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\"\
          , shell=True)\n        subprocess.run(\n            f'huggingface-cli download\
          \ {base_model_name_or_path} --local-dir-use-symlinks=False --local-dir=base-model\
          \ --include \"*.safetensors\" \"*.json\" $$',\n            shell=True)\n\
          \    subprocess.run(f\"gsutil -m cp {lora_adapter_path}/* ./base-model/\"\
          , shell=True)\n    print('** Finsihed Downloading Model Checkpoint ** ')\n\
          \n    def parse_prediction(prompt, prediction):\n        prompt_nums = []\n\
          \        for line in prompt.split('\\n'):\n            line = line.strip()\n\
          \            if not re.findall('^[0-9]{3}', line):\n                continue\n\
          \            prompt_nums.append(int(re.findall('^[0-9]{3}', line)[0]))\n\
          \n        prompt_max_num = max(prompt_nums)\n\n        eng_dict = {i: ''\
          \ for i in range(prompt_max_num + 1)}\n\n        for line in prediction.split('\\\
          n'):\n            line = line.strip()\n            if not re.findall('^[0-9]{3}',\
          \ line):\n                continue\n            key = int(re.findall('^[0-9]{3}',\
          \ line)[0])\n            value = re.sub('^[0-9]{3}', '', line).strip()\n\
          \            # value = re.sub(r\"\\[.*?\\]\", \"\", re.sub('^[0-9]{3}',\
          \ '', line).strip()).strip()\n            if key in eng_dict:\n        \
          \        eng_dict[key] = value\n\n        return list(eng_dict.values())\n\
          \n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n \
          \       bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n\
          \        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_storage=torch.bfloat16,\n\
          \    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        'base-model',\n\
          \        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n   \
          \     quantization_config=bnb_config,\n        attn_implementation=\"flash_attention_2\"\
          ,\n    )\n    #lora \uBAA8\uB4C8 \uC801\uC6A9\n    model = PeftModel.from_pretrained(model,\
          \ 'base-model')\n    model.eval()\n\n    def print_gpu_memory():\n     \
          \   for i in range(torch.cuda.device_count()):\n            allocated =\
          \ torch.cuda.memory_allocated(i) / (1024 ** 3)\n            reserved = torch.cuda.memory_reserved(i)\
          \ / (1024 ** 3)\n            print(f\"GPU {i}: Allocated = {allocated:.2f}\
          \ GB, Reserved = {reserved:.2f} GB\")\n\n    # \uBAA8\uB378 \uB85C\uB529\
          \ \uC9C1\uD6C4 \uD638\uCD9C\n    print_gpu_memory()\n\n    # \uAE30\uC874\
          \ tokenizer\uB97C \uB798\uD551\n    class MaxLengthTokenizerWrapper:\n \
          \       def __init__(self, tokenizer, max_length):\n            self._tokenizer\
          \ = tokenizer\n            self.max_length = max_length\n\n        def pad(self,\
          \ *args, **kwargs):\n            kwargs[\"padding\"] = \"max_length\"\n\
          \            kwargs[\"max_length\"] = self.max_length\n            return\
          \ self._tokenizer.pad(*args, **kwargs)\n\n        def __getattr__(self,\
          \ name):\n            return getattr(self._tokenizer, name)\n\n        def\
          \ __call__(self, *args, **kwargs):\n            return self._tokenizer(*args,\
          \ **kwargs)\n\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\
          , trust_remote_code=True)\n    # A decoder-only architecture is being used,\
          \ but right-padding was detected! For correct generation results, please\
          \ set `padding_side='left'` when initializing the tokenizer.\n    tokenizer.padding_side\
          \ = \"left\"\n    #tokenizer = MaxLengthTokenizerWrapper(tokenizer, max_length=4096)\n\
          \n    def instruct_structure(prompt, system_prompt=\"\"\"You're an expert\
          \ translator who translates Korean webtoon in English. Make sure the number\
          \ of target sentences matches the number of source sentences. The result\
          \ should be TSV formatted. \n    \u2022 Find a balance between staying true\
          \ to the Korean meaning and keeping a natural flow. Don't be afraid to add\
          \ to the text. Embellish it. \n    \u2022 Avoid translating word-for-word.\
          \ Keep the general feeling and translate the text accordingly. \n    \u2022\
          \ Translate with an American audience in mind. This means easy-to-read,\
          \ conversational English.\"\"\"):\n        input_text, output_text = prompt.split('###\
          \ target')\n        input_text = input_text.replace('### glossaries', '###\
          \ glossary').replace('\\n* ', '\\n\u2022 ')\n        input_text = re.sub(r\"\
          \\[[^\\]]+\\] \", \"none \", input_text)\n        return f\"\"\"<|begin_of_text|><|header_start|>system<|header_end|>\n\
          \n{system_prompt}<|eot|><|header_start|>user<|header_end|>\n\n{input_text.strip()}<|eot|><|header_start|>assistant<|header_end|>\"\
          \"\"\n\n    sql = \"\"\"\n          select series_id, episode_id, org_input_text,\
          \ org_output_text, prompt \n          from webtoon_translation.structured_240820_ep_line\n\
          \          where data_split = 'romance_valid'\n          \"\"\"\n    df\
          \ = client.query(sql).result().to_dataframe()\n    df['prompt'] = df.prompt.progress_apply(lambda\
          \ x: instruct_structure(x))\n    infer_dataset = Dataset.from_pandas(df[['prompt']],\
          \ split=\"test\")\n\n    # \uC0DD\uC131 \uD30C\uB77C\uBBF8\uD130\n    generation_args\
          \ = {\n        \"max_new_tokens\": 4097,\n        \"do_sample\": True,\n\
          \        \"temperature\": 0.1,\n        \"top_p\": 0.9,\n        \"top_k\"\
          : 30,\n        \"repetition_penalty\": 1.2,\n    }\n\n    predictions =\
          \ []\n    cnt = 0\n    for item in tqdm(infer_dataset):\n        prompt\
          \ = item[\"prompt\"]\n\n        # \uC778\uCF54\uB529\n        inputs = tokenizer(\n\
          \            prompt,\n            return_tensors=\"pt\",\n            padding=\"\
          max_length\",\n            truncation=True,\n            max_length=4096,\n\
          \        ).to(model.device)\n\n\n        # \uC0DD\uC131\n        with torch.no_grad():\n\
          \            outputs = model.generate(\n                **inputs,\n    \
          \            **generation_args,\n                eos_token_id=tokenizer.convert_tokens_to_ids(\"\
          <|eot|>\")\n            )\n\n        # \uCCAB \uBC88\uC9F8 \uCD9C\uB825\uB9CC\
          \ \uCD94\uCD9C\uD558\uACE0 \uB514\uCF54\uB529\n        generated_text =\
          \ tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=False)\n\
          \        predictions.append(generated_text.lower())\n\n        logging.info(cnt)\n\
          \        if cnt == 0:\n            logging.info(generated_text)\n      \
          \  cnt += 1\n\n    df['prediction'] = predictions\n    df['model_path']\
          \ = base_model_name_or_path\n\n    df['parsed_prediction'] = df.progress_apply(lambda\
          \ x: parse_prediction(x['prompt'].split('### target')[0], x['prediction']),\
          \ axis=1)\n\n    table_id = f'{project_id}.webtoon_translation.{output_table_name}'\n\
          \    job_config = bigquery.LoadJobConfig(write_disposition='WRITE_TRUNCATE')\n\
          \    job = client.load_table_from_dataframe(\n        df, table_id, job_config=job_config\n\
          \    )\n    job.result()\n\n"
        image: asia-northeast3-docker.pkg.dev/prod-ai-project/tmp-h100/llama3.1-base@sha256:8125b2d27a8382d644041b37610ca65f4fa7bf61d5461f5665b34e6f190ab966
pipelineInfo:
  name: inference
root:
  dag:
    tasks:
      inference:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-inference
        inputs:
          parameters:
            base_model_name_or_path:
              componentInputParameter: base_model_name_or_path
            lora_adapter_path:
              componentInputParameter: lora_adapter_path
            output_table_name:
              componentInputParameter: output_table_name
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: inference
  inputDefinitions:
    parameters:
      base_model_name_or_path:
        defaultValue: meta-llama/Llama-4-Scout-17B-16E-Instruct
        isOptional: true
        parameterType: STRING
      lora_adapter_path:
        defaultValue: gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250414
        isOptional: true
        parameterType: STRING
      output_table_name:
        defaultValue: llama4_109b_sft_250414_evaluation_parsed
        isOptional: true
        parameterType: STRING
      project_id:
        defaultValue: prod-ai-project
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.10.1
