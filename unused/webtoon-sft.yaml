# PIPELINE DEFINITION
# Name: train
# Inputs:
#    base_model_name_or_path: str [Default: 'meta-llama/Llama-4-Scout-17B-16E-Instruct']
#    bf16: bool [Default: True]
#    data_sql: str [Default: 'where data_split in ('train') and create_date = (select max(create_date) from webtoon_translation.sft_dataset)']
#    eval_steps: int [Default: 1000000000000.0]
#    gcs_sft_output_dir: str [Default: 'gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250417']
#    gradient_accumulation_steps: int [Default: 1.0]
#    gradient_checkpointing: bool [Default: True]
#    learning_rate: float [Default: 5e-05]
#    logging_steps: int [Default: 100.0]
#    lora_alpha: int [Default: 16.0]
#    lora_dropout: float [Default: 0.0]
#    lora_r: int [Default: 32.0]
#    lr_scheduler_type: str [Default: 'cosine']
#    max_seq_length: int [Default: 8192.0]
#    max_steps: int [Default: -1.0]
#    num_train_epochs: int [Default: 3.0]
#    output_dir: str [Default: './sft']
#    per_device_eval_batch_size: int [Default: 1.0]
#    per_device_train_batch_size: int [Default: 1.0]
#    project_id: str [Default: 'prod-ai-project']
#    remove_unused_columns: bool [Default: True]
#    report_to: str [Default: 'wandb']
#    run_name: str [Default: 'sft_webtoon_250417_llama4_109b']
#    save_steps: int [Default: 1000000000000.0]
#    warmup_ratio: float [Default: 0.01]
#    weight_decay: float [Default: 0.05]
components:
  comp-train:
    executorLabel: exec-train
    inputDefinitions:
      parameters:
        base_model_name_or_path:
          defaultValue: meta-llama/Llama-4-Scout-17B-16E-Instruct
          isOptional: true
          parameterType: STRING
        bf16:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        data_sql:
          defaultValue: where data_split in ('train') and create_date = (select max(create_date)
            from webtoon_translation.sft_dataset)
          isOptional: true
          parameterType: STRING
        eval_steps:
          defaultValue: 1000000000000.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        gcs_sft_output_dir:
          defaultValue: gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250417
          isOptional: true
          parameterType: STRING
        gradient_accumulation_steps:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        gradient_checkpointing:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        learning_rate:
          defaultValue: 5.0e-05
          isOptional: true
          parameterType: NUMBER_DOUBLE
        logging_steps:
          defaultValue: 100.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        lora_alpha:
          defaultValue: 16.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        lora_dropout:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        lora_r:
          defaultValue: 32.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        lr_scheduler_type:
          defaultValue: cosine
          isOptional: true
          parameterType: STRING
        max_seq_length:
          defaultValue: 8192.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        max_steps:
          defaultValue: -1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        num_train_epochs:
          defaultValue: 3.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        output_dir:
          defaultValue: ./sft
          isOptional: true
          parameterType: STRING
        per_device_eval_batch_size:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        per_device_train_batch_size:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        project_id:
          defaultValue: prod-ai-project
          isOptional: true
          parameterType: STRING
        remove_unused_columns:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        report_to:
          defaultValue: wandb
          isOptional: true
          parameterType: STRING
        run_name:
          defaultValue: sft_webtoon_250417_llama4_109b
          isOptional: true
          parameterType: STRING
        save_steps:
          defaultValue: 1000000000000.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        warmup_ratio:
          defaultValue: 0.01
          isOptional: true
          parameterType: NUMBER_DOUBLE
        weight_decay:
          defaultValue: 0.05
          isOptional: true
          parameterType: NUMBER_DOUBLE
deploymentSpec:
  executors:
    exec-train:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.10.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'transformers==4.51.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train(project_id: str = \"prod-ai-project\",\n        # base_model_name_or_path:\
          \ str = \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n        # data_sql:\
          \ str = \"where data_split in ('train')\",\n        base_model_name_or_path:\
          \ str = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n        data_sql:\
          \ str = \"where data_split in ('train') and create_date = (select max(create_date)\
          \ from webtoon_translation.sft_dataset)\",\n        max_seq_length: int\
          \ = 8192,\n        lora_alpha: int = 16,\n        lora_dropout: float =\
          \ 0,\n        lora_r: int = 32,\n        output_dir: str = \"./sft\",\n\
          \        max_steps: int = -1,\n        num_train_epochs: int = 3,\n    \
          \    logging_steps: int = 100,\n        eval_steps: int = 1000000000000,\
          \ # skip evaluation\n        save_steps: int = 1000000000000, # skip saving\n\
          \        per_device_train_batch_size: int = 1,\n        per_device_eval_batch_size:\
          \ int = 1,\n        gradient_accumulation_steps: int = 1,\n        gradient_checkpointing:\
          \ bool = True,\n        learning_rate: float = 5e-5,\n        lr_scheduler_type:\
          \ str = \"cosine\",\n        warmup_ratio: float = 0.01,\n        weight_decay:\
          \ float = 0.05,\n        bf16: bool = True,\n        remove_unused_columns:\
          \ bool = True,\n        run_name: str = \"sft_webtoon_250417_llama4_109b\"\
          ,\n        report_to: str = \"wandb\",\n        gcs_sft_output_dir: str\
          \ = \"gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250417\"\
          ,):\n\n\n    import os\n    import subprocess\n    import torch\n    import\
          \ torch.nn as nn\n    from transformers.activations import ACT2FN\n    from\
          \ accelerate import Accelerator, init_empty_weights, infer_auto_device_map\n\
          \    from datasets import load_dataset, Dataset\n    from peft import AutoPeftModelForCausalLM,\
          \ LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n\
          \    from peft.tuners.lora import Linear as LoRALinear\n    from tqdm import\
          \ tqdm\n    tqdm.pandas()\n    import pandas as pd\n    from transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser,\
          \ \\\n        TrainingArguments, AutoConfig\n    from transformers.models.llama4.modeling_llama4\
          \ import Llama4TextMoe\n    from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n\
          \    from google.cloud import bigquery\n    import gcsfs\n    import shutil\n\
          \    import wandb\n    import glob\n    import gc\n\n    wandb.login(key='6f5b18e94c20bee59a84b92ca785d1a3acd0f06a')\n\
          \n    import torch\n    import torch.nn as nn\n    from transformers.activations\
          \ import ACT2FN\n\n    class Llama4TextExpertsWithLinear(nn.Module):\n \
          \       def __init__(self, config):\n            super().__init__()\n  \
          \          self.num_experts = config.num_local_experts\n            self.intermediate_size\
          \ = config.intermediate_size\n            self.hidden_size = config.hidden_size\n\
          \            self.expert_dim = self.intermediate_size\n\n            self.gate_up_proj\
          \ = nn.ModuleList([\n                nn.Linear(self.hidden_size, 2 * self.expert_dim,\
          \ bias=False)\n                for _ in range(self.num_experts)\n      \
          \      ])\n            self.down_proj = nn.ModuleList([\n              \
          \  nn.Linear(self.expert_dim, self.hidden_size, bias=False)\n          \
          \      for _ in range(self.num_experts)\n            ])\n\n            self.act_fn\
          \ = ACT2FN[config.hidden_act]\n        #nn.Linear(in_features, out_features)\uC5D0\
          \uC11C weight shape\uC740 (out_features, in_features)\uC774\uBBC0\uB85C\
          : T \uD574\uC57C\uD568\n        def load_from_param_tensor(self, gate_up_tensor,\
          \ down_tensor):\n            print(f\"[Before cleanup Allocated]: {torch.cuda.memory_allocated()\
          \ / 1024**2:.4f} MB\")\n            print(f\"[Before cleanup Reserved] {torch.cuda.memory_reserved()\
          \ / 1024**2:.4f} MB\")\n            for i in range(self.num_experts):\n\
          \                # \uBA3C\uC800 Linear \uB808\uC774\uC5B4\uC758 weight\uC640\
          \ bias\uB97C bfloat16\uC73C\uB85C \uAC15\uC81C \uBCC0\uD658\n          \
          \      self.gate_up_proj[i] = self.gate_up_proj[i].to(dtype=torch.bfloat16)\n\
          \                self.down_proj[i] = self.down_proj[i].to(dtype=torch.bfloat16)\n\
          \n                # \uB808\uC774\uC5B4 \uC790\uCCB4\uB97C \uBCF5\uC0AC \uB300\
          \uC0C1 \uD150\uC11C\uC758 \uB514\uBC14\uC774\uC2A4\uB85C \uC774\uB3D9\n\
          \                gate_up_tensor_device = gate_up_tensor[i].device\n    \
          \            down_tensor_device = down_tensor[i].device\n              \
          \  self.gate_up_proj[i] = self.gate_up_proj[i].to(gate_up_tensor_device)\n\
          \                self.gate_up_proj[i].weight = self.gate_up_proj[i].weight.to(gate_up_tensor_device)\n\
          \n                self.down_proj[i] = self.down_proj[i].to(down_tensor_device)\n\
          \                self.down_proj[i].weight = self.down_proj[i].weight.to(down_tensor_device)\n\
          \n                # \uD30C\uB77C\uBBF8\uD130 \uBCF5\uC0AC (\uC774\uBBF8\
          \ \uAC19\uC740 \uB514\uBC14\uC774\uC2A4\uC5D0 \uC788\uC74C)\n          \
          \      self.gate_up_proj[i].weight.data.copy_(\n                    gate_up_tensor[i].T.contiguous()\n\
          \                )\n                self.down_proj[i].weight.data.copy_(\n\
          \                    down_tensor[i].T.contiguous()\n                )\n\n\
          \n            del gate_up_tensor\n            del down_tensor\n        \
          \    gc.collect()\n            torch.cuda.empty_cache()\n\n            print(f\"\
          [After cleanup Allocated]: {torch.cuda.memory_allocated() / 1024**2:.4f}\
          \ MB\")\n            print(f\"[After cleanup Reserved]: {torch.cuda.memory_reserved()\
          \ / 1024**2:.4f} MB\")\n\n\n\n        def forward(self, hidden_states: torch.Tensor)\
          \ -> torch.Tensor:\n            hidden_states = hidden_states.view(self.num_experts,\
          \ -1, self.hidden_size)\n            outputs = []\n            for i in\
          \ range(self.num_experts):\n                local_input = hidden_states[i].to(self.gate_up_proj[i].weight.device)\n\
          \n                gate_up = self.gate_up_proj[i](local_input)\n        \
          \        gate, up = gate_up.chunk(2, dim=-1)\n                out = self.down_proj[i](up\
          \ * self.act_fn(gate))\n                outputs.append(out)\n          \
          \  outputs = [out.to(\"cuda:0\") for out in outputs]  # \uB610\uB294 cat\
          \ \uD558\uAE30 \uC804\uC5D0 \uD558\uB098\uC758 device\uB85C \uBAA8\uC74C\
          , \uC548 \uADF8\uB7EC\uBA74 \uD130\uC9D0\n            next_states = torch.cat(outputs,\
          \ dim=0)\n            return next_states\n\n    print(f'::: torch.cuda.device_count()\
          \ {torch.cuda.device_count()}:::')\n\n    client = bigquery.Client(project=project_id)\n\
          \    fs = gcsfs.GCSFileSystem(project=project_id)\n    auth_token = \"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\"\
          \n    #pytorch memory segmentation \uBC29\uC9C0\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"\
          ] = \"expandable_segments:False,max_split_size_mb:1024\"\n    os.environ[\"\
          HUGGING_FACE_HUB_TOKEN\"] = auth_token\n\n\n    if os.path.exists('base-model2'):\n\
          \        shutil.rmtree('base-model2')\n    os.mkdir('base-model2')\n   \
          \ # base model\uC774 GCS\uC5D0 \uC800\uC7A5\uB418\uC5B4 \uC788\uB294 \uACBD\
          \uC6B0\n    if base_model_name_or_path.startswith('gs://'):\n        subprocess.run(f\"\
          gsutil -m cp {base_model_name_or_path}/* ./base-model2/\", shell=True)\n\
          \    # base model\uC774 huggingface hub\uC5D0 \uC788\uB294 \uACBD\uC6B0\n\
          \    else:\n        subprocess.run(\"huggingface-cli login --token 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\"\
          , shell=True)\n        subprocess.run(\n            f'huggingface-cli download\
          \ {base_model_name_or_path} --local-dir-use-symlinks=False --local-dir=base-model2\
          \ --include \"*.safetensors\" \"*.json\" $$',\n            shell=True)\n\
          \    print('** Finsihed Downloading Model Checkpoint ** ')\n\n    peft_config\
          \ = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n   \
          \     lora_dropout=lora_dropout,\n        target_modules=[\n           \
          \ \"q_proj\", \"v_proj\", \"o_proj\", \"k_proj\",\n            \"shared_expert.up_proj\"\
          , \"shared_expert.down_proj\", \"shared_expert.gate_proj\", \"router\"]+\n\
          \        [f\"experts.gate_up_proj.{i}\" for i in range(16)]+\n        [f\"\
          experts.down_proj.{i}\" for i in range(16)],\n        bias=\"none\",\n \
          \       task_type=\"CAUSAL_LM\",\n    )\n\n\n    bnb_config = BitsAndBytesConfig(\n\
          \        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n    \
          \    bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n\
          \        bnb_4bit_quant_storage=torch.bfloat16,#torch.float32\n    )\n\n\
          \    # device_map = {}\n    # num_layers = 48\n    # layers_per_gpu = num_layers\
          \ // 8\n\n    # for i in range(8):\n    #     start = i * layers_per_gpu\n\
          \    #     end = (i + 1) * layers_per_gpu\n    #     for j in range(start,\
          \ end):\n    #         device_map[f\"model.layers.{j}\"] = i\n\n    # device_map.update({\n\
          \    #     \"model.embed_tokens\": 0,\n    #     \"model.norm\": 7,\n  \
          \  #     \"model.rotary_emb\": 7,\n    #     \"lm_head\": 7,\n    # })\n\
          \n    #print(device_map)\n    # \u2705 \uADF8\uB9AC\uACE0 \uB098\uC11C auto\
          \ device mapping\n    from accelerate import infer_auto_device_map\n   \
          \ from accelerate.utils import get_balanced_memory\n\n\n    # 2. \uBE48\
          \ \uBAA8\uB378 \uB85C\uB529 (\uBA54\uBAA8\uB9AC \uB0AD\uBE44 \uC5C6\uC774\
          \ \uAD6C\uC870\uB9CC \uB85C\uB4DC\uB428)\n    # with init_empty_weights():\n\
          \    #     model = AutoModelForCausalLM.from_pretrained(\n    #     'base-model2',\n\
          \    #     torch_dtype=torch.bfloat16,\n    #     attn_implementation=\"\
          flash_attention_2\",\n    #     trust_remote_code=True,\n    # )\n    #\
          \ # 3. device_map \uCD94\uB860\n    # max_memory=get_balanced_memory(\n\
          \    #             model,\n    #             no_split_module_classes=[\"\
          LlamaDecoderLayer\"],\n    #             dtype=torch.bfloat16,\n    #  \
          \       )\n    # # \u2705 95%\uB9CC \uC4F0\uB3C4\uB85D \uBE44\uC728 \uC870\
          \uC815\n    # max_memory = {gpu_id: int(mem * 0.95) for gpu_id, mem in max_memory.items()}\n\
          \n\n    # device_map = infer_auto_device_map(\n    #     model,\n    # \
          \    max_memory=max_memory,\n    #     no_split_module_classes=[\"LlamaDecoderLayer\"\
          ],\n    #     dtype=torch.bfloat16,\n    # )\n\n    # print(device_map)\n\
          \n    model = AutoModelForCausalLM.from_pretrained(\n        'base-model2',\n\
          \        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",#device_map,\n\
          \        attn_implementation=\"flash_attention_2\",\n        quantization_config=bnb_config,\n\
          \        trust_remote_code=True,\n    )\n\n    print(f\"\uD30C\uB77C\uBBF8\
          \uD130 \u2705\uCCB4\uD06C\u2705 : \",model.hf_device_map)    \n\n    for\
          \ layer in model.model.layers:\n        if not isinstance(layer.feed_forward,\
          \ Llama4TextMoe):\n            continue  # MoE\uAC00 \uC544\uB2CC \uACBD\
          \uC6B0 skip\n\n        old_expert = layer.feed_forward.experts\n\n     \
          \   # \uC0C8\uB85C\uC6B4 Linear \uAE30\uBC18 Expert \uBAA8\uB4C8 \uC0DD\uC131\
          \n        new_expert = Llama4TextExpertsWithLinear(model.config)\n\n   \
          \     # nn.Linear\uC758 weight\uB9CC \uCD94\uCD9C\uD574\uC11C \uB118\uACA8\
          \uC90C\n        #device = next(layer.feed_forward.parameters()).device \
          \ # \uB204\uB77D\uB41C `.device`\n        gate_up_weights = [proj.clone().detach().to(proj.device)\
          \ for proj in old_expert.gate_up_proj]\n        down_weights = [proj.clone().detach().to(proj.device)\
          \ for proj in old_expert.down_proj]\n        #\uC704 \uCF54\uB4DC\uB791\
          \ \uAC19\uC740 \uCF54\uB4DC\n        # gate_up_weights = [proj.clone().detach()\
          \ for proj in old_expert.gate_up_proj]\n        # down_weights = [proj.clone().detach()\
          \ for proj in old_expert.down_proj]\n\n\n        # weight \uBCF5\uC0AC &\
          \ free\n        new_expert.load_from_param_tensor(\n            gate_up_tensor=gate_up_weights,\n\
          \            down_tensor=down_weights,\n        )\n\n        # Expert \uBAA8\
          \uB4C8 \uAD50\uCCB4\n        layer.feed_forward.experts = new_expert\n\n\
          \        print(\"\uD30C\uB77C\uBBF8\uD130 \u2705\uCCB4\uD06C\u2705\")\n\
          \        for name, param in layer.named_parameters():\n            print(f\"\
          {name}: {param.shape}, device={param.device}, dtype={param.dtype}\")\n\n\
          \    print(\"\u2705\u2705\u2705 Llama4TextExperts \uACC4\uCE35\uC774 Llama4TextExpertsWithLinear\uC73C\
          \uB85C \uC131\uACF5\uC801\uC73C\uB85C \uAD50\uCCB4\uB418\uC5C8\uC2B5\uB2C8\
          \uB2E4!\")\n\n    # tmp = []\n    # expert_num = 16\n    # for i in range(expert_num):\n\
          \    #     tmp.append(f\"experts.gate_up_proj.{i}\")\n    #     tmp.append(f\"\
          experts.down_proj.{i}\")\n    # peft_config.target_modules.update(tmp)\n\
          \n\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model,\
          \ peft_config)  # LoRA \uB808\uC774\uC5B4 \uC0DD\uC131\uB428!\n\n\n    model.print_trainable_parameters()\n\
          \n    def print_gpu_memory():\n        for i in range(torch.cuda.device_count()):\n\
          \            allocated = torch.cuda.memory_allocated(i) / (1024 ** 3)\n\
          \            reserved = torch.cuda.memory_reserved(i) / (1024 ** 3)\n  \
          \          print(f\"GPU {i}: Allocated = {allocated:.2f} GB, Reserved =\
          \ {reserved:.2f} GB\")\n\n    # \uBAA8\uB378 \uB85C\uB529 \uC9C1\uD6C4 \uD638\
          \uCD9C\n    print_gpu_memory()\n\n    #moe bug \uD68C\uD53C\uC6A9 (monkey\
          \ patch)\n    orig_scatter_add = torch.Tensor.scatter_add_\n    def safe_scatter_add_(self,\
          \ dim, index, src):\n        if self.dtype != src.dtype:\n            src\
          \ = src.to(self.dtype)\n        return orig_scatter_add(self, dim, index,\
          \ src)\n\n    torch.Tensor.scatter_add_ = safe_scatter_add_\n\n\n    # prevent\
          \ ```RuntimeError: element 0 of tensors does not require grad and does not\
          \ have a grad_fn```\n    model.enable_input_require_grads()\n\n\n    # \uAE30\
          \uC874 tokenizer\uB97C \uB798\uD551\n    class MaxLengthTokenizerWrapper:\n\
          \        def __init__(self, tokenizer, max_length):\n            self._tokenizer\
          \ = tokenizer\n            self.max_length = max_length\n\n        def pad(self,\
          \ *args, **kwargs):\n            kwargs[\"padding\"] = \"max_length\"\n\
          \            kwargs[\"max_length\"] = self.max_length\n            return\
          \ self._tokenizer.pad(*args, **kwargs)\n\n        def __getattr__(self,\
          \ name):\n            return getattr(self._tokenizer, name)\n\n        def\
          \ __call__(self, *args, **kwargs):\n            return self._tokenizer(*args,\
          \ **kwargs)\n\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\
          , trust_remote_code=True)\n    tokenizer.pad_token = tokenizer.eos_token\
          \ #\uBE7C\uC57C\uB428, llama4\uB294 pad\uD1A0\uD070 \uAC00\uC9C0\uACE0 \uC788\
          \uC74C -> \uADF8\uB798\uB3C4 \uC0C1\uAD00\uC5C6\uC74C\n    tokenizer.padding_side\
          \ = \"right\"  # Fix weird overflow issue with fp16 training\n    #tokenizer\
          \ = MaxLengthTokenizerWrapper(tokenizer, max_length=max_seq_length)\n\n\n\
          \    def instruct_structure(prompt, system_prompt=\"\"\"You're an expert\
          \ translator who translates Korean webtoon in English. Make sure the number\
          \ of target sentences matches the number of source sentences. The result\
          \ should be TSV formatted. \n    \u2022 Find a balance between staying true\
          \ to the Korean meaning and keeping a natural flow. Don't be afraid to add\
          \ to the text. Embellish it. \n    \u2022 Avoid translating word-for-word.\
          \ Keep the general feeling and translate the text accordingly. \n    \u2022\
          \ Translate with an American audience in mind. This means easy-to-read,\
          \ conversational English.\"\"\"):\n        input_text, output_text = prompt.split('###\
          \ target')\n        input_text = input_text.replace('### glossaries', '###\
          \ glossary').replace('\\n* ', '\\n\u2022 ')\n        return f\"\"\"<|begin_of_text|><|header_start|>system<|header_end|>\n\
          \n{system_prompt}<|eot|><|header_start|>user<|header_end|>\n\n{input_text.strip()}<|eot|><|header_start|>assistant<|header_end|>\n\
          \n{output_text.strip()}<|eot|>\"\"\"\n\n    train_sql = f\"\"\"        \
          \  \n              select prompt\n              from webtoon_translation.sft_dataset\n\
          \              {data_sql}\n              \"\"\"\n    train_df = client.query(train_sql).result().to_dataframe()\n\
          \    train_df['text'] = train_df.prompt.progress_apply(lambda x: instruct_structure(x))\n\
          \n    train_dataset = Dataset.from_pandas(train_df[['text']])\n\n    #print(':::\
          \ Dataset Example :::')\n    #print(train_dataset[0])\n\n    '''\n    eval_sql\
          \ = \"\"\"          \n              select prompt\n              from webtoon_translation.sft_dataset\n\
          \              where data_split in ('valid')\n              \"\"\"\n   \
          \ eval_df = client.query(eval_sql).result().to_dataframe()\n    eval_df['text']\
          \ = eval_df.prompt.progress_apply(lambda x: instruct_structure(x))\n   \
          \ eval_dataset = Dataset.from_pandas(eval_df[['text']])\n    '''\n\n   \
          \ def formatting_prompts_func(example):\n        output_texts = []\n   \
          \     for i in range(len(example['text'])):\n            output_texts.append(example['text'][i])\n\
          \        return output_texts\n\n    response_template_with_context = '<|header_start|>assistant<|header_end|>'\n\
          \    response_template_ids = tokenizer.encode(response_template_with_context,\
          \ add_special_tokens=False)\n    collator = DataCollatorForCompletionOnlyLM(response_template_ids,\
          \ \n                                               tokenizer=tokenizer,)\n\
          \n\n    training_args = SFTConfig(\n        output_dir=output_dir,\n   \
          \     max_steps=max_steps,\n        max_seq_length=max_seq_length,\n   \
          \     num_train_epochs=num_train_epochs,\n        logging_steps=logging_steps,\n\
          \        # eval_steps=eval_steps,\n        save_steps=save_steps,\n    \
          \    # evaluation_strategy='steps',\n        per_device_train_batch_size=per_device_train_batch_size,\n\
          \        # per_device_eval_batch_size=per_device_eval_batch_size,\n    \
          \    gradient_accumulation_steps=gradient_accumulation_steps,\n        #\
          \ eval_accumulation_steps=gradient_accumulation_steps,\n        gradient_checkpointing=gradient_checkpointing,\n\
          \        learning_rate=learning_rate,\n        lr_scheduler_type=lr_scheduler_type,\n\
          \        warmup_ratio=warmup_ratio,\n        weight_decay=weight_decay,\n\
          \        bf16=bf16,\n        remove_unused_columns=remove_unused_columns,\n\
          \        run_name=run_name,\n        report_to=report_to,\n        ddp_find_unused_parameters=False,\
          \ # RuntimeError: Expected to mark a variable ready only once.\n    )\n\n\
          \    #optimizer.train() \uD638\uCD9C \uBB34\uC2DC\uD558\uAE30 (Monkey Patch\
          \ \uBC29\uC2DD)\n    # \uAE30\uC874 optimizer\uC778 AdamW\uC5D0 train \uBA54\
          \uC11C\uB4DC\uAC00 \uC5C6\uC73C\uBA74 \uB9CC\uB4E4\uC5B4\uC90C\n    if not\
          \ hasattr(torch.optim.AdamW, \"train\"):\n        def train_stub(self):\n\
          \            # \uADF8\uB0E5 \uC544\uBB34 \uAC83\uB3C4 \uC548 \uD568\n  \
          \          pass\n    torch.optim.AdamW.train = train_stub\n\n\n    class\
          \ SafeSFTTrainer(SFTTrainer):\n        def compute_loss(self, model, inputs,\
          \ return_outputs=False, **kwargs):\n            device = model.device\n\
          \            num_items_in_batch = kwargs.get(\"num_items_in_batch\", None)\n\
          \n            # inputs \uB0B4 tensor\uB4E4\uC744 \uC62C\uBC14\uB978 \uB514\
          \uBC14\uC774\uC2A4\uB85C \uC774\uB3D9\n            inputs = {\n        \
          \        k: v.to(device) if isinstance(v, torch.Tensor) else v\n       \
          \         for k, v in inputs.items()\n            }\n\n            outputs\
          \ = model(**inputs)\n\n            if isinstance(outputs, tuple):\n    \
          \            loss = outputs[0]\n            else:\n                loss\
          \ = outputs.loss if hasattr(outputs, \"loss\") else outputs[\"loss\"]\n\n\
          \            if num_items_in_batch is not None:\n                if isinstance(num_items_in_batch,\
          \ torch.Tensor):\n                    num_items_in_batch = num_items_in_batch.to(loss.device)\n\
          \                loss = loss / num_items_in_batch\n\n            return\
          \ (loss, outputs) if return_outputs else loss\n\n    trainer = SafeSFTTrainer(\n\
          \        model=model,\n        train_dataset=train_dataset,\n        # eval_dataset=eval_dataset,\n\
          \        formatting_func=formatting_prompts_func,\n        data_collator=collator,\n\
          \        #peft_config=peft_config,\n        tokenizer=tokenizer,\n     \
          \   packing=False,\n        args=training_args,\n    )\n\n    trainer.train()\n\
          \    trainer.save_model(output_dir)\n    output_dir = os.path.join(output_dir,\
          \ \"final_checkpoint\")\n    trainer.model.save_pretrained(output_dir)\n\
          \n    # save finetuned model to GCS\n    # \uB85C\uCEEC \uB514\uB809\uD1A0\
          \uB9AC\uC758 \uBAA8\uB4E0 \uD30C\uC77C \uAC00\uC838\uC624\uAE30\n    local_files\
          \ = glob.glob(os.path.join(output_dir, '*'))\n    # \uD30C\uC77C \uD558\uB098\
          \uC529 \uC5C5\uB85C\uB4DC\n    for local_file in local_files:\n        filename\
          \ = os.path.basename(local_file)\n        gcs_path = f\"{gcs_sft_output_dir}/{filename}\"\
          \n        fs.put(local_file, gcs_path)\n        print(f\"Uploaded {local_file}\
          \ to {gcs_path}\")\n\n"
        image: asia-northeast3-docker.pkg.dev/prod-ai-project/tmp-h100/llama3.1-base@sha256:8125b2d27a8382d644041b37610ca65f4fa7bf61d5461f5665b34e6f190ab966
pipelineInfo:
  name: train
root:
  dag:
    tasks:
      train:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train
        inputs:
          parameters:
            base_model_name_or_path:
              componentInputParameter: base_model_name_or_path
            bf16:
              componentInputParameter: bf16
            data_sql:
              componentInputParameter: data_sql
            eval_steps:
              componentInputParameter: eval_steps
            gcs_sft_output_dir:
              componentInputParameter: gcs_sft_output_dir
            gradient_accumulation_steps:
              componentInputParameter: gradient_accumulation_steps
            gradient_checkpointing:
              componentInputParameter: gradient_checkpointing
            learning_rate:
              componentInputParameter: learning_rate
            logging_steps:
              componentInputParameter: logging_steps
            lora_alpha:
              componentInputParameter: lora_alpha
            lora_dropout:
              componentInputParameter: lora_dropout
            lora_r:
              componentInputParameter: lora_r
            lr_scheduler_type:
              componentInputParameter: lr_scheduler_type
            max_seq_length:
              componentInputParameter: max_seq_length
            max_steps:
              componentInputParameter: max_steps
            num_train_epochs:
              componentInputParameter: num_train_epochs
            output_dir:
              componentInputParameter: output_dir
            per_device_eval_batch_size:
              componentInputParameter: per_device_eval_batch_size
            per_device_train_batch_size:
              componentInputParameter: per_device_train_batch_size
            project_id:
              componentInputParameter: project_id
            remove_unused_columns:
              componentInputParameter: remove_unused_columns
            report_to:
              componentInputParameter: report_to
            run_name:
              componentInputParameter: run_name
            save_steps:
              componentInputParameter: save_steps
            warmup_ratio:
              componentInputParameter: warmup_ratio
            weight_decay:
              componentInputParameter: weight_decay
        taskInfo:
          name: train
  inputDefinitions:
    parameters:
      base_model_name_or_path:
        defaultValue: meta-llama/Llama-4-Scout-17B-16E-Instruct
        isOptional: true
        parameterType: STRING
      bf16:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      data_sql:
        defaultValue: where data_split in ('train') and create_date = (select max(create_date)
          from webtoon_translation.sft_dataset)
        isOptional: true
        parameterType: STRING
      eval_steps:
        defaultValue: 1000000000000.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      gcs_sft_output_dir:
        defaultValue: gs://kakao-entertainment-cel-applied-ai-prod/bun/llama4/llama4_109b/sft-webtoon-250417
        isOptional: true
        parameterType: STRING
      gradient_accumulation_steps:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      gradient_checkpointing:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      learning_rate:
        defaultValue: 5.0e-05
        isOptional: true
        parameterType: NUMBER_DOUBLE
      logging_steps:
        defaultValue: 100.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      lora_alpha:
        defaultValue: 16.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      lora_dropout:
        defaultValue: 0.0
        isOptional: true
        parameterType: NUMBER_DOUBLE
      lora_r:
        defaultValue: 32.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      lr_scheduler_type:
        defaultValue: cosine
        isOptional: true
        parameterType: STRING
      max_seq_length:
        defaultValue: 8192.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      max_steps:
        defaultValue: -1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      num_train_epochs:
        defaultValue: 3.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      output_dir:
        defaultValue: ./sft
        isOptional: true
        parameterType: STRING
      per_device_eval_batch_size:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      per_device_train_batch_size:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      project_id:
        defaultValue: prod-ai-project
        isOptional: true
        parameterType: STRING
      remove_unused_columns:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      report_to:
        defaultValue: wandb
        isOptional: true
        parameterType: STRING
      run_name:
        defaultValue: sft_webtoon_250417_llama4_109b
        isOptional: true
        parameterType: STRING
      save_steps:
        defaultValue: 1000000000000.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      warmup_ratio:
        defaultValue: 0.01
        isOptional: true
        parameterType: NUMBER_DOUBLE
      weight_decay:
        defaultValue: 0.05
        isOptional: true
        parameterType: NUMBER_DOUBLE
schemaVersion: 2.1.0
sdkVersion: kfp-2.10.1
