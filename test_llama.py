import torch
from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoConfig
from accelerate import Accelerator, infer_auto_device_map
import csv
LLAMA_PATH = 'model/dpo-240820-ep-line-merged'

# 4bit í€€íƒ€ì´ì œì´ì…˜ ì„¤ì •
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_storage=torch.bfloat16,
        llm_int8_enable_fp32_cpu_offload=True  # CPU ì˜¤í”„ë¡œë”© í™œì„±í™”
)
# ê° GPUì— ìµœëŒ€ ë©”ëª¨ë¦¬ ì„¤ì • (ì˜ˆì‹œë¡œ 40GBì”© í• ë‹¹)
max_memory = {i: "40GB" for i in range(torch.cuda.device_count())}

# ëª¨ë¸ì˜ ì„¤ì •ì„ ë¨¼ì € ë¶ˆëŸ¬ì˜¤ê¸°
config = AutoConfig.from_pretrained(LLAMA_PATH)

from transformers.modeling_utils import init_empty_weights  # âœ… ì˜¬ë°”ë¥¸ import

# ì•„ì§ ëª¨ë¸ì„ ì„ ì–¸í•˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ `config`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹ˆ ëª¨ë¸ ìƒì„±
# ğŸš€ ê°€ì¤‘ì¹˜ë¥¼ ì „í˜€ ë¡œë“œí•˜ì§€ ì•ŠëŠ” ì™„ì „ ë¹ˆ(empty) ëª¨ë¸ ìƒì„±
with init_empty_weights():
    empty_model = LlamaForCausalLM(config)
print("ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ")

num_gpus = torch.cuda.device_count()

# **ğŸš€ GPU 8ëŒ€ì— ê· ë“±í•˜ê²Œ ë¶„ë°°í•˜ëŠ” ìˆ˜ë™ device_map ìƒì„±**
device_map = {}

# ì„ë² ë”© & ì¶œë ¥ ë ˆì´ì–´ëŠ” GPU 0ì— ë°°ì¹˜
device_map["model.embed_tokens"] = 0
device_map["lm_head"] = 1

# 126ê°œ LlamaDecoderLayerë¥¼ 8ê°œì˜ GPUì— ê· ë“± ë¶„ë°°
for i, layer in enumerate(empty_model.model.layers):
    assigned_gpu = i % num_gpus  # GPU ì¸ë±ìŠ¤ 0ë¶€í„° 7ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ í• ë‹¹
    device_map[f"model.layers.{i}"] = assigned_gpu

# ë§ˆì§€ë§‰ RMSNormë„ ë§ˆì§€ë§‰ GPUë¡œ ë°°ì¹˜
device_map["model.norm"] = 7
device_map["model.rotary_emb"] = 7  # Rotary Embeddingë„ ë§ˆì§€ë§‰ GPUë¡œ

# ëª¨ë¸ ë¡œë“œ (`device_map` ì ìš©)
model = LlamaForCausalLM.from_pretrained(
    LLAMA_PATH,
    quantization_config=bnb_config,
    device_map=device_map,  # ìë™ ë¶„ë°°ëœ device_map ì ìš©
    attn_implementation="flash_attention_2"
)

from transformers import AutoTokenizer

TOKENIZER_PATH = 'model/dpo-240820-ep-line-merged'

# Tokenizer ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)
tokenizer.pad_token = tokenizer.eos_token
# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
tokenizer.padding_side = "left"

import re
def instruct_structure(prompt,system_prompt=
                       """You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. 
    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. 
    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. 
    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English."""):
    input_text, output_text = prompt.split('### target')
    input_text = input_text.replace('### glossaries', '### glossary').replace('\n* ', '\nâ€¢ ')
    input_text = re.sub(r"\[[^\]]+\] ", "[UNK] ", input_text)
    return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>
{input_text.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>"""

project_id = "prod-ai-project"

from google.cloud import bigquery
client = bigquery.Client(project=project_id)
sql = """select series_id, episode_id, org_input_text, org_output_text, prompt 
        from webtoon_translation.structured_240820_ep_line
        where data_split = 'romance_valid'"""
df = client.query(sql).result().to_dataframe()
from tqdm import tqdm
tqdm.pandas()
df['prompt'] = df['prompt'].progress_apply(lambda x: instruct_structure(x))

def clean_text(text):
        return re.sub(r'^\d+\s+\[UNK\]\s+', '', text)
def clean_text2(text):
    return re.sub(r'^\d+\s+','', text)

for data_idx in range(74):
    sample = df['prompt'][data_idx]
    
    # ì…ë ¥ ë³€í™˜ ë° í† í°í™”
    inputs = tokenizer(sample, return_tensors="pt").to(model.device)
    import time
    start_time = time.time()
    # ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        output = model.generate(**inputs, 
                                max_new_tokens=1000,#ì…ë ¥ ê¸¸ì´ì™€ ë¬´ê´€í•˜ê²Œ ì¶œë ¥ê¸¸ì´ë§Œ ì œí•œ
                                do_sample=True,
                                temperature=0.1,
                                top_p=0.9,
                                top_k=30,
                                repetition_penalty=1.2,
                                use_cache=True #kv cache
                               )
    
    
    
    pre = sample.split('### source')[1].strip()
    pre_li = [clean_text(d) for d in pre.split('\n')]
    pre_li[-1] = pre_li[-1].split('<|eot_id|>')[0]
    pre_len = len(pre_li)
    
    # ê²°ê³¼ ì¶œë ¥
    response = tokenizer.decode(output[0], skip_special_tokens=False)
    result = response.split('assistant<|end_header_id|>')[1].strip()
    result_li = [clean_text2(d) for d in result.split('\n')]
    result_li = result_li[:pre_len]
    result_li[-1]= result_li[-1].split('<|')[0]
    
    with open(f"data/llama_result/llama_{data_idx}.csv", mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(['hangle', 'first_translate'])
        for row in zip(pre_li, result_li):
            writer.writerow(row)    
    
    print(response)
    print(f"{time.time()-start_time:.2f}")