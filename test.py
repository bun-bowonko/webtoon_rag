import torch
from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoConfig
from accelerate import Accelerator, infer_auto_device_map

LLAMA_PATH = 'model/llama_405b_quantized'

# 4bit í€€íƒ€ì´ì œì´ì…˜ ì„¤ì •
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_storage=torch.bfloat16,
        llm_int8_enable_fp32_cpu_offload=True  # CPU ì˜¤í”„ë¡œë”© í™œì„±í™”
)
# ê° GPUì— ìµœëŒ€ ë©”ëª¨ë¦¬ ì„¤ì • (ì˜ˆì‹œë¡œ 40GBì”© í• ë‹¹)
max_memory = {i: "40GB" for i in range(torch.cuda.device_count())}

# ëª¨ë¸ì˜ ì„¤ì •ì„ ë¨¼ì € ë¶ˆëŸ¬ì˜¤ê¸°
config = AutoConfig.from_pretrained(LLAMA_PATH)

from transformers.modeling_utils import init_empty_weights  # âœ… ì˜¬ë°”ë¥¸ import

# ì•„ì§ ëª¨ë¸ì„ ì„ ì–¸í•˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ `config`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹ˆ ëª¨ë¸ ìƒì„±
# ğŸš€ ê°€ì¤‘ì¹˜ë¥¼ ì „í˜€ ë¡œë“œí•˜ì§€ ì•ŠëŠ” ì™„ì „ ë¹ˆ(empty) ëª¨ë¸ ìƒì„±
with init_empty_weights():
    empty_model = LlamaForCausalLM(config)
print("ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ")


num_gpus = torch.cuda.device_count()

# **ğŸš€ GPU 8ëŒ€ì— ê· ë“±í•˜ê²Œ ë¶„ë°°í•˜ëŠ” ìˆ˜ë™ device_map ìƒì„±**
device_map = {}

# ì„ë² ë”© & ì¶œë ¥ ë ˆì´ì–´ëŠ” GPU 0ì— ë°°ì¹˜
device_map["model.embed_tokens"] = 0
device_map["lm_head"] = 1

# 126ê°œ LlamaDecoderLayerë¥¼ 8ê°œì˜ GPUì— ê· ë“± ë¶„ë°°
for i, layer in enumerate(empty_model.model.layers):
    assigned_gpu = i % num_gpus  # GPU ì¸ë±ìŠ¤ 0ë¶€í„° 7ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ í• ë‹¹
    device_map[f"model.layers.{i}"] = assigned_gpu

# ë§ˆì§€ë§‰ RMSNormë„ ë§ˆì§€ë§‰ GPUë¡œ ë°°ì¹˜
device_map["model.norm"] = 2
device_map["model.rotary_emb"] = 3  # Rotary Embeddingë„ ë§ˆì§€ë§‰ GPUë¡œ

# ëª¨ë¸ ë¡œë“œ (`device_map` ì ìš©)
model = LlamaForCausalLM.from_pretrained(
    LLAMA_PATH,
    quantization_config=bnb_config,
    device_map=device_map,  # ìë™ ë¶„ë°°ëœ device_map ì ìš©
    #attn_implementation="flash_attention_2"
)

from transformers import AutoTokenizer

TOKENIZER_PATH = 'model/llama_405b_quantized'

# Tokenizer ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)
tokenizer.pad_token = tokenizer.eos_token
# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
tokenizer.padding_side = "left"

#tokenizer.save_pretrained('model/llama_405b_quantized')

import re
def instruct_structure(prompt,system_prompt=
                       """You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. 
    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. 
    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. 
    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English."""):
    input_text, output_text = prompt.split('### target')
    input_text = input_text.replace('### glossaries', '### glossary').replace('\n* ', '\nâ€¢ ')
    input_text = re.sub(r"\[[^\]]+\] ", "none ", input_text)
    return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
    {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>
    {input_text.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>"""


project_id = "prod-ai-project"

from google.cloud import bigquery
client = bigquery.Client(project=project_id)
sql = """select series_id, episode_id, org_input_text, org_output_text, prompt 
        from webtoon_translation.structured_240820_ep_line
        where data_split = 'romance_valid'"""
df = client.query(sql).result().to_dataframe()
from tqdm import tqdm
tqdm.pandas()
df['prompt'] = df['prompt'].progress_apply(lambda x: instruct_structure(x))

sample = df['prompt'][0]

# ì…ë ¥ ë³€í™˜ ë° í† í°í™”
inputs = tokenizer(sample, return_tensors="pt").to("cuda")
print(inputs)


# ëª¨ë¸ ì¶”ë¡ 
with torch.no_grad():
    output = model.generate(**inputs, 
                            max_length=1000,
                            repetition_penalty=1.2
                           )

# ê²°ê³¼ ì¶œë ¥
response = tokenizer.decode(output[0], skip_special_tokens=False)
print(response)
