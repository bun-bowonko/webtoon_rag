{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e819127c-2a20-446d-a5fd-5a0b22e3899b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 00:28:04 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 00:28:05,145\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becc3aa5-2c16-4537-b5a9-94021bce61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face 계정에서 받은 API 토큰을 입력\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50824994-3d0a-4bc8-ba83-ecafe5b33a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 00:28:40 config.py:542] This model supports multiple tasks: {'generate', 'classify', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 03-25 00:28:40 config.py:621] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 03-25 00:28:40 config.py:1401] Defaulting to use mp for distributed inference\n",
      "INFO 03-25 00:28:41 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='meta-llama/Llama-3.1-405B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-405B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=30000, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-405B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 03-25 00:28:46 utils.py:2152] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information.\n",
      "WARNING 03-25 00:28:46 multiproc_worker_utils.py:300] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-25 00:28:46 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 03-25 00:28:46 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 03-25 00:28:53 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 03-25 00:28:53 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 03-25 00:28:53 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 03-25 00:28:53 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 03-25 00:28:53 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 03-25 00:28:53 __init__.py:190] Automatically detected platform cuda.\n",
      "INFO 03-25 00:28:53 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 00:28:53 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 00:28:54 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 00:28:54 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 00:28:54 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 00:28:54 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 00:28:54 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 00:28:54 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 00:28:55 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 00:28:55 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 00:28:55 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 00:28:55 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 00:28:55 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 00:28:55 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 00:28:55 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 03-25 00:28:57 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 03-25 00:28:57 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 00:28:57 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 00:28:57 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 00:28:57 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 00:28:57 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 00:28:57 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 00:28:57 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 00:28:57 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 00:28:57 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 00:28:57 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 00:28:57 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 00:28:57 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 00:28:57 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 00:28:57 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 00:28:57 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 03-25 00:29:00 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/bun.2/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 00:29:00 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/bun.2/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 00:29:00 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/bun.2/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 00:29:00 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/bun.2/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 00:29:00 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/bun.2/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 00:29:00 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/bun.2/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 00:29:00 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/bun.2/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 00:29:00 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/bun.2/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 03-25 00:29:00 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_e2bbbe46'), local_subscribe_port=57563, remote_subscribe_port=None)\n",
      "INFO 03-25 00:29:00 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-405B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 00:29:00 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-405B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 00:29:00 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-405B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 00:29:00 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-405B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 00:29:00 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-405B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 00:29:00 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-405B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 00:29:00 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-405B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 00:29:00 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-405B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 00:29:01 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 00:29:01 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 00:29:01 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 00:29:01 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 03-25 00:29:01 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 00:29:01 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 00:29:01 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 00:29:01 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 00:29:02 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 00:29:02 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 03-25 00:29:02 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 00:29:02 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 00:29:02 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 00:29:02 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 00:29:02 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 00:29:02 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/191 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   1% Completed | 1/191 [00:14<47:25, 14.97s/it]\n",
      "Loading safetensors checkpoint shards:   1% Completed | 2/191 [00:30<48:03, 15.26s/it]\n",
      "Loading safetensors checkpoint shards:   2% Completed | 3/191 [00:45<47:56, 15.30s/it]\n",
      "Loading safetensors checkpoint shards:   2% Completed | 4/191 [01:03<50:04, 16.07s/it]\n",
      "Loading safetensors checkpoint shards:   3% Completed | 5/191 [01:20<51:56, 16.75s/it]\n",
      "Loading safetensors checkpoint shards:   3% Completed | 6/191 [01:38<52:22, 16.99s/it]\n",
      "Loading safetensors checkpoint shards:   4% Completed | 7/191 [01:39<36:12, 11.81s/it]\n",
      "Loading safetensors checkpoint shards:   4% Completed | 8/191 [01:40<25:24,  8.33s/it]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 9/191 [01:41<18:26,  6.08s/it]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 10/191 [01:58<28:37,  9.49s/it]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 11/191 [01:59<20:30,  6.83s/it]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 12/191 [02:00<14:58,  5.02s/it]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 13/191 [02:12<21:01,  7.09s/it]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 14/191 [02:13<15:36,  5.29s/it]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 15/191 [02:14<11:51,  4.04s/it]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 16/191 [02:15<09:17,  3.18s/it]\n",
      "Loading safetensors checkpoint shards:   9% Completed | 17/191 [02:34<23:10,  7.99s/it]\n",
      "Loading safetensors checkpoint shards:   9% Completed | 18/191 [02:52<31:26, 10.90s/it]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 19/191 [03:05<33:13, 11.59s/it]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 20/191 [03:07<24:14,  8.50s/it]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 21/191 [03:24<31:27, 11.10s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 22/191 [03:43<38:05, 13.52s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 23/191 [03:55<36:54, 13.18s/it]\n",
      "Loading safetensors checkpoint shards:  13% Completed | 24/191 [04:13<40:38, 14.60s/it]\n",
      "Loading safetensors checkpoint shards:  13% Completed | 25/191 [04:33<44:21, 16.03s/it]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 26/191 [04:45<41:09, 14.97s/it]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 27/191 [05:03<43:35, 15.95s/it]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 28/191 [05:22<45:19, 16.68s/it]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 29/191 [05:39<45:16, 16.77s/it]\n",
      "Loading safetensors checkpoint shards:  16% Completed | 30/191 [05:55<44:30, 16.59s/it]\n",
      "Loading safetensors checkpoint shards:  16% Completed | 31/191 [06:12<44:36, 16.73s/it]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 32/191 [06:28<43:55, 16.58s/it]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 33/191 [06:45<44:13, 16.80s/it]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 34/191 [07:04<45:07, 17.25s/it]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 35/191 [07:20<43:55, 16.89s/it]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 36/191 [07:34<41:19, 16.00s/it]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 37/191 [07:46<38:29, 15.00s/it]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 38/191 [07:47<27:36, 10.83s/it]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 39/191 [08:02<29:59, 11.84s/it]\n",
      "Loading safetensors checkpoint shards:  21% Completed | 40/191 [08:09<26:42, 10.61s/it]\n",
      "Loading safetensors checkpoint shards:  21% Completed | 41/191 [08:26<31:10, 12.47s/it]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 42/191 [08:43<34:29, 13.89s/it]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 43/191 [09:02<37:32, 15.22s/it]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 44/191 [09:17<37:41, 15.38s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 45/191 [09:32<36:30, 15.01s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 46/191 [09:47<36:43, 15.20s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 47/191 [09:56<31:47, 13.24s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 48/191 [10:03<27:23, 11.49s/it]\n",
      "Loading safetensors checkpoint shards:  26% Completed | 49/191 [10:14<26:38, 11.26s/it]\n",
      "Loading safetensors checkpoint shards:  26% Completed | 50/191 [10:32<31:30, 13.41s/it]\n",
      "Loading safetensors checkpoint shards:  27% Completed | 51/191 [10:45<30:57, 13.27s/it]\n",
      "Loading safetensors checkpoint shards:  27% Completed | 52/191 [10:53<26:26, 11.41s/it]\n",
      "Loading safetensors checkpoint shards:  28% Completed | 53/191 [11:09<30:02, 13.07s/it]\n",
      "Loading safetensors checkpoint shards:  28% Completed | 54/191 [11:17<26:23, 11.56s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 55/191 [11:31<27:13, 12.01s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 56/191 [11:48<30:55, 13.75s/it]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 57/191 [11:57<27:31, 12.33s/it]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 58/191 [12:15<31:07, 14.04s/it]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 59/191 [12:31<32:05, 14.59s/it]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 60/191 [12:37<25:53, 11.86s/it]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 61/191 [12:53<28:38, 13.22s/it]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 62/191 [13:08<29:17, 13.63s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 63/191 [13:22<29:29, 13.82s/it]\n",
      "Loading safetensors checkpoint shards:  34% Completed | 64/191 [13:36<29:24, 13.90s/it]\n",
      "Loading safetensors checkpoint shards:  34% Completed | 65/191 [13:53<30:57, 14.74s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 66/191 [14:02<27:30, 13.20s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 67/191 [14:20<29:42, 14.38s/it]\n",
      "Loading safetensors checkpoint shards:  36% Completed | 68/191 [14:30<26:48, 13.08s/it]\n",
      "Loading safetensors checkpoint shards:  36% Completed | 69/191 [14:47<29:29, 14.50s/it]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 70/191 [15:02<29:05, 14.43s/it]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 71/191 [15:16<28:55, 14.46s/it]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 72/191 [15:26<25:53, 13.05s/it]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 73/191 [15:31<20:44, 10.55s/it]\n",
      "Loading safetensors checkpoint shards:  39% Completed | 74/191 [15:45<22:54, 11.75s/it]\n",
      "Loading safetensors checkpoint shards:  39% Completed | 75/191 [15:51<19:14,  9.95s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 76/191 [16:08<23:22, 12.19s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 77/191 [16:19<22:03, 11.61s/it]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 78/191 [16:32<22:42, 12.06s/it]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 79/191 [16:44<22:40, 12.14s/it]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 80/191 [17:00<24:16, 13.12s/it]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 81/191 [17:13<24:24, 13.31s/it]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 82/191 [17:23<22:19, 12.29s/it]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 83/191 [17:38<23:15, 12.92s/it]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 84/191 [17:53<24:36, 13.80s/it]\n",
      "Loading safetensors checkpoint shards:  45% Completed | 85/191 [18:06<23:58, 13.57s/it]\n",
      "Loading safetensors checkpoint shards:  45% Completed | 86/191 [18:17<22:20, 12.77s/it]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 87/191 [18:25<19:42, 11.37s/it]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 88/191 [18:32<16:47,  9.78s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 89/191 [18:42<16:44,  9.85s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 90/191 [18:57<19:12, 11.41s/it]\n",
      "Loading safetensors checkpoint shards:  48% Completed | 91/191 [19:08<18:48, 11.29s/it]\n",
      "Loading safetensors checkpoint shards:  48% Completed | 92/191 [19:18<18:20, 11.12s/it]\n",
      "Loading safetensors checkpoint shards:  49% Completed | 93/191 [19:31<19:06, 11.69s/it]\n",
      "Loading safetensors checkpoint shards:  49% Completed | 94/191 [19:42<18:14, 11.28s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 95/191 [19:53<18:06, 11.32s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 96/191 [20:08<19:39, 12.42s/it]\n",
      "Loading safetensors checkpoint shards:  51% Completed | 97/191 [20:19<18:37, 11.89s/it]\n",
      "Loading safetensors checkpoint shards:  51% Completed | 98/191 [20:29<17:51, 11.53s/it]\n",
      "Loading safetensors checkpoint shards:  52% Completed | 99/191 [20:45<19:43, 12.87s/it]\n",
      "Loading safetensors checkpoint shards:  52% Completed | 100/191 [21:01<20:56, 13.81s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 101/191 [21:09<18:05, 12.06s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 102/191 [21:25<19:17, 13.01s/it]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 103/191 [21:40<19:55, 13.59s/it]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 104/191 [21:49<18:05, 12.48s/it]\n",
      "Loading safetensors checkpoint shards:  55% Completed | 105/191 [22:07<19:53, 13.88s/it]\n",
      "Loading safetensors checkpoint shards:  55% Completed | 106/191 [22:23<20:55, 14.77s/it]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 107/191 [22:39<20:59, 14.99s/it]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 108/191 [22:54<20:42, 14.97s/it]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 109/191 [23:09<20:33, 15.04s/it]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 110/191 [23:21<19:03, 14.11s/it]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 111/191 [23:31<17:19, 13.00s/it]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 112/191 [23:42<15:58, 12.14s/it]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 113/191 [23:57<17:02, 13.11s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 114/191 [24:12<17:40, 13.78s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 115/191 [24:26<17:18, 13.67s/it]\n",
      "Loading safetensors checkpoint shards:  61% Completed | 116/191 [24:35<15:28, 12.38s/it]\n",
      "Loading safetensors checkpoint shards:  61% Completed | 117/191 [24:51<16:45, 13.59s/it]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 118/191 [25:03<15:50, 13.02s/it]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 119/191 [25:15<15:08, 12.62s/it]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 120/191 [25:27<14:40, 12.41s/it]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 121/191 [25:38<13:59, 11.99s/it]\n",
      "Loading safetensors checkpoint shards:  64% Completed | 122/191 [25:52<14:33, 12.66s/it]\n",
      "Loading safetensors checkpoint shards:  64% Completed | 123/191 [26:02<13:24, 11.82s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 124/191 [26:12<12:39, 11.33s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 125/191 [26:25<12:54, 11.74s/it]\n",
      "Loading safetensors checkpoint shards:  66% Completed | 126/191 [26:39<13:24, 12.38s/it]\n",
      "Loading safetensors checkpoint shards:  66% Completed | 127/191 [26:53<13:47, 12.92s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 128/191 [27:05<13:30, 12.86s/it]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 129/191 [27:20<13:53, 13.44s/it]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 130/191 [27:36<14:15, 14.03s/it]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 131/191 [27:49<13:50, 13.84s/it]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 132/191 [28:02<13:14, 13.47s/it]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 133/191 [28:12<12:01, 12.43s/it]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 134/191 [28:26<12:12, 12.85s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 135/191 [28:35<11:06, 11.91s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 136/191 [28:45<10:22, 11.33s/it]\n",
      "Loading safetensors checkpoint shards:  72% Completed | 137/191 [29:03<11:52, 13.20s/it]\n",
      "Loading safetensors checkpoint shards:  72% Completed | 138/191 [29:19<12:20, 13.98s/it]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 139/191 [29:31<11:49, 13.64s/it]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 140/191 [29:49<12:29, 14.70s/it]\n",
      "Loading safetensors checkpoint shards:  74% Completed | 141/191 [30:02<12:02, 14.45s/it]\n",
      "Loading safetensors checkpoint shards:  74% Completed | 142/191 [30:18<12:01, 14.73s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 143/191 [30:27<10:31, 13.15s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 144/191 [30:43<10:54, 13.92s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 145/191 [30:58<10:48, 14.09s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 146/191 [31:12<10:42, 14.27s/it]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 147/191 [31:25<10:08, 13.84s/it]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 148/191 [31:36<09:17, 12.97s/it]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 149/191 [31:50<09:21, 13.36s/it]\n",
      "Loading safetensors checkpoint shards:  79% Completed | 150/191 [32:06<09:39, 14.14s/it]\n",
      "Loading safetensors checkpoint shards:  79% Completed | 151/191 [32:21<09:29, 14.24s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 152/191 [32:36<09:24, 14.48s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 153/191 [32:49<08:53, 14.04s/it]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 154/191 [33:04<08:54, 14.45s/it]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 155/191 [33:18<08:38, 14.40s/it]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 156/191 [33:35<08:44, 14.99s/it]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 157/191 [33:50<08:33, 15.10s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 158/191 [34:02<07:49, 14.23s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 159/191 [34:18<07:51, 14.73s/it]\n",
      "Loading safetensors checkpoint shards:  84% Completed | 160/191 [34:34<07:49, 15.16s/it]\n",
      "Loading safetensors checkpoint shards:  84% Completed | 161/191 [34:45<06:51, 13.72s/it]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 162/191 [35:00<06:55, 14.31s/it]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 163/191 [35:14<06:30, 13.96s/it]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 164/191 [35:24<05:47, 12.88s/it]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 165/191 [35:37<05:34, 12.86s/it]\n",
      "Loading safetensors checkpoint shards:  87% Completed | 166/191 [35:47<04:58, 11.93s/it]\n",
      "Loading safetensors checkpoint shards:  87% Completed | 167/191 [35:57<04:38, 11.60s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 168/191 [36:10<04:31, 11.81s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 169/191 [36:20<04:08, 11.30s/it]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 170/191 [36:37<04:35, 13.12s/it]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 171/191 [36:49<04:12, 12.62s/it]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 172/191 [36:58<03:40, 11.59s/it]\n",
      "Loading safetensors checkpoint shards:  91% Completed | 173/191 [37:08<03:22, 11.26s/it]\n",
      "Loading safetensors checkpoint shards:  91% Completed | 174/191 [37:20<03:12, 11.34s/it]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 175/191 [37:37<03:29, 13.08s/it]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 176/191 [37:50<03:13, 12.93s/it]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 177/191 [38:07<03:18, 14.16s/it]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 178/191 [38:22<03:10, 14.65s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 179/191 [38:38<03:00, 15.00s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 180/191 [38:54<02:46, 15.16s/it]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 181/191 [39:07<02:27, 14.75s/it]\n",
      "Loading safetensors checkpoint shards:  95% Completed | 182/191 [39:16<01:56, 12.93s/it]\n",
      "Loading safetensors checkpoint shards:  96% Completed | 183/191 [39:29<01:42, 12.86s/it]\n",
      "Loading safetensors checkpoint shards:  96% Completed | 184/191 [39:43<01:32, 13.16s/it]\n",
      "Loading safetensors checkpoint shards:  97% Completed | 185/191 [39:53<01:13, 12.32s/it]\n",
      "Loading safetensors checkpoint shards:  97% Completed | 186/191 [40:03<00:58, 11.73s/it]\n",
      "Loading safetensors checkpoint shards:  98% Completed | 187/191 [40:19<00:51, 12.93s/it]\n",
      "Loading safetensors checkpoint shards:  98% Completed | 188/191 [40:33<00:39, 13.23s/it]\n",
      "Loading safetensors checkpoint shards:  99% Completed | 189/191 [40:46<00:26, 13.19s/it]\n",
      "Loading safetensors checkpoint shards:  99% Completed | 190/191 [41:02<00:14, 14.12s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 191/191 [41:15<00:00, 13.60s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 191/191 [41:15<00:00, 12.96s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 01:38:25 model_runner.py:1115] Loading model weights took 25.1436 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 01:38:26 model_runner.py:1115] Loading model weights took 25.1436 GB\n",
      "INFO 03-25 01:38:27 model_runner.py:1115] Loading model weights took 25.1436 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 01:38:26 model_runner.py:1115] Loading model weights took 25.1436 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 01:38:26 model_runner.py:1115] Loading model weights took 25.1436 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 01:38:26 model_runner.py:1115] Loading model weights took 25.1436 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 01:38:26 model_runner.py:1115] Loading model weights took 25.1436 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 01:38:26 model_runner.py:1115] Loading model weights took 25.1436 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] Memory profiling takes 19.81 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.99) = 39.18GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] model weights take 25.14GiB; non_torch_memory takes 2.10GiB; PyTorch activation peak memory takes 5.16GiB; the rest of the memory reserved for KV Cache is 6.77GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] Memory profiling takes 19.84 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.99) = 39.18GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] model weights take 25.14GiB; non_torch_memory takes 2.10GiB; PyTorch activation peak memory takes 5.16GiB; the rest of the memory reserved for KV Cache is 6.77GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] Memory profiling takes 19.84 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.99) = 39.18GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] model weights take 25.14GiB; non_torch_memory takes 2.10GiB; PyTorch activation peak memory takes 5.16GiB; the rest of the memory reserved for KV Cache is 6.77GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] Memory profiling takes 19.84 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.99) = 39.18GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] model weights take 25.14GiB; non_torch_memory takes 2.10GiB; PyTorch activation peak memory takes 5.16GiB; the rest of the memory reserved for KV Cache is 6.77GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] Memory profiling takes 19.85 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.99) = 39.18GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] model weights take 25.14GiB; non_torch_memory takes 1.82GiB; PyTorch activation peak memory takes 5.16GiB; the rest of the memory reserved for KV Cache is 7.06GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] Memory profiling takes 19.82 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.99) = 39.18GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] model weights take 25.14GiB; non_torch_memory takes 2.10GiB; PyTorch activation peak memory takes 5.16GiB; the rest of the memory reserved for KV Cache is 6.77GiB.\n",
      "INFO 03-25 01:38:47 worker.py:267] Memory profiling takes 19.88 seconds\n",
      "INFO 03-25 01:38:47 worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.99) = 39.18GiB\n",
      "INFO 03-25 01:38:47 worker.py:267] model weights take 25.14GiB; non_torch_memory takes 2.95GiB; PyTorch activation peak memory takes 5.16GiB; the rest of the memory reserved for KV Cache is 5.93GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] Memory profiling takes 19.97 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.99) = 39.18GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 01:38:47 worker.py:267] model weights take 25.14GiB; non_torch_memory takes 2.10GiB; PyTorch activation peak memory takes 5.16GiB; the rest of the memory reserved for KV Cache is 6.77GiB.\n",
      "INFO 03-25 01:38:47 executor_base.py:110] # CUDA blocks: 6166, # CPU blocks: 4161\n",
      "INFO 03-25 01:38:47 executor_base.py:115] Maximum concurrency for 30000 tokens per request: 3.29x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 01:38:54 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 01:38:54 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-25 01:38:54 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 01:38:54 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|                                                                                         | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 01:38:55 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 01:38:55 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 01:38:55 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 01:38:55 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  97%|█████████████████████████████████████████████████████████████████████████████▋  | 34/35 [00:53<00:01,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 01:39:48 custom_all_reduce.py:226] Registering 8602 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 01:39:48 custom_all_reduce.py:226] Registering 8602 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 01:39:49 custom_all_reduce.py:226] Registering 8602 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|████████████████████████████████████████████████████████████████████████████████| 35/35 [00:55<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 01:39:49 custom_all_reduce.py:226] Registering 8602 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 01:39:49 custom_all_reduce.py:226] Registering 8602 cuda graph addresses\n",
      "INFO 03-25 01:39:49 custom_all_reduce.py:226] Registering 8602 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 01:39:50 custom_all_reduce.py:226] Registering 8602 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 01:39:50 custom_all_reduce.py:226] Registering 8602 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494465)\u001b[0;0m INFO 03-25 01:39:50 model_runner.py:1562] Graph capturing finished in 57 secs, took 2.20 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494463)\u001b[0;0m INFO 03-25 01:39:50 model_runner.py:1562] Graph capturing finished in 56 secs, took 2.20 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494464)\u001b[0;0m INFO 03-25 01:39:50 model_runner.py:1562] Graph capturing finished in 57 secs, took 2.20 GiB\n",
      "INFO 03-25 01:39:50 model_runner.py:1562] Graph capturing finished in 56 secs, took 2.20 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494459)\u001b[0;0m INFO 03-25 01:39:50 model_runner.py:1562] Graph capturing finished in 56 secs, took 2.20 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494462)\u001b[0;0m INFO 03-25 01:39:50 model_runner.py:1562] Graph capturing finished in 56 secs, took 2.20 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494460)\u001b[0;0m INFO 03-25 01:39:50 model_runner.py:1562] Graph capturing finished in 56 secs, took 2.20 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=494461)\u001b[0;0m INFO 03-25 01:39:50 model_runner.py:1562] Graph capturing finished in 56 secs, took 2.20 GiB\n",
      "INFO 03-25 01:39:50 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 83.82 seconds\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드, bitsandbytes 압축된 모델을 4비트로 로드\n",
    "model = LLM(\n",
    "    'meta-llama/Llama-3.1-405B-Instruct', \n",
    "    quantization=\"bitsandbytes\",  # bitsandbytes로 양자화된 모델 로드\n",
    "    load_format=\"bitsandbytes\",\n",
    "    tensor_parallel_size=8,  # GPU 사용\n",
    "    max_model_len=30000,#입력제한\n",
    "    gpu_memory_utilization=0.995\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17194cdf-4cff-447c-adca-6c6e77a7d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_PATH = 'meta-llama/Llama-3.1-405B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "015c83ee-2e56-4bf2-9ba1-78b930f4dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(temperature=0.1,\n",
    "                                 repetition_penalty=1.2,\n",
    "                                 top_p=0.9,\n",
    "                                 top_k=30,\n",
    "                                 max_tokens=2000,\n",
    "                                 skip_special_tokens=False,\n",
    "                                 stop = [tokenizer.eos_token]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52678db4-b9c7-4f4d-81f9-d7e990b048f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def instruct_structure(prompt):\n",
    "    input_text, output_text = prompt.split('### target')\n",
    "    input_text = input_text.replace('### glossaries', '### glossary').replace('\\n* ', '\\n• ')\n",
    "    input_text = re.sub(r\"\\[[^\\]]+\\] \", \"[UNK] \", input_text)\n",
    "    return input_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed41b127-a58c-4755-8cc4-c28163acd2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:00<00:00, 17220.29it/s]\n"
     ]
    }
   ],
   "source": [
    "project_id = \"prod-ai-project\"\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project_id)\n",
    "sql = \"\"\"select series_id, episode_id, org_input_text, org_output_text, prompt \n",
    "        from webtoon_translation.structured_240820_ep_line\n",
    "        where data_split = 'romance_valid'\"\"\"\n",
    "df = client.query(sql).result().to_dataframe()\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['prompt'] = df['prompt'].progress_apply(lambda x: instruct_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceed569b-243b-49df-82f4-a7003a8452a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### glossary\n",
      "• 아벨 헤일론 (M): abel heylon\n",
      "• 아벨 (M): abel\n",
      "• 피오나 (F): fiona\n",
      "• 마법: magic / spell\n",
      "\n",
      "000\t[UNK] 북부 최전방 헤일론\n",
      "001\t[UNK] 여기가 헤일론 공작 성….\n",
      "002\t[UNK] 엄청난 위압감이야…\n",
      "003\t[UNK] 나는 결국 가족에게 떠밀려 무자비한 공작이 다스린다는 북부 최전방에 왔다.\n",
      "004\t[UNK] 딱 봐도 마왕성 같은 게 사람 엄청 굴릴 것 같다.\n",
      "005\t[UNK] 내 무덤을 내가 팠지...\n",
      "006\t[UNK] 이쪽으로.\n",
      "007\t[UNK] 문을 열어라.\n",
      "008\t[UNK] 문이, 엄청 커…!!\n",
      "009\t[UNK] 북부의 공작, 아벨 헤일론\n",
      "010\t[UNK] 너 나름대로는 각오했겠지. 하지만…\n",
      "011\t[UNK] 너 같은 어린애는 여기에 필요 없다.\n",
      "012\t[UNK] 집으로 돌아가도록.\n",
      "013\t[UNK] 쓸모없는 자에게 투자할 시간은 없다 이건가.\n",
      "014\t[UNK] 아벨은 모르겠지만, 지금의 나는 마땅히 돌아갈 곳도 없어.\n",
      "015\t[UNK] 원작 속의, 자신의 마법적 재능을 모르던 피오나는-\n",
      "016\t[UNK] 이렇게 그린 가문에서도 헤일론에서도 쫓겨난다.\n",
      "017\t[UNK] 그리고 이때 세상에 버림받은 상처가\n",
      "018\t[UNK] 후에 피오나가 악역이 되는 결정적인 계기가 된다.\n",
      "019\t[UNK] 하지만 지금의 나는 알고 있어.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████████████████| 1/1 [00:01<00:00,  1.90s/it, est. speed input: 350.28 toks/s, output: 8.41 toks/s]\n"
     ]
    }
   ],
   "source": [
    "def instruct_idiom_prompt(user_input):\n",
    "    return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\\\n",
    "너는 웹툰 한 회차에 대한 한국어 대사 여러 줄을 입력으로 받고, \\\n",
    "해당 대사에 포함된 한글 관용어구를 추출하고 이를 영어로 번역하는 일을 수행할 거야. \\\n",
    "대신 모든 관용어구를 추출하는 건 아니고 한글을 영어로 번역했을 때 직역이 아닌 의역해야하는 관용어만 추출하고 번역할거야. \\\n",
    "구체적으로, 먼저 대사 내의 한국어 관용어구가 있으면 이를 추출하고 영어로 번역해. \\\n",
    "그 다음 한국어 관용어와 번역한 영어를 비교해보면서 직역인지 의역인지 판단해. \\\n",
    "만약 의역으로 판단되는 경우 의역된 관용표현을 다음과 같이 생성해줘.\\\n",
    "'<idiom> 한국어 관용표현 : 의역된 영어 표현 </idiom>'\\\n",
    "그리고 한 문장 내에 의역되어야 하는 여러 개의 관용표현이 있다면 아래와 같이 여러 번 생성하면 돼.\\n\\\n",
    "'<idiom> 한국어 관용표현1 : 의역된 영어 표현1 </idiom>'\\n'<idiom> 한국어 관용표현2 : 의역된 영어 표현2 </idiom>'... \\n\\\n",
    "그리고 만약 한 문장 내에 의역되어야 하는 관용표현이 없다면 아무것도 생성하지마.<|eot_id|>\\\n",
    "<|start_header_id|>user<|end_header_id|>\\n\\\n",
    "{user_input}<|eot_id|>\\n\\\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    \n",
    "for data_idx in range(74):\n",
    "    data = df['prompt'][data_idx]\n",
    "    user_input = data.split(\"### source\")[1].strip()\n",
    "    glossary = data.split(\"### source\")[0].strip()\n",
    "    print(glossary)\n",
    "    print()\n",
    "    print(user_input)\n",
    "    prompt = instruct_idiom_prompt(user_input)\n",
    "    #print()\n",
    "    #print(prompt)\n",
    "    #print()\n",
    "    outputs = model.generate([prompt])\n",
    "    #print(outputs[0].outputs[0])\n",
    "    #print(outputs[0].outputs[0].text.strip())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f3309-ffce-471d-9767-44b103b678ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████| 2/2 [03:01<00:00, 90.93s/it, est. speed input: 4.57 toks/s, output: 11.70 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████| 2/2 [02:07<00:00, 63.76s/it, est. speed input: 16.06 toks/s, output: 8.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [03:34<00:00, 107.34s/it, est. speed input: 13.12 toks/s, output: 7.86 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [05:34<00:00, 167.26s/it, est. speed input: 10.73 toks/s, output: 6.40 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [05:55<00:00, 177.66s/it, est. speed input: 11.15 toks/s, output: 6.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [05:26<00:00, 163.00s/it, est. speed input: 11.46 toks/s, output: 6.35 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [05:44<00:00, 172.12s/it, est. speed input: 10.33 toks/s, output: 6.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [05:48<00:00, 174.36s/it, est. speed input: 11.34 toks/s, output: 6.91 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [06:11<00:00, 185.85s/it, est. speed input: 10.64 toks/s, output: 6.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [07:07<00:00, 213.62s/it, est. speed input: 10.79 toks/s, output: 6.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [05:55<00:00, 177.85s/it, est. speed input: 10.31 toks/s, output: 6.37 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [06:10<00:00, 185.36s/it, est. speed input: 10.85 toks/s, output: 6.19 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [05:38<00:00, 169.15s/it, est. speed input: 10.78 toks/s, output: 6.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████| 2/2 [05:35<00:00, 167.64s/it, est. speed input: 10.94 toks/s, output: 6.27 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                      | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "        return re.sub(r'^\\d+\\s+\\[UNK\\]\\s+', '', text)\n",
    "def clean_text2(text):\n",
    "    return re.sub(r'^\\d+\\s+','', text)\n",
    "\n",
    "import csv\n",
    "import time\n",
    "\n",
    "for data_idx in range(0, 74, 2):\n",
    "    prompt1 = df['prompt'][data_idx]\n",
    "    prompt2 = df['prompt'][data_idx+1] if data_idx + 1 < 74 else None\n",
    "\n",
    "    pre1 = prompt1.split('### source')[1].strip()\n",
    "    pre_li1 = [clean_text(d) for d in pre1.split('\\n')]\n",
    "    pre_li1[-1] = pre_li1[-1].split('<|eot_id|>')[0]\n",
    "    pre_len1 = len(pre_li1)\n",
    "\n",
    "    if prompt2:\n",
    "        pre2 = prompt2.split('### source')[1].strip()\n",
    "        pre_li2 = [clean_text(d) for d in pre2.split('\\n')]\n",
    "        pre_li2[-1] = pre_li2[-1].split('<|eot_id|>')[0]\n",
    "        pre_len2 = len(pre_li2)\n",
    "        \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate([prompt1, prompt2] if prompt2 else [prompt1], sampling_params)\n",
    "        print(f\"{time.time()-start_time:.2f}\")\n",
    "\n",
    "        result1 = outputs[0].outputs[0].text.strip()\n",
    "        result_li1 = [clean_text2(d) for d in result1.split('\\n')][:pre_len1]\n",
    "\n",
    "        with open(f\"data/llama_result/llama_vllm2_{data_idx}.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['hangle', 'first_translate'])\n",
    "            for row in zip(pre_li1, result_li1):\n",
    "                writer.writerow(row)\n",
    "\n",
    "        if prompt2:\n",
    "            result2 = outputs[1].outputs[0].text.strip()\n",
    "            result_li2 = [clean_text2(d) for d in result2.split('\\n')][:pre_len2]\n",
    "\n",
    "            with open(f\"data/llama_result/llama_vllm2_{data_idx+1}.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['hangle', 'first_translate'])\n",
    "                for row in zip(pre_li2, result_li2):\n",
    "                    writer.writerow(row)\n",
    "\n",
    "\n",
    "# for data_idx in range(74):\n",
    "#     prompt = df['prompt'][data_idx]\n",
    "#     pre = prompt.split('### source')[1].strip()\n",
    "#     pre_li = [clean_text(d) for d in pre.split('\\n')]\n",
    "#     pre_li[-1] = pre_li[-1].split('<|eot_id|>')[0]\n",
    "#     pre_len = len(pre_li)\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     # 모델 추론\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(prompt, sampling_params)\n",
    "#         print(f\"{time.time()-start_time:.2f}\")\n",
    "#         result = output[0].outputs[0].text.strip()\n",
    "#         result_li = [clean_text2(d) for d in result.split('\\n')]\n",
    "#         result_li = result_li[:pre_len]\n",
    "#         #print(result_li)\n",
    "    \n",
    "#     with open(f\"data/llama_result/llama_vllm2_{data_idx}.csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow(['hangle', 'first_translate'])\n",
    "#         for row in  zip(pre_li, result_li):\n",
    "#             writer.writerow(row)\n",
    "#     #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8458c4f-c196-4d3a-ab35-436e19a66f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
