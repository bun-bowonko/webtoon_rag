{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bac7126-1417-48b0-b9df-3066f0c4cc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from accelerate import Accelerator, infer_auto_device_map\n",
    "\n",
    "LLAMA_PATH = 'model/dpo-240820-ep-line-merged'\n",
    "\n",
    "# 4bit í€€íƒ€ì´ì œì´ì…˜ ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "        llm_int8_enable_fp32_cpu_offload=True  # CPU ì˜¤í”„ë¡œë”© í™œì„±í™”\n",
    ")\n",
    "# ê° GPUì— ìµœëŒ€ ë©”ëª¨ë¦¬ ì„¤ì • (ì˜ˆì‹œë¡œ 40GBì”© í• ë‹¹)\n",
    "max_memory = {i: \"40GB\" for i in range(torch.cuda.device_count())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c68620-1797-4d75-aae5-2e37bcabaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ ì„¤ì •ì„ ë¨¼ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "config = AutoConfig.from_pretrained(LLAMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280cb594-6c13-4290-a887-4b156b8bd1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_utils import init_empty_weights  # âœ… ì˜¬ë°”ë¥¸ import\n",
    "\n",
    "# ì•„ì§ ëª¨ë¸ì„ ì„ ì–¸í•˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ `config`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹ˆ ëª¨ë¸ ìƒì„±\n",
    "# ğŸš€ ê°€ì¤‘ì¹˜ë¥¼ ì „í˜€ ë¡œë“œí•˜ì§€ ì•ŠëŠ” ì™„ì „ ë¹ˆ(empty) ëª¨ë¸ ìƒì„±\n",
    "with init_empty_weights():\n",
    "    empty_model = LlamaForCausalLM(config)\n",
    "print(\"ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8f6bfa-56e7-4497-8aa9-00e2b9cf2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# **ğŸš€ GPU 8ëŒ€ì— ê· ë“±í•˜ê²Œ ë¶„ë°°í•˜ëŠ” ìˆ˜ë™ device_map ìƒì„±**\n",
    "device_map = {}\n",
    "\n",
    "# ì„ë² ë”© & ì¶œë ¥ ë ˆì´ì–´ëŠ” GPU 0ì— ë°°ì¹˜\n",
    "device_map[\"model.embed_tokens\"] = 0\n",
    "device_map[\"lm_head\"] = 0\n",
    "\n",
    "# 126ê°œ LlamaDecoderLayerë¥¼ 8ê°œì˜ GPUì— ê· ë“± ë¶„ë°°\n",
    "for i, layer in enumerate(empty_model.model.layers):\n",
    "    assigned_gpu = i % num_gpus  # GPU ì¸ë±ìŠ¤ 0ë¶€í„° 7ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ í• ë‹¹\n",
    "    device_map[f\"model.layers.{i}\"] = assigned_gpu\n",
    "\n",
    "# ë§ˆì§€ë§‰ RMSNormë„ ë§ˆì§€ë§‰ GPUë¡œ ë°°ì¹˜\n",
    "device_map[\"model.norm\"] = 7\n",
    "device_map[\"model.rotary_emb\"] = 7  # Rotary Embeddingë„ ë§ˆì§€ë§‰ GPUë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3624c46-eecd-44c5-b334-ab007544c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'lm_head': 0, 'model.layers.0': 0, 'model.layers.1': 1, 'model.layers.2': 2, 'model.layers.3': 3, 'model.layers.4': 4, 'model.layers.5': 5, 'model.layers.6': 6, 'model.layers.7': 7, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 2, 'model.layers.11': 3, 'model.layers.12': 4, 'model.layers.13': 5, 'model.layers.14': 6, 'model.layers.15': 7, 'model.layers.16': 0, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 3, 'model.layers.20': 4, 'model.layers.21': 5, 'model.layers.22': 6, 'model.layers.23': 7, 'model.layers.24': 0, 'model.layers.25': 1, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 4, 'model.layers.29': 5, 'model.layers.30': 6, 'model.layers.31': 7, 'model.layers.32': 0, 'model.layers.33': 1, 'model.layers.34': 2, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 5, 'model.layers.38': 6, 'model.layers.39': 7, 'model.layers.40': 0, 'model.layers.41': 1, 'model.layers.42': 2, 'model.layers.43': 3, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 6, 'model.layers.47': 7, 'model.layers.48': 0, 'model.layers.49': 1, 'model.layers.50': 2, 'model.layers.51': 3, 'model.layers.52': 4, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 7, 'model.layers.56': 0, 'model.layers.57': 1, 'model.layers.58': 2, 'model.layers.59': 3, 'model.layers.60': 4, 'model.layers.61': 5, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 0, 'model.layers.65': 1, 'model.layers.66': 2, 'model.layers.67': 3, 'model.layers.68': 4, 'model.layers.69': 5, 'model.layers.70': 6, 'model.layers.71': 7, 'model.layers.72': 0, 'model.layers.73': 1, 'model.layers.74': 2, 'model.layers.75': 3, 'model.layers.76': 4, 'model.layers.77': 5, 'model.layers.78': 6, 'model.layers.79': 7, 'model.layers.80': 0, 'model.layers.81': 1, 'model.layers.82': 2, 'model.layers.83': 3, 'model.layers.84': 4, 'model.layers.85': 5, 'model.layers.86': 6, 'model.layers.87': 7, 'model.layers.88': 0, 'model.layers.89': 1, 'model.layers.90': 2, 'model.layers.91': 3, 'model.layers.92': 4, 'model.layers.93': 5, 'model.layers.94': 6, 'model.layers.95': 7, 'model.layers.96': 0, 'model.layers.97': 1, 'model.layers.98': 2, 'model.layers.99': 3, 'model.layers.100': 4, 'model.layers.101': 5, 'model.layers.102': 6, 'model.layers.103': 7, 'model.layers.104': 0, 'model.layers.105': 1, 'model.layers.106': 2, 'model.layers.107': 3, 'model.layers.108': 4, 'model.layers.109': 5, 'model.layers.110': 6, 'model.layers.111': 7, 'model.layers.112': 0, 'model.layers.113': 1, 'model.layers.114': 2, 'model.layers.115': 3, 'model.layers.116': 4, 'model.layers.117': 5, 'model.layers.118': 6, 'model.layers.119': 7, 'model.layers.120': 0, 'model.layers.121': 1, 'model.layers.122': 2, 'model.layers.123': 3, 'model.layers.124': 4, 'model.layers.125': 5, 'model.norm': 7, 'model.rotary_emb': 7}\n"
     ]
    }
   ],
   "source": [
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30c0af9-6a75-4bcc-b5dc-318e4e62e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [11:44<00:00,  3.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ë¡œë“œ (`device_map` ì ìš©)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    LLAMA_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,  # ìë™ ë¶„ë°°ëœ device_map ì ìš©\n",
    "    #attn_implementation=\"flash_attention_2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce02cd5-92e0-46e6-b655-c6ed55f9ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face ê³„ì •ì—ì„œ ë°›ì€ API í† í°ì„ ì…ë ¥\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b10c993-2396-49b7-a42f-288d41784cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerator ê°ì²´ ìƒì„± í›„ ëª¨ë¸ ì¤€ë¹„\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e11cf94-f5e3-4fd1-86dc-6817904b6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = 'model/dpo-240820-ep-line-merged/tokenizer'\n",
    "\n",
    "# Tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "262c3e54-c103-4290-9718-f22e3523a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|>\n",
      "<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d60dee-9ee9-491e-a1c2-d007e952edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def instruct_structure(prompt,system_prompt=\n",
    "                       \"\"\"You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
    "    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
    "    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
    "    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English.\"\"\"):\n",
    "    input_text, output_text = prompt.split('### target')\n",
    "    input_text = input_text.replace('### glossaries', '### glossary').replace('\\n* ', '\\nâ€¢ ')\n",
    "    input_text = re.sub(r\"\\[[^\\]]+\\] \", \"none \", input_text)\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {input_text.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f79e1fd4-374f-444a-9318-b86cfd6afa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:00<00:00, 17557.33it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "project_id = \"prod-ai-project\"\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project_id)\n",
    "sql = \"\"\"select series_id, episode_id, org_input_text, org_output_text, prompt \n",
    "        from webtoon_translation.structured_240820_ep_line\n",
    "        where data_split = 'romance_valid'\"\"\"\n",
    "df = client.query(sql).result().to_dataframe()\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['prompt'] = df['prompt'].progress_apply(lambda x: instruct_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc15e8b-2031-42cb-b97a-14399b8c986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "    You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
      "    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
      "    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
      "    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "    ### glossary\n",
      "â€¢ ì•„ë²¨ í—¤ì¼ë¡  (M): abel heylon\n",
      "â€¢ ì•„ë²¨ (M): abel\n",
      "â€¢ í”¼ì˜¤ë‚˜ (F): fiona\n",
      "â€¢ ë§ˆë²•: magic / spell\n",
      "\n",
      "### source\n",
      "000\tnone ë¶ë¶€ ìµœì „ë°© í—¤ì¼ë¡ \n",
      "001\tnone ì—¬ê¸°ê°€ í—¤ì¼ë¡  ê³µì‘ ì„±â€¦.\n",
      "002\tnone ì—„ì²­ë‚œ ìœ„ì••ê°ì´ì•¼â€¦\n",
      "003\tnone ë‚˜ëŠ” ê²°êµ­ ê°€ì¡±ì—ê²Œ ë– ë°€ë ¤ ë¬´ìë¹„í•œ ê³µì‘ì´ ë‹¤ìŠ¤ë¦°ë‹¤ëŠ” ë¶ë¶€ ìµœì „ë°©ì— ì™”ë‹¤.\n",
      "004\tnone ë”± ë´ë„ ë§ˆì™•ì„± ê°™ì€ ê²Œ ì‚¬ëŒ ì—„ì²­ êµ´ë¦´ ê²ƒ ê°™ë‹¤.\n",
      "005\tnone ë‚´ ë¬´ë¤ì„ ë‚´ê°€ íŒ ì§€...\n",
      "006\tnone ì´ìª½ìœ¼ë¡œ.\n",
      "007\tnone ë¬¸ì„ ì—´ì–´ë¼.\n",
      "008\tnone ë¬¸ì´, ì—„ì²­ ì»¤â€¦!!\n",
      "009\tnone ë¶ë¶€ì˜ ê³µì‘, ì•„ë²¨ í—¤ì¼ë¡ \n",
      "010\tnone ë„ˆ ë‚˜ë¦„ëŒ€ë¡œëŠ” ê°ì˜¤í–ˆê² ì§€. í•˜ì§€ë§Œâ€¦\n",
      "011\tnone ë„ˆ ê°™ì€ ì–´ë¦°ì• ëŠ” ì—¬ê¸°ì— í•„ìš” ì—†ë‹¤.\n",
      "012\tnone ì§‘ìœ¼ë¡œ ëŒì•„ê°€ë„ë¡.\n",
      "013\tnone ì“¸ëª¨ì—†ëŠ” ìì—ê²Œ íˆ¬ìí•  ì‹œê°„ì€ ì—†ë‹¤ ì´ê±´ê°€.\n",
      "014\tnone ì•„ë²¨ì€ ëª¨ë¥´ê² ì§€ë§Œ, ì§€ê¸ˆì˜ ë‚˜ëŠ” ë§ˆë•…íˆ ëŒì•„ê°ˆ ê³³ë„ ì—†ì–´.\n",
      "015\tnone ì›ì‘ ì†ì˜, ìì‹ ì˜ ë§ˆë²•ì  ì¬ëŠ¥ì„ ëª¨ë¥´ë˜ í”¼ì˜¤ë‚˜ëŠ”-\n",
      "016\tnone ì´ë ‡ê²Œ ê·¸ë¦° ê°€ë¬¸ì—ì„œë„ í—¤ì¼ë¡ ì—ì„œë„ ì«“ê²¨ë‚œë‹¤.\n",
      "017\tnone ê·¸ë¦¬ê³  ì´ë•Œ ì„¸ìƒì— ë²„ë¦¼ë°›ì€ ìƒì²˜ê°€\n",
      "018\tnone í›„ì— í”¼ì˜¤ë‚˜ê°€ ì•…ì—­ì´ ë˜ëŠ” ê²°ì •ì ì¸ ê³„ê¸°ê°€ ëœë‹¤.\n",
      "019\tnone í•˜ì§€ë§Œ ì§€ê¸ˆì˜ ë‚˜ëŠ” ì•Œê³  ìˆì–´.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(df['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d405a225-add5-439e-9437-6b438c522625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "    You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
      "    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
      "    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
      "    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "    ### glossary\n",
      "â€¢ ì•„ë²¨ í—¤ì¼ë¡  (M): abel heylon\n",
      "â€¢ ì•„ë²¨ (M): abel\n",
      "â€¢ í”¼ì˜¤ë‚˜ (F): fiona\n",
      "â€¢ ë§ˆë²•: magic / spell\n",
      "\n",
      "### source\n",
      "000\tnone ë¶ë¶€ ìµœì „ë°© í—¤ì¼ë¡ \n",
      "001\tnone ì—¬ê¸°ê°€ í—¤ì¼ë¡  ê³µì‘ ì„±â€¦.\n",
      "002\tnone ì—„ì²­ë‚œ ìœ„ì••ê°ì´ì•¼â€¦\n",
      "003\tnone ë‚˜ëŠ” ê²°êµ­ ê°€ì¡±ì—ê²Œ ë– ë°€ë ¤ ë¬´ìë¹„í•œ ê³µì‘ì´ ë‹¤ìŠ¤ë¦°ë‹¤ëŠ” ë¶ë¶€ ìµœì „ë°©ì— ì™”ë‹¤.\n",
      "004\tnone ë”± ë´ë„ ë§ˆì™•ì„± ê°™ì€ ê²Œ ì‚¬ëŒ ì—„ì²­ êµ´ë¦´ ê²ƒ ê°™ë‹¤.\n",
      "005\tnone ë‚´ ë¬´ë¤ì„ ë‚´ê°€ íŒ ì§€...\n",
      "006\tnone ì´ìª½ìœ¼ë¡œ.\n",
      "007\tnone ë¬¸ì„ ì—´ì–´ë¼.\n",
      "008\tnone ë¬¸ì´, ì—„ì²­ ì»¤â€¦!!\n",
      "009\tnone ë¶ë¶€ì˜ ê³µì‘, ì•„ë²¨ í—¤ì¼ë¡ \n",
      "010\tnone ë„ˆ ë‚˜ë¦„ëŒ€ë¡œëŠ” ê°ì˜¤í–ˆê² ì§€. í•˜ì§€ë§Œâ€¦\n",
      "011\tnone ë„ˆ ê°™ì€ ì–´ë¦°ì• ëŠ” ì—¬ê¸°ì— í•„ìš” ì—†ë‹¤.\n",
      "012\tnone ì§‘ìœ¼ë¡œ ëŒì•„ê°€ë„ë¡.\n",
      "013\tnone ì“¸ëª¨ì—†ëŠ” ìì—ê²Œ íˆ¬ìí•  ì‹œê°„ì€ ì—†ë‹¤ ì´ê±´ê°€.\n",
      "014\tnone ì•„ë²¨ì€ ëª¨ë¥´ê² ì§€ë§Œ, ì§€ê¸ˆì˜ ë‚˜ëŠ” ë§ˆë•…íˆ ëŒì•„ê°ˆ ê³³ë„ ì—†ì–´.\n",
      "015\tnone ì›ì‘ ì†ì˜, ìì‹ ì˜ ë§ˆë²•ì  ì¬ëŠ¥ì„ ëª¨ë¥´ë˜ í”¼ì˜¤ë‚˜ëŠ”-\n",
      "016\tnone ì´ë ‡ê²Œ ê·¸ë¦° ê°€ë¬¸ì—ì„œë„ í—¤ì¼ë¡ ì—ì„œë„ ì«“ê²¨ë‚œë‹¤.\n",
      "017\tnone ê·¸ë¦¬ê³  ì´ë•Œ ì„¸ìƒì— ë²„ë¦¼ë°›ì€ ìƒì²˜ê°€\n",
      "018\tnone í›„ì— í”¼ì˜¤ë‚˜ê°€ ì•…ì—­ì´ ë˜ëŠ” ê²°ì •ì ì¸ ê³„ê¸°ê°€ ëœë‹¤.\n",
      "019\tnone í•˜ì§€ë§Œ ì§€ê¸ˆì˜ ë‚˜ëŠ” ì•Œê³  ìˆì–´.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "    000\tthe northernmost region of helion territory\n",
      "001\tthis is house helionâ€™s castle...!\n",
      "002\tit looks so intimidating...\n",
      "003\tI was forced by my family to come all this way up north where duke helion rules with an iron fist.\n",
      "004\tthat place seriously resembles a demon king's lair! I bet they treat people like dirt there.\n",
      "005\tdid I just dig myself into my own grave?\n",
      "006\tthis way.\n",
      "007\topen the gates.\n",
      "008\tth-the gate is huge...!!!\n",
      "009\tduke abel helion of the north\n",
      "010\tIâ€™m sure youâ€™re prepared for what will happen next,\n",
      "011\tbut we donâ€™t need someone as young as you here.\n",
      "012\tgo back home.\n",
      "013\tis that his way of saying he doesnâ€™t have time to invest in useless people?\n",
      "014\tlittle does abel know, but I no longer have anywhere else to go now.\n",
      "015\tin the original story, since fiona didnâ€™t realize she had magical powers yet,\n",
      "016\tshe gets kicked out from both her family and house helion.\n",
      "017\tand when she feels abandoned by everyone around her,\n",
      "018\tfiona becomes more evil than ever before.\n",
      "019\tbut things are different now because I already know about my abilities. besides, Iâ€™ve been preparing for this moment. I wonâ€™t let them chase me away without putting up a fight first. bring it on, abel! I dare you!!<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "sample = df['prompt'][0]\n",
    "\n",
    "# ì…ë ¥ ë³€í™˜ ë° í† í°í™”\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\").to(\"cuda:7\")\n",
    "\n",
    "# ëª¨ë¸ ì¶”ë¡ \n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, \n",
    "                            max_length=1000,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.1,\n",
    "                            top_p=0.9,\n",
    "                            top_k=30,\n",
    "                            repetition_penalty=1.2\n",
    "                           )\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d42379-512d-404f-836b-bdaa278a1129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
