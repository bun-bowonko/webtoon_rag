{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bac7126-1417-48b0-b9df-3066f0c4cc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from accelerate import Accelerator, infer_auto_device_map\n",
    "\n",
    "LLAMA_PATH = 'model/dpo-240820-ep-line-merged'\n",
    "\n",
    "# 4bit 퀀타이제이션 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "        llm_int8_enable_fp32_cpu_offload=True  # CPU 오프로딩 활성화\n",
    ")\n",
    "# 각 GPU에 최대 메모리 설정 (예시로 40GB씩 할당)\n",
    "max_memory = {i: \"40GB\" for i in range(torch.cuda.device_count())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c68620-1797-4d75-aae5-2e37bcabaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 설정을 먼저 불러오기\n",
    "config = AutoConfig.from_pretrained(LLAMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280cb594-6c13-4290-a887-4b156b8bd1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈 모델 선언 완료\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_utils import init_empty_weights  # ✅ 올바른 import\n",
    "\n",
    "# 아직 모델을 선언하지 않은 상태에서 `config`를 기반으로 빈 모델 생성\n",
    "# 🚀 가중치를 전혀 로드하지 않는 완전 빈(empty) 모델 생성\n",
    "with init_empty_weights():\n",
    "    empty_model = LlamaForCausalLM(config)\n",
    "print(\"빈 모델 선언 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8f6bfa-56e7-4497-8aa9-00e2b9cf2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# **🚀 GPU 8대에 균등하게 분배하는 수동 device_map 생성**\n",
    "device_map = {}\n",
    "\n",
    "# 임베딩 & 출력 레이어는 GPU 0에 배치\n",
    "device_map[\"model.embed_tokens\"] = 0\n",
    "device_map[\"lm_head\"] = 0\n",
    "\n",
    "# 126개 LlamaDecoderLayer를 8개의 GPU에 균등 분배\n",
    "for i, layer in enumerate(empty_model.model.layers):\n",
    "    assigned_gpu = i % num_gpus  # GPU 인덱스 0부터 7까지 순차적으로 할당\n",
    "    device_map[f\"model.layers.{i}\"] = assigned_gpu\n",
    "\n",
    "# 마지막 RMSNorm도 마지막 GPU로 배치\n",
    "device_map[\"model.norm\"] = 7\n",
    "device_map[\"model.rotary_emb\"] = 7  # Rotary Embedding도 마지막 GPU로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3624c46-eecd-44c5-b334-ab007544c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'lm_head': 0, 'model.layers.0': 0, 'model.layers.1': 1, 'model.layers.2': 2, 'model.layers.3': 3, 'model.layers.4': 4, 'model.layers.5': 5, 'model.layers.6': 6, 'model.layers.7': 7, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 2, 'model.layers.11': 3, 'model.layers.12': 4, 'model.layers.13': 5, 'model.layers.14': 6, 'model.layers.15': 7, 'model.layers.16': 0, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 3, 'model.layers.20': 4, 'model.layers.21': 5, 'model.layers.22': 6, 'model.layers.23': 7, 'model.layers.24': 0, 'model.layers.25': 1, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 4, 'model.layers.29': 5, 'model.layers.30': 6, 'model.layers.31': 7, 'model.layers.32': 0, 'model.layers.33': 1, 'model.layers.34': 2, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 5, 'model.layers.38': 6, 'model.layers.39': 7, 'model.layers.40': 0, 'model.layers.41': 1, 'model.layers.42': 2, 'model.layers.43': 3, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 6, 'model.layers.47': 7, 'model.layers.48': 0, 'model.layers.49': 1, 'model.layers.50': 2, 'model.layers.51': 3, 'model.layers.52': 4, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 7, 'model.layers.56': 0, 'model.layers.57': 1, 'model.layers.58': 2, 'model.layers.59': 3, 'model.layers.60': 4, 'model.layers.61': 5, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 0, 'model.layers.65': 1, 'model.layers.66': 2, 'model.layers.67': 3, 'model.layers.68': 4, 'model.layers.69': 5, 'model.layers.70': 6, 'model.layers.71': 7, 'model.layers.72': 0, 'model.layers.73': 1, 'model.layers.74': 2, 'model.layers.75': 3, 'model.layers.76': 4, 'model.layers.77': 5, 'model.layers.78': 6, 'model.layers.79': 7, 'model.layers.80': 0, 'model.layers.81': 1, 'model.layers.82': 2, 'model.layers.83': 3, 'model.layers.84': 4, 'model.layers.85': 5, 'model.layers.86': 6, 'model.layers.87': 7, 'model.layers.88': 0, 'model.layers.89': 1, 'model.layers.90': 2, 'model.layers.91': 3, 'model.layers.92': 4, 'model.layers.93': 5, 'model.layers.94': 6, 'model.layers.95': 7, 'model.layers.96': 0, 'model.layers.97': 1, 'model.layers.98': 2, 'model.layers.99': 3, 'model.layers.100': 4, 'model.layers.101': 5, 'model.layers.102': 6, 'model.layers.103': 7, 'model.layers.104': 0, 'model.layers.105': 1, 'model.layers.106': 2, 'model.layers.107': 3, 'model.layers.108': 4, 'model.layers.109': 5, 'model.layers.110': 6, 'model.layers.111': 7, 'model.layers.112': 0, 'model.layers.113': 1, 'model.layers.114': 2, 'model.layers.115': 3, 'model.layers.116': 4, 'model.layers.117': 5, 'model.layers.118': 6, 'model.layers.119': 7, 'model.layers.120': 0, 'model.layers.121': 1, 'model.layers.122': 2, 'model.layers.123': 3, 'model.layers.124': 4, 'model.layers.125': 5, 'model.norm': 7, 'model.rotary_emb': 7}\n"
     ]
    }
   ],
   "source": [
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30c0af9-6a75-4bcc-b5dc-318e4e62e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████| 191/191 [11:44<00:00,  3.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드 (`device_map` 적용)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    LLAMA_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,  # 자동 분배된 device_map 적용\n",
    "    #attn_implementation=\"flash_attention_2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce02cd5-92e0-46e6-b655-c6ed55f9ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face 계정에서 받은 API 토큰을 입력\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b10c993-2396-49b7-a42f-288d41784cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerator 객체 생성 후 모델 준비\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e11cf94-f5e3-4fd1-86dc-6817904b6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_PATH = 'model/dpo-240820-ep-line-merged/tokenizer'\n",
    "\n",
    "# Tokenizer 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "262c3e54-c103-4290-9718-f22e3523a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|>\n",
      "<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d60dee-9ee9-491e-a1c2-d007e952edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def instruct_structure(prompt,system_prompt=\n",
    "                       \"\"\"You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
    "    • Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
    "    • Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
    "    • Translate with an American audience in mind. This means easy-to-read, conversational English.\"\"\"):\n",
    "    input_text, output_text = prompt.split('### target')\n",
    "    input_text = input_text.replace('### glossaries', '### glossary').replace('\\n* ', '\\n• ')\n",
    "    input_text = re.sub(r\"\\[[^\\]]+\\] \", \"none \", input_text)\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {input_text.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f79e1fd4-374f-444a-9318-b86cfd6afa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:00<00:00, 17557.33it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "project_id = \"prod-ai-project\"\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project_id)\n",
    "sql = \"\"\"select series_id, episode_id, org_input_text, org_output_text, prompt \n",
    "        from webtoon_translation.structured_240820_ep_line\n",
    "        where data_split = 'romance_valid'\"\"\"\n",
    "df = client.query(sql).result().to_dataframe()\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['prompt'] = df['prompt'].progress_apply(lambda x: instruct_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc15e8b-2031-42cb-b97a-14399b8c986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "    You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
      "    • Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
      "    • Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
      "    • Translate with an American audience in mind. This means easy-to-read, conversational English.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "    ### glossary\n",
      "• 아벨 헤일론 (M): abel heylon\n",
      "• 아벨 (M): abel\n",
      "• 피오나 (F): fiona\n",
      "• 마법: magic / spell\n",
      "\n",
      "### source\n",
      "000\tnone 북부 최전방 헤일론\n",
      "001\tnone 여기가 헤일론 공작 성….\n",
      "002\tnone 엄청난 위압감이야…\n",
      "003\tnone 나는 결국 가족에게 떠밀려 무자비한 공작이 다스린다는 북부 최전방에 왔다.\n",
      "004\tnone 딱 봐도 마왕성 같은 게 사람 엄청 굴릴 것 같다.\n",
      "005\tnone 내 무덤을 내가 팠지...\n",
      "006\tnone 이쪽으로.\n",
      "007\tnone 문을 열어라.\n",
      "008\tnone 문이, 엄청 커…!!\n",
      "009\tnone 북부의 공작, 아벨 헤일론\n",
      "010\tnone 너 나름대로는 각오했겠지. 하지만…\n",
      "011\tnone 너 같은 어린애는 여기에 필요 없다.\n",
      "012\tnone 집으로 돌아가도록.\n",
      "013\tnone 쓸모없는 자에게 투자할 시간은 없다 이건가.\n",
      "014\tnone 아벨은 모르겠지만, 지금의 나는 마땅히 돌아갈 곳도 없어.\n",
      "015\tnone 원작 속의, 자신의 마법적 재능을 모르던 피오나는-\n",
      "016\tnone 이렇게 그린 가문에서도 헤일론에서도 쫓겨난다.\n",
      "017\tnone 그리고 이때 세상에 버림받은 상처가\n",
      "018\tnone 후에 피오나가 악역이 되는 결정적인 계기가 된다.\n",
      "019\tnone 하지만 지금의 나는 알고 있어.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(df['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d405a225-add5-439e-9437-6b438c522625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "    You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
      "    • Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
      "    • Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
      "    • Translate with an American audience in mind. This means easy-to-read, conversational English.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "    ### glossary\n",
      "• 아벨 헤일론 (M): abel heylon\n",
      "• 아벨 (M): abel\n",
      "• 피오나 (F): fiona\n",
      "• 마법: magic / spell\n",
      "\n",
      "### source\n",
      "000\tnone 북부 최전방 헤일론\n",
      "001\tnone 여기가 헤일론 공작 성….\n",
      "002\tnone 엄청난 위압감이야…\n",
      "003\tnone 나는 결국 가족에게 떠밀려 무자비한 공작이 다스린다는 북부 최전방에 왔다.\n",
      "004\tnone 딱 봐도 마왕성 같은 게 사람 엄청 굴릴 것 같다.\n",
      "005\tnone 내 무덤을 내가 팠지...\n",
      "006\tnone 이쪽으로.\n",
      "007\tnone 문을 열어라.\n",
      "008\tnone 문이, 엄청 커…!!\n",
      "009\tnone 북부의 공작, 아벨 헤일론\n",
      "010\tnone 너 나름대로는 각오했겠지. 하지만…\n",
      "011\tnone 너 같은 어린애는 여기에 필요 없다.\n",
      "012\tnone 집으로 돌아가도록.\n",
      "013\tnone 쓸모없는 자에게 투자할 시간은 없다 이건가.\n",
      "014\tnone 아벨은 모르겠지만, 지금의 나는 마땅히 돌아갈 곳도 없어.\n",
      "015\tnone 원작 속의, 자신의 마법적 재능을 모르던 피오나는-\n",
      "016\tnone 이렇게 그린 가문에서도 헤일론에서도 쫓겨난다.\n",
      "017\tnone 그리고 이때 세상에 버림받은 상처가\n",
      "018\tnone 후에 피오나가 악역이 되는 결정적인 계기가 된다.\n",
      "019\tnone 하지만 지금의 나는 알고 있어.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "    000\tthe northernmost region of helion territory\n",
      "001\tthis is house helion’s castle...!\n",
      "002\tit looks so intimidating...\n",
      "003\tI was forced by my family to come all this way up north where duke helion rules with an iron fist.\n",
      "004\tthat place seriously resembles a demon king's lair! I bet they treat people like dirt there.\n",
      "005\tdid I just dig myself into my own grave?\n",
      "006\tthis way.\n",
      "007\topen the gates.\n",
      "008\tth-the gate is huge...!!!\n",
      "009\tduke abel helion of the north\n",
      "010\tI’m sure you’re prepared for what will happen next,\n",
      "011\tbut we don’t need someone as young as you here.\n",
      "012\tgo back home.\n",
      "013\tis that his way of saying he doesn’t have time to invest in useless people?\n",
      "014\tlittle does abel know, but I no longer have anywhere else to go now.\n",
      "015\tin the original story, since fiona didn’t realize she had magical powers yet,\n",
      "016\tshe gets kicked out from both her family and house helion.\n",
      "017\tand when she feels abandoned by everyone around her,\n",
      "018\tfiona becomes more evil than ever before.\n",
      "019\tbut things are different now because I already know about my abilities. besides, I’ve been preparing for this moment. I won’t let them chase me away without putting up a fight first. bring it on, abel! I dare you!!<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "sample = df['prompt'][0]\n",
    "\n",
    "# 입력 변환 및 토큰화\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\").to(\"cuda:7\")\n",
    "\n",
    "# 모델 추론\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, \n",
    "                            max_length=1000,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.1,\n",
    "                            top_p=0.9,\n",
    "                            top_k=30,\n",
    "                            repetition_penalty=1.2\n",
    "                           )\n",
    "\n",
    "# 결과 출력\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d42379-512d-404f-836b-bdaa278a1129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
