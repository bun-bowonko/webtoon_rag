{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1244e714-c1ac-43fd-bebc-9f8f7acc47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoConfig\n",
    "from accelerate import infer_auto_device_map\n",
    "from transformers.modeling_utils import init_empty_weights  # âœ… ì˜¬ë°”ë¥¸ import\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"  # 0ë²ˆ GPUë§Œ ì‚¬ìš©\n",
    "\n",
    "\n",
    "TOKENIZER_PATH = 'model/dpo-240820-ep-line-merged/tokenizer'\n",
    "\n",
    "# Tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7eb723-b98e-4597-827f-45c2cc84013e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AutoGPTQForCausalLM.from_pretrained() missing 1 required positional argument: 'quantize_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#config = AutoConfig.from_pretrained(LLAMA_PATH)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(config)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#         desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m init_empty_weights():\n\u001b[0;32m---> 15\u001b[0m     empty_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoGPTQForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124më¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: AutoGPTQForCausalLM.from_pretrained() missing 1 required positional argument: 'quantize_config'"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "LLAMA_PATH = 'model/dpo-240820-ep-line-merged'\n",
    "config = AutoConfig.from_pretrained(LLAMA_PATH)\n",
    "max_memory = {i: \"40GB\" for i in range(torch.cuda.device_count())}\n",
    "#config = AutoConfig.from_pretrained(LLAMA_PATH)\n",
    "#print(config)\n",
    "\n",
    "# quantize_config = BaseQuantizeConfig(\n",
    "#         bits=4,  # quantize model to 4-bit\n",
    "#         group_size=128,  # it is recommended to set the value to 128\n",
    "#         desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    "# )\n",
    "\n",
    "with init_empty_weights():\n",
    "    empty_model = AutoGPTQForCausalLM.from_pretrained(config)\n",
    "\n",
    "print(\"ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073ddaa-7df3-4552-881e-c7b2cd45df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# **ğŸš€ GPU 8ëŒ€ì— ê· ë“±í•˜ê²Œ ë¶„ë°°í•˜ëŠ” ìˆ˜ë™ device_map ìƒì„±**\n",
    "device_map = {}\n",
    "\n",
    "# ì„ë² ë”© & ì¶œë ¥ ë ˆì´ì–´ëŠ” GPU 0ì— ë°°ì¹˜\n",
    "device_map[\"model.embed_tokens\"] = 0\n",
    "device_map[\"lm_head\"] = 0\n",
    "\n",
    "# 126ê°œ LlamaDecoderLayerë¥¼ 8ê°œì˜ GPUì— ê· ë“± ë¶„ë°°\n",
    "for i, layer in enumerate(empty_model.model.layers):\n",
    "    assigned_gpu = i % num_gpus  # GPU ì¸ë±ìŠ¤ 0ë¶€í„° 7ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ í• ë‹¹\n",
    "    device_map[f\"model.layers.{i}\"] = assigned_gpu\n",
    "\n",
    "# ë§ˆì§€ë§‰ RMSNormë„ ë§ˆì§€ë§‰ GPUë¡œ ë°°ì¹˜\n",
    "device_map[\"model.norm\"] = 7\n",
    "device_map[\"model.rotary_emb\"] = 7  # Rotary Embeddingë„ ë§ˆì§€ë§‰ GPUë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec9701-46cf-489c-afa1-97b144518f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 96/191 [08:17<09:39,  6.10s/it]"
     ]
    }
   ],
   "source": [
    "# 4ë¹„íŠ¸ ëª¨ë¸ ë³€í™˜\n",
    "\n",
    "quantized_model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    LLAMA_PATH,\n",
    "    device_map=device_map,#\"auto\"ê°€ ì•ˆ ë¨¹ìŒ\n",
    "    offload_state_dict=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantize_config={\"bits\": 4, \"group_size\": 128}  # 4ë¹„íŠ¸ ì ìš©\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8f63d-5632-4840-9df7-7423dddd2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.save_quantized(\"model/llama405b_quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8bd8d-8db3-47bf-8509-fe1bb152c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from accelerate import Accelerator, infer_auto_device_map\n",
    "\n",
    "LLAMA_PATH = 'model/dpo-240820-ep-line-merged'\n",
    "\n",
    "# 4bit í€€íƒ€ì´ì œì´ì…˜ ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "        llm_int8_enable_fp32_cpu_offload=True  # CPU ì˜¤í”„ë¡œë”© í™œì„±í™”\n",
    ")\n",
    "# ê° GPUì— ìµœëŒ€ ë©”ëª¨ë¦¬ ì„¤ì • (ì˜ˆì‹œë¡œ 40GBì”© í• ë‹¹)\n",
    "max_memory = {i: \"40GB\" for i in range(torch.cuda.device_count())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c68620-1797-4d75-aae5-2e37bcabaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ ì„¤ì •ì„ ë¨¼ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "config = AutoConfig.from_pretrained(LLAMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280cb594-6c13-4290-a887-4b156b8bd1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_utils import init_empty_weights  # âœ… ì˜¬ë°”ë¥¸ import\n",
    "\n",
    "# ì•„ì§ ëª¨ë¸ì„ ì„ ì–¸í•˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ `config`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹ˆ ëª¨ë¸ ìƒì„±\n",
    "# ğŸš€ ê°€ì¤‘ì¹˜ë¥¼ ì „í˜€ ë¡œë“œí•˜ì§€ ì•ŠëŠ” ì™„ì „ ë¹ˆ(empty) ëª¨ë¸ ìƒì„±\n",
    "with init_empty_weights():\n",
    "    empty_model = LlamaForCausalLM(config)\n",
    "print(\"ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252957f7-445c-438a-9895-cfb04a93c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# **ğŸš€ GPU 8ëŒ€ì— ê· ë“±í•˜ê²Œ ë¶„ë°°í•˜ëŠ” ìˆ˜ë™ device_map ìƒì„±**\n",
    "device_map = {}\n",
    "\n",
    "# ì„ë² ë”© & ì¶œë ¥ ë ˆì´ì–´ëŠ” GPU 0ì— ë°°ì¹˜\n",
    "device_map[\"model.embed_tokens\"] = 0\n",
    "device_map[\"lm_head\"] = 0\n",
    "\n",
    "# 126ê°œ LlamaDecoderLayerë¥¼ 8ê°œì˜ GPUì— ê· ë“± ë¶„ë°°\n",
    "for i, layer in enumerate(empty_model.model.layers):\n",
    "    assigned_gpu = i % num_gpus  # GPU ì¸ë±ìŠ¤ 0ë¶€í„° 7ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ í• ë‹¹\n",
    "    device_map[f\"model.layers.{i}\"] = assigned_gpu\n",
    "\n",
    "# ë§ˆì§€ë§‰ RMSNormë„ ë§ˆì§€ë§‰ GPUë¡œ ë°°ì¹˜\n",
    "device_map[\"model.norm\"] = 7\n",
    "device_map[\"model.rotary_emb\"] = 7  # Rotary Embeddingë„ ë§ˆì§€ë§‰ GPUë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30c0af9-6a75-4bcc-b5dc-318e4e62e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 00:47:35,943\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [21:20<00:00,  6.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (`device_map` ì ìš©)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    LLAMA_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,  # ìë™ ë¶„ë°°ëœ device_map ì ìš©\n",
    ")\n",
    "\n",
    "# Accelerator ê°ì²´ ìƒì„± í›„ ëª¨ë¸ ì¤€ë¹„\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_PATH = 'model/dpo-240820-ep-line-merged/tokenizer'\n",
    "\n",
    "# Tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d264c465-f7f8-4da4-b127-e3b1f5374ba7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not LlamaForCausalLM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/vllm/utils.py:1022\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1018\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1019\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/vllm/entrypoints/llm.py:242\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/vllm/engine/llm_engine.py:486\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m engine_config \u001b[38;5;241m=\u001b[39m \u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/vllm/engine/arg_utils.py:1103\u001b[0m, in \u001b[0;36mEngineArgs.create_engine_config\u001b[0;34m(self, usage_context)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_override_v1_engine_args(usage_context)\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;66;03m# gguf file needs a specific model loader and doesn't use hf_repo\u001b[39;00m\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcheck_gguf_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# bitsandbytes quantization needs a specific model loader\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# so we make sure the quant method and the load format are consistent\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/vllm/transformers_utils/utils.py:14\u001b[0m, in \u001b[0;36mcheck_gguf_file\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_gguf_file\u001b[39m(model: Union[\u001b[38;5;28mstr\u001b[39m, PathLike]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check if the file is a GGUF model.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/pathlib.py:960\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    963\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/pathlib.py:594\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args):\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 594\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/pathlib.py:578\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    576\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39m_parts\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[1;32m    581\u001b[0m         parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not LlamaForCausalLM"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model, tokenizer=tokenizer, dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23d60dee-9ee9-491e-a1c2-d007e952edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def instruct_structure(prompt,system_prompt=\n",
    "                       \"\"\"You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
    "    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
    "    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
    "    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English.\"\"\"):\n",
    "    input_text, output_text = prompt.split('### target')\n",
    "    input_text = input_text.replace('### glossaries', '### glossary').replace('\\n* ', '\\nâ€¢ ')\n",
    "    input_text = re.sub(r\"\\[[^\\]]+\\] \", \"none \", input_text)\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {input_text.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f79e1fd4-374f-444a-9318-b86cfd6afa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:00<00:00, 18163.54it/s]\n"
     ]
    }
   ],
   "source": [
    "project_id = \"prod-ai-project\"\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project_id)\n",
    "sql = \"\"\"select series_id, episode_id, org_input_text, org_output_text, prompt \n",
    "        from webtoon_translation.structured_240820_ep_line\n",
    "        where data_split = 'romance_valid'\"\"\"\n",
    "df = client.query(sql).result().to_dataframe()\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['prompt'] = df['prompt'].progress_apply(lambda x: instruct_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dc15e8b-2031-42cb-b97a-14399b8c986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "    You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
      "    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
      "    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
      "    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "    ### glossary\n",
      "â€¢ ì•„ë²¨ í—¤ì¼ë¡  (M): abel heylon\n",
      "â€¢ ì•„ë²¨ (M): abel\n",
      "â€¢ í”¼ì˜¤ë‚˜ (F): fiona\n",
      "â€¢ ë§ˆë²•: magic / spell\n",
      "\n",
      "### source\n",
      "000\tnone ë¶ë¶€ ìµœì „ë°© í—¤ì¼ë¡ \n",
      "001\tnone ì—¬ê¸°ê°€ í—¤ì¼ë¡  ê³µì‘ ì„±â€¦.\n",
      "002\tnone ì—„ì²­ë‚œ ìœ„ì••ê°ì´ì•¼â€¦\n",
      "003\tnone ë‚˜ëŠ” ê²°êµ­ ê°€ì¡±ì—ê²Œ ë– ë°€ë ¤ ë¬´ìë¹„í•œ ê³µì‘ì´ ë‹¤ìŠ¤ë¦°ë‹¤ëŠ” ë¶ë¶€ ìµœì „ë°©ì— ì™”ë‹¤.\n",
      "004\tnone ë”± ë´ë„ ë§ˆì™•ì„± ê°™ì€ ê²Œ ì‚¬ëŒ ì—„ì²­ êµ´ë¦´ ê²ƒ ê°™ë‹¤.\n",
      "005\tnone ë‚´ ë¬´ë¤ì„ ë‚´ê°€ íŒ ì§€...\n",
      "006\tnone ì´ìª½ìœ¼ë¡œ.\n",
      "007\tnone ë¬¸ì„ ì—´ì–´ë¼.\n",
      "008\tnone ë¬¸ì´, ì—„ì²­ ì»¤â€¦!!\n",
      "009\tnone ë¶ë¶€ì˜ ê³µì‘, ì•„ë²¨ í—¤ì¼ë¡ \n",
      "010\tnone ë„ˆ ë‚˜ë¦„ëŒ€ë¡œëŠ” ê°ì˜¤í–ˆê² ì§€. í•˜ì§€ë§Œâ€¦\n",
      "011\tnone ë„ˆ ê°™ì€ ì–´ë¦°ì• ëŠ” ì—¬ê¸°ì— í•„ìš” ì—†ë‹¤.\n",
      "012\tnone ì§‘ìœ¼ë¡œ ëŒì•„ê°€ë„ë¡.\n",
      "013\tnone ì“¸ëª¨ì—†ëŠ” ìì—ê²Œ íˆ¬ìí•  ì‹œê°„ì€ ì—†ë‹¤ ì´ê±´ê°€.\n",
      "014\tnone ì•„ë²¨ì€ ëª¨ë¥´ê² ì§€ë§Œ, ì§€ê¸ˆì˜ ë‚˜ëŠ” ë§ˆë•…íˆ ëŒì•„ê°ˆ ê³³ë„ ì—†ì–´.\n",
      "015\tnone ì›ì‘ ì†ì˜, ìì‹ ì˜ ë§ˆë²•ì  ì¬ëŠ¥ì„ ëª¨ë¥´ë˜ í”¼ì˜¤ë‚˜ëŠ”-\n",
      "016\tnone ì´ë ‡ê²Œ ê·¸ë¦° ê°€ë¬¸ì—ì„œë„ í—¤ì¼ë¡ ì—ì„œë„ ì«“ê²¨ë‚œë‹¤.\n",
      "017\tnone ê·¸ë¦¬ê³  ì´ë•Œ ì„¸ìƒì— ë²„ë¦¼ë°›ì€ ìƒì²˜ê°€\n",
      "018\tnone í›„ì— í”¼ì˜¤ë‚˜ê°€ ì•…ì—­ì´ ë˜ëŠ” ê²°ì •ì ì¸ ê³„ê¸°ê°€ ëœë‹¤.\n",
      "019\tnone í•˜ì§€ë§Œ ì§€ê¸ˆì˜ ë‚˜ëŠ” ì•Œê³  ìˆì–´.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(df['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405a225-add5-439e-9437-6b438c522625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sample = df['prompt'][0]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.1, \n",
    "                                 top_p=0.9,\n",
    "                                 max_length=1000,\n",
    "                                 repetition_penalty=1.2,\n",
    "                                 top_k=30,\n",
    "                                 do_sample=True\n",
    "                                )\n",
    "\n",
    "outputs = llm.generate([sample], sampling_params)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
