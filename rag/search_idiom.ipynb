{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "086c6903-0863-4aed-b82c-7b43ff25b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:00<00:00, 18400.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def instruct_structure(prompt):\n",
    "    input_text, output_text = prompt.split('### target')\n",
    "    input_text = input_text.replace('### glossaries', '### glossary').replace('\\n* ', '\\n• ')\n",
    "    input_text = re.sub(r\"\\[[^\\]]+\\] \", \"[UNK] \", input_text)\n",
    "    return input_text\n",
    "\n",
    "project_id = \"prod-ai-project\"\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project_id)\n",
    "sql = \"\"\"select series_id, episode_id, org_input_text, org_output_text, prompt \n",
    "        from webtoon_translation.structured_240820_ep_line\n",
    "        where data_split = 'romance_valid'\"\"\"\n",
    "df = client.query(sql).result().to_dataframe()\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['prompt'] = df['prompt'].progress_apply(lambda x: instruct_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a661bbd1-2a66-4abb-b6e2-2d903689aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def word_overlap_boost(s1, s2):\n",
    "    \"\"\"공통 단어 개수 비율 계산\"\"\"\n",
    "    words_s1, words_s2 = set(s1.split()), set(s2.split())\n",
    "    common_words = len(words_s1 & words_s2)\n",
    "    total_words = len(words_s1 | words_s2)\n",
    "    return common_words / total_words if total_words > 0 else 0\n",
    "\n",
    "# def substring_containment(s1, s2):\n",
    "#     \"\"\"짧은 문장이 긴 문장에 포함되는 정도 계산\"\"\"\n",
    "#     if len(s1) > len(s2):\n",
    "#         long_s, short_s = s1, s2\n",
    "#     else:\n",
    "#         long_s, short_s = s2, s1\n",
    "    \n",
    "#     common_length = len(short_s) if short_s in long_s else sum(c in long_s for c in short_s)\n",
    "#     return common_length / len(short_s) if len(short_s) > 0 else 0\n",
    "\n",
    "# def substring_containment(s1, s2):\n",
    "#     \"\"\"bi-gram 단위로 짧은 문장이 긴 문장에 포함되는 정도 계산\"\"\"\n",
    "#     if len(s1) > len(s2):\n",
    "#         long_s, short_s = s1, s2\n",
    "#     else:\n",
    "#         long_s, short_s = s2, s1\n",
    "    \n",
    "#     # bi-gram(2-gram) 집합 생성\n",
    "#     def get_bigrams(s):\n",
    "#         return {s[i:i+2] for i in range(len(s) - 1)}\n",
    "\n",
    "#     long_bigrams = get_bigrams(long_s)\n",
    "#     short_bigrams = get_bigrams(short_s)\n",
    "\n",
    "#     # 짧은 문장의 bi-gram 중 긴 문장에 포함된 bi-gram의 개수\n",
    "#     common_bigrams = short_bigrams & long_bigrams\n",
    "\n",
    "#     return len(common_bigrams) / len(short_bigrams) if short_bigrams else 0\n",
    "\n",
    "def substring_containment(s1, s2, tri_weight=4, bi_weight=2, char_weight=1):\n",
    "    \"\"\"bi-gram과 tri-gram 단위로 짧은 문장이 긴 문장에 포함되는 정도 계산 (tri-gram에 높은 가중치)\"\"\"\n",
    "    if len(s1) > len(s2):\n",
    "        long_s, short_s = s1, s2\n",
    "    else:\n",
    "        long_s, short_s = s2, s1\n",
    "    \n",
    "    # n-gram(2-gram, 3-gram) 집합 생성\n",
    "    def get_ngrams(s, n):\n",
    "        return {s[i:i+n] for i in range(len(s) - (n-1))}\n",
    "\n",
    "    long_char = get_ngrams(long_s, 1)\n",
    "    short_char = get_ngrams(short_s, 1)\n",
    "    \n",
    "    long_bigrams = get_ngrams(long_s, 2)\n",
    "    short_bigrams = get_ngrams(short_s, 2)\n",
    "    \n",
    "    long_trigrams = get_ngrams(long_s, 3)\n",
    "    short_trigrams = get_ngrams(short_s, 3)\n",
    "\n",
    "    #char 계산\n",
    "    common_char = long_char & short_char\n",
    "    char_score = len(common_char) / len(short_char) if short_char else 0\n",
    "    \n",
    "    # bi-gram 포함도 계산\n",
    "    common_bigrams = short_bigrams & long_bigrams\n",
    "    bigram_score = len(common_bigrams) / len(short_bigrams) if short_bigrams else 0\n",
    "\n",
    "    # tri-gram 포함도 계산\n",
    "    common_trigrams = short_trigrams & long_trigrams\n",
    "    trigram_score = len(common_trigrams) / len(short_trigrams) if short_trigrams else 0\n",
    "\n",
    "    # 최종 점수 계산 (tri-gram의 가중치를 더 높게 설정)\n",
    "    total_weight = char_weight + bi_weight + tri_weight\n",
    "    final_score = (char_weight * char_score + bi_weight * bigram_score + tri_weight * trigram_score) / total_weight\n",
    "\n",
    "    return final_score\n",
    "\n",
    "def adjusted_edit_distance(s1, s2):\n",
    "    \"\"\"Levenshtein 거리 기반 + 공통 단어 + 포함도 반영\"\"\"\n",
    "    #edit_distance = Levenshtein.distance(s1, s2)\n",
    "    #word_overlap = word_overlap_boost(s1, s2)\n",
    "    containment_ratio = substring_containment(s1, s2)\n",
    "\n",
    "    #print(word_overlap, containment_ratio)\n",
    "    \n",
    "    # 가중치 설정\n",
    "    #WEIGHT_WORD = 0.4\n",
    "    #WEIGHT_CONTAINMENT = 1.0\n",
    "    #WEIGHT_DECREASE = 0.5  # 포함 비율이 높을 때 거리 감소 가중치\n",
    "    #WEIGHT_INCREASE = 2.5  # 포함 비율이 낮을 때 거리 증가 가중치\n",
    "\n",
    "    #combined_similarity = (\n",
    "    #    WEIGHT_WORD * word_overlap +\n",
    "    #    WEIGHT_CONTAINMENT * containment_ratio\n",
    "    #)\n",
    "    combined_similarity = containment_ratio\n",
    "\n",
    "    # 겹치는 문자가 없으면 후보 제외\n",
    "    if combined_similarity == 0:\n",
    "        return -float('inf')\n",
    "\n",
    "    # 거리 조정 (가중치를 반영한 새로운 유사도 적용)\n",
    "    #WEIGHT_DISTANCE = 0.5\n",
    "    #adjusted_distance = edit_distance * (1 - WEIGHT_DISTANCE * combined_similarity)\n",
    "    #adjusted_distance = edit_distance\n",
    "    adjusted_distance = combined_similarity\n",
    "    # 포함 비율에 따른 거리 추가 조정\n",
    "    # if containment_ratio > 0.7:  # 짧은 문장이 충분히 포함되면 거리 감소\n",
    "    #     adjusted_distance *= (1 - WEIGHT_DECREASE * containment_ratio)\n",
    "    # elif containment_ratio < 0.3:  # 짧은 문장이 거의 포함되지 않으면 거리 증가\n",
    "    #     adjusted_distance *= (1 + WEIGHT_INCREASE * (1 - containment_ratio))\n",
    "\n",
    "    # 길이 정규화 적용\n",
    "    len1, len2 = len(s1), len(s2)\n",
    "    length_factor = max(len1, len2) / min(len1, len2)\n",
    "    adjusted_distance /= length_factor ** 0.5\n",
    "\n",
    "    return adjusted_distance\n",
    "\n",
    "def find_most_similar_sentence(target, sentences):\n",
    "    \"\"\"주어진 문장 목록에서 target 문장과 가장 유사한 문장을 찾음\"\"\"\n",
    "    similarities = [(sentence, adjusted_edit_distance(target, sentence)) for sentence in sentences]\n",
    "    similarities.sort(key=lambda x: x[1],reverse=True)  # 거리 기준 정렬 (높을수록 유사)\n",
    "    return similarities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "077846df-89dc-404e-9b27-9e8e5a244666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "import bs4\n",
    "import ssl\n",
    "import urllib3\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63766c80-c760-4646-869b-787ee810fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top(question):\n",
    "    file_path = \"./data/idiom_dict.txt\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    origin_lines = [line.split('>')[0].strip() for line in lines]\n",
    "\n",
    "    result = find_most_similar_sentence(question, origin_lines)\n",
    "    #문법적 유사성 판단 결과 \n",
    "    #4점 이하는 문법적 유사성이 낮다고 판단\n",
    "    result = [r[0] for r in result if r[1] < 3]\n",
    "    #문법적 유사성으로 1차 필터링\n",
    "    if len(result) == 0:\n",
    "        return None\n",
    "    print(result)\n",
    "    \n",
    "    docs = [Document(page_content=f\"{line}\") for line in result]\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"nlpai-lab/KURE-v1\")\n",
    "    #print(docs)\n",
    "    vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "    #retriever = vectorstore.as_retriever(search_type='similarity_score_threshold', \n",
    "    #                                 search_kwargs={'k':5,'score_threshold' : 0.3})\n",
    "\n",
    "    docs_with_scores = retriever.vectorstore.similarity_search_with_score(question, k=5)\n",
    "    doc = docs_with_scores[0][0]\n",
    "    score = 1 / (1+ docs_with_scores[0][1])\n",
    "    #print(\"@@@\")\n",
    "    #for doc, score in docs_with_scores:\n",
    "        #cosine_sim = 1 / (1 + score)  # FAISS L2 거리 → 코사인 유사도 변환\n",
    "        #print(f\"문서 내용: {doc.page_content}\")\n",
    "        #print(f\"유사도 점수 (코사인 유사도 변환): {cosine_sim}\")\n",
    "    #idiom = retriever.invoke(question) #retriever로 문서 가져오고\n",
    "\n",
    "    if hasattr(vectorstore, \"index\") and isinstance(vectorstore.index, faiss.Index):\n",
    "        # FAISS 인덱스를 CPU로 변환 (GPU 해제)\n",
    "        vectorstore.index = faiss.index_gpu_to_cpu(vectorstore.index)\n",
    "    \n",
    "    # FAISS 객체 삭제\n",
    "    del vectorstore\n",
    "    \n",
    "    # Python 가비지 컬렉션 실행\n",
    "    gc.collect()\n",
    "    \n",
    "    # PyTorch GPU 캐시 정리\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #print(idiom)\n",
    "    if score > 0.5:\n",
    "        return doc.page_content\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e0fbb0d6-769a-4e9e-ae71-0069e7fd65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idx = 3\n",
    "data = df['prompt'][data_idx]\n",
    "example = data.split(\"### source\")[1].strip()\n",
    "#print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "561fd7c1-6d85-4ad3-9293-82664efe966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_debug(question):\n",
    "    file_path = \"./data/idiom_dict.txt\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    origin_lines = [line.split('>')[0].strip() for line in lines]\n",
    "\n",
    "    result = find_most_similar_sentence(question, origin_lines)\n",
    "    #문법적 유사성 판단 결과 \n",
    "    #3점 이하는 문법적 유사성이 높다고 판단\n",
    "    #result = [(r[0], r[1]) for r in result if r[1] < 3]\n",
    "    #문법적 유사성으로 1차 필터링\n",
    "    if len(result) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return result\n",
    "    #print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d768997c-3657-4fc8-a1e6-f43b3a238d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = extract_top_debug(\"왜 남주가 벌써 내 코앞에서 굴러다니고 있는 거지?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d41dc308-8ba8-4495-955c-b8229819f266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('코앞에서 놓치다', 0.29725995273932915), ('뼈가 있는 말', 0.2571428571428572), ('화성에 가 있는 듯', 0.21461942290627503), ('접시는 깨라고 있는 것이다', 0.21427478217774165), ('코 앞에서 놓치다', 0.21405130869400113)]\n",
      "[('코 앞이다', 0.048294528841629526)]\n"
     ]
    }
   ],
   "source": [
    "print(tmp)\n",
    "print(find_most_similar_sentence(\"왜 남주가 벌써 내 코앞에서 굴러다니고 있는 거지?\",[\"코 앞이다\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b6b022d0-2953-4016-b6c1-9b988beba47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000\t[UNK] 이 녀석, 남자 주인공인 시그렌 아냐?! / [('곰 같은 남자', 0.1584785162583248), ('여우 같은 여자, 곰 같은 남자', 0.15241143878485672), ('차가운 도시 남자', 0.1435841559129621), ('울지 않는 녀석이 더 무섭다', 0.1261701018119691), ('개도 주인을 알아본다', 0.1257078722109418)]\n",
      "\n",
      "001\t[UNK] 원작 시작까지는 아직 6년이나 남았는데 / [('지는 해', 0.25458753860865774), ('아직 이르다', 0.18835554191923243), ('아직 멀었다', 0.17562881611387887), ('낯 가리는 아이', 0.15955176322610862), ('지는 해가 아름답다', 0.1588246128645101)]\n",
      "\n",
      "002\t[UNK] 왜 남주가 벌써 내 코앞에서 굴러다니고 있는 거지? / [('코앞에서 놓치다', 0.29725995273932915), ('뼈가 있는 말', 0.2571428571428572), ('화성에 가 있는 듯', 0.21461942290627503), ('접시는 깨라고 있는 것이다', 0.21427478217774165), ('코 앞에서 놓치다', 0.21405130869400113)]\n",
      "\n",
      "003\t[UNK] 얘가 이 시기에 아벨을 만나는 거던가? / [('곡식을 만나다', 0.2615750199185652), ('열에 아홉', 0.20447430099874028), ('홍시 먹다가 이 빠진다', 0.1932909224162628), ('하늘을 나는 기분', 0.18203380129379976), ('뛰는 놈 위에 나는 놈', 0.18169720697949243)]\n",
      "\n",
      "004\t[UNK] 아니, 지금 이런 생각 할 때가 아니지! / [('서둘러야 할 때가 아니다', 0.4276135746752218), ('찬 밥 더운 밥 가릴 때가 아니다', 0.2827158321885004), ('찬밥 더운밥 가릴 때가 아니다', 0.2816306308816227), ('늦었다고 생각할 때가 가장 빠르다', 0.2800722703026803), ('늦었다고 생각할 때가 제일 빠르다', 0.27581229840126753)]\n",
      "\n",
      "005\t[UNK] 이 녀석이 이 세계를 구원할 남주가 맞다면 / [('배가 맞다', 0.33081718197096843), ('앞뒤가 맞다', 0.2821307359086138), ('아귀가 맞다', 0.2821307359086138), ('코드가 맞다', 0.2821307359086138), ('박자가 맞다', 0.2821307359086138)]\n",
      "\n",
      "006\t[UNK] 이대로 날 구하고 사망하면 세계멸망이잖아! / [('척하면 척', 0.20537308276721192), ('쨍하고 해 뜰 날 있다', 0.20345730322374153), ('쿵하면 짝', 0.19538195982178003), ('하고 많은', 0.19538195982178003), ('쿵하면 탁', 0.19538195982178003)]\n",
      "\n",
      "007\t[UNK] 뭘 해도 내가 최종보스야?! / [('내가 낸다', 0.24193725566041463), ('천천히 해도 괜찮아', 0.2381448361039201), ('자식은 나쁜 짓을 해도 내 새끼다', 0.23502485460713568), ('거꾸로 해도 이만큼', 0.23328473740792174), ('참가만 해도 영광이다', 0.2310782580288965)]\n",
      "\n",
      "008\t[UNK] 여, 여기 빨리 의원 좀 불러 주세요! / [('빨리 죽다', 0.20447430099874028), ('기가 빨리다', 0.18835554191923243), ('신문물이 빨리 퍼지다', 0.18495753202757947), ('머리 좀 식히다', 0.17214795505974878), ('나쁜 소문은 빨리 퍼진다', 0.16859909838630924)]\n",
      "\n",
      "009\t[UNK] 이틀 뒤 / [('장사 하루 이틀 하냐?', 0.3367876570272817), ('눈이 뒤집히다', 0.15298562002754432), ('샅샅이 뒤지다', 0.15298562002754432), ('속이 뒤집히다', 0.15298562002754432), ('이잡듯 뒤지다', 0.15298562002754432)]\n",
      "\n",
      "010\t[UNK] 흐음… / [('흐리고 맑음', 0.06734350297014738), ('흐린 뒤 맑음', 0.06234796863885496), ('처음처럼', 0.041239304942116126), ('흐려지다', 0.041239304942116126), ('흐느끼다', 0.041239304942116126)]\n",
      "\n",
      "011\t[UNK] 심각한 부상이지만…. / [('백지 한 장 차이', 0.10614429987686574), ('심심한 위로', 0.10550699226799949), ('설상이 가상', 0.10550699226799949), ('상다리 부러지다', 0.10442484066397245), ('산산이 부서지다', 0.10442484066397245)]\n",
      "\n",
      "012\t[UNK] 다행히 큰 고비는 넘겼습니다. / [('흐린 물에 큰 고기', 0.2305827460539443), ('겨우 고비를 넘기다', 0.1945051735421027), ('힘든 고비를 넘기다', 0.1945051735421027), ('죽을 고비를 넘기다', 0.1945051735421027), ('통 큰 사람', 0.19245990836153545)]\n",
      "\n",
      "013\t[UNK] 정말요! / [('잘났어 정말', 0.13608276348795434), ('정말 가관이다', 0.1259881576697424), ('정말 눈물겹다', 0.1259881576697424), ('정말 눈 깜짝할 새', 0.10540925533894596), ('정말 손에 땀을 쥐다', 0.10050378152592121)]\n",
      "\n",
      "014\t[UNK] 무슨 일 있으면 불러 주세요. / [('뜻이 있으면 길이 있다', 0.3367876570272817), ('시작이 있으면 끝이 있다', 0.31477034944526894), ('음지가 있으면 양지도 있다', 0.2761699499761718), ('앉아 있으면 죽순이 돋나', 0.27314782389878706), ('가만히 있으면 중간은 간다', 0.26445780231194443)]\n",
      "\n",
      "015\t[UNK] 네! / [('동네북', 0.058321184351980436), ('동네 북', 0.050507627227610534), ('심봤다!', 0.050507627227610534), ('등이 푸네', 0.04517539514526256), ('동네북이다', 0.04517539514526256)]\n",
      "\n",
      "016\t[UNK] 다행이다~ / [('천만다행이다', 0.6411831526350159), ('화불단행이다', 0.4020979092795862), ('껌이다', 0.18442777839082936), ('무늬만 ~이다', 0.15092040263009227), ('코앞이다', 0.14907119849998596)]\n",
      "\n",
      "017\t[UNK] 세계 멸망의 위기는 넘겼어~! / [('누란의 위기', 0.3382628692414865), ('기는 놈 위에 나는 놈 있다', 0.18722213866947832), ('껍데기는 가라', 0.17908316697341958), ('남의 말 하기는 쉽다', 0.1763602071061998), ('굶어 죽기는 싫다', 0.16836734693877553)]\n",
      "\n",
      "018\t[UNK] 그나저나 이렇게 보면 / [('곱게 보다', 0.2825218471274538), ('알고 보면', 0.2825218471274538), ('길게 보면 돌고 돌아 온다', 0.2448166909753298), ('따지고 보면', 0.24266608221639882), ('가볍게 보다', 0.24266608221639882)]\n",
      "\n",
      "019\t[UNK] 세계를 구할 영웅이라며 칭송받을 거라곤 안 믿기네. / [('씹을 거리', 0.1770799390859749), ('손을 거치다', 0.15209901639120488), ('숨을 거두다', 0.15209901639120488), ('핑계를 대다', 0.15209901639120488), ('한계를 넘다', 0.15209901639120488)]\n",
      "\n",
      "020\t[UNK] 이 시기면 시그렌 나이가 열네 살쯤인가…? / [('나이가 들다', 0.2821307359086138), ('나이가 아깝다', 0.24994358597868624), ('나이가 무색하다', 0.22667947000684868), ('차이가 나다', 0.2091658904150068), ('이가 시리다', 0.2091658904150068)]\n",
      "\n",
      "021\t[UNK] 나도 간단하게 몇 줄 설정만 했을 뿐, / [('찐하게 하다', 0.18326485159709102), ('개미 새끼 하나도 얼씬 못하게 하다', 0.1801542283419025), ('간을 빼 줄 듯하다', 0.17662392292691206), ('찐하게 살다', 0.17562881611387887), ('환하게 웃다', 0.17562881611387887)]\n",
      "\n",
      "022\t[UNK] 본편에서 세세하게 다룬 적은 없는 시그렌의 과거. / [('차별 없는 세상은 없다', 0.273015873015873), ('죄 없는 사람은 없다', 0.2596197845106472), ('속에 없는 말', 0.23761503603474726), ('의리 없는 놈', 0.23761503603474726), ('차별 없는 세상', 0.23698766974773)]\n",
      "\n",
      "023\t[UNK] 시그렌은 남자 주인공답게 출생의 비밀을 가지고 있다. / [('출생의 비밀', 0.4548588261473421), ('입만 가지고 있다', 0.4064454085609962), ('놀고 있다', 0.2946137261477918), ('떨고 있다', 0.2946137261477918), ('꿰고 있다', 0.2946137261477918)]\n",
      "\n",
      "024\t[UNK] 그는 내가 현재 있는 제국의 황자다. / [('뼈가 있는 말', 0.304255531702266), ('힘 있는 자가 이긴다', 0.24956096496258426), ('힘 있는 자가 법이다', 0.24956096496258426), ('뼈대 있는 가문', 0.24954027794525985), ('화성에 가 있는 듯', 0.24131421897636143)]\n",
      "\n",
      "025\t[UNK] 정확히는, 공표되지 않은 사생아. / [('되지 않은 소리', 0.4580498866213152), ('가지 않은 길', 0.39198315480488916), ('초대받지 않은 손님', 0.3016917112499716), ('현혹되지 않다', 0.28253331287884864), ('잘 이해되지 않다', 0.24171507316070756)]\n",
      "\n",
      "026\t[UNK] 시그렌은 힘없는 평민 출신의 어머니에게서 태어났다. / [('배에서 태어났냐?', 0.2738314039148482), ('실패는 성공의 어머니', 0.2626519952688799), ('필요는 발명의 어머니', 0.2626519952688799), ('다시 태어나다', 0.16428571428571428), ('없는 살림에', 0.16312068424564002)]\n",
      "\n",
      "027\t[UNK] 그를 눈엣가시처럼 여겨 죽이고 싶어 하는 자들은 많았다. / [('눈엣가시처럼 여기다', 0.4426496200242997), ('눈엣가시', 0.3592106040535498), ('눈엣가시다', 0.29642618090449363), ('가시처럼 느껴지다', 0.2602444172224697), ('눈엣가시 같다', 0.23468614922730804)]\n",
      "\n",
      "028\t[UNK] 시그렌은 어릴 적부터 늘 목숨을 위협받으며 살아왔다. / [('이 목숨을 내놓다', 0.26575276713603596), ('목숨을 걸다', 0.26208532363727804), ('아까운 목숨을 잃다', 0.2563262862961007), ('겨우 목숨을 건지다', 0.24700533043078798), ('제 목숨을 재촉하다', 0.24700533043078798)]\n",
      "\n",
      "029\t[UNK] 시그렌은 북부의 헤일론으로 도망친다. / [('본론으로 들어가다', 0.23463502416962081), ('눈으로 먹다', 0.19300699645420138), ('떡으로 치다', 0.19300699645420138), ('산으로 가다', 0.19300699645420138), ('악으로 깡으로', 0.18593393604027367)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sens = example.split('\\n')\n",
    "cnt = 0\n",
    "for sen in sens: \n",
    "    q = sen.split('[UNK]')[1].strip()\n",
    "    idiom = extract_top_debug(q)\n",
    "    if idiom and cnt < 30:\n",
    "        print(sen,'/',idiom)\n",
    "        print()\n",
    "        cnt +=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1833eeb0-f64d-4ebb-8c9f-96887c883d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(extract_top(\"그를 눈엣가시처럼 여겨 죽이고 싶어 하는 자들은 많았다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7dcb8-07a7-4dba-8e86-1d2e0d3bedf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
