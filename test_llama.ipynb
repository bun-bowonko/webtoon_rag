{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bac7126-1417-48b0-b9df-3066f0c4cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from accelerate import Accelerator, infer_auto_device_map\n",
    "\n",
    "LLAMA_PATH = 'model/dpo-240820-ep-line-merged'\n",
    "\n",
    "# 4bit í€€íƒ€ì´ì œì´ì…˜ ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "        llm_int8_enable_fp32_cpu_offload=True  # CPU ì˜¤í”„ë¡œë”© í™œì„±í™”\n",
    ")\n",
    "# ê° GPUì— ìµœëŒ€ ë©”ëª¨ë¦¬ ì„¤ì • (ì˜ˆì‹œë¡œ 40GBì”© í• ë‹¹)\n",
    "max_memory = {i: \"40GB\" for i in range(torch.cuda.device_count())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45c68620-1797-4d75-aae5-2e37bcabaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ ì„¤ì •ì„ ë¨¼ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "config = AutoConfig.from_pretrained(LLAMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280cb594-6c13-4290-a887-4b156b8bd1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_utils import init_empty_weights  # âœ… ì˜¬ë°”ë¥¸ import\n",
    "\n",
    "# ì•„ì§ ëª¨ë¸ì„ ì„ ì–¸í•˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ `config`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹ˆ ëª¨ë¸ ìƒì„±\n",
    "# ğŸš€ ê°€ì¤‘ì¹˜ë¥¼ ì „í˜€ ë¡œë“œí•˜ì§€ ì•ŠëŠ” ì™„ì „ ë¹ˆ(empty) ëª¨ë¸ ìƒì„±\n",
    "with init_empty_weights():\n",
    "    empty_model = LlamaForCausalLM(config)\n",
    "print(\"ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8f6bfa-56e7-4497-8aa9-00e2b9cf2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# **ğŸš€ GPU 8ëŒ€ì— ê· ë“±í•˜ê²Œ ë¶„ë°°í•˜ëŠ” ìˆ˜ë™ device_map ìƒì„±**\n",
    "device_map = {}\n",
    "\n",
    "# ì„ë² ë”© & ì¶œë ¥ ë ˆì´ì–´ëŠ” GPU 0ì— ë°°ì¹˜\n",
    "device_map[\"model.embed_tokens\"] = 0\n",
    "device_map[\"lm_head\"] = 0\n",
    "\n",
    "# 126ê°œ LlamaDecoderLayerë¥¼ 8ê°œì˜ GPUì— ê· ë“± ë¶„ë°°\n",
    "for i, layer in enumerate(empty_model.model.layers):\n",
    "    assigned_gpu = i % num_gpus  # GPU ì¸ë±ìŠ¤ 0ë¶€í„° 7ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ í• ë‹¹\n",
    "    device_map[f\"model.layers.{i}\"] = assigned_gpu\n",
    "\n",
    "# ë§ˆì§€ë§‰ RMSNormë„ ë§ˆì§€ë§‰ GPUë¡œ ë°°ì¹˜\n",
    "device_map[\"model.norm\"] = 7\n",
    "device_map[\"model.rotary_emb\"] = 7  # Rotary Embeddingë„ ë§ˆì§€ë§‰ GPUë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30c0af9-6a75-4bcc-b5dc-318e4e62e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 191/191 [20:57<00:00,  6.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ë¡œë“œ (`device_map` ì ìš©)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    LLAMA_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,  # ìë™ ë¶„ë°°ëœ device_map ì ìš©\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "895a92bf-2456-4613-a513-0b31badd4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_PATH = 'model/dpo-240820-ep-line-merged'\n",
    "\n",
    "# Tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6fd4eea-6d14-4a3e-91b1-04e4bd90a94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-24 01:25:38,301] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/opt/conda/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/opt/conda/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/opt/conda/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/opt/conda/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('model/llama_405b_quantized_uint8/tokenizer_config.json',\n",
       " 'model/llama_405b_quantized_uint8/special_tokens_map.json',\n",
       " 'model/llama_405b_quantized_uint8/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('model/llama_405b_quantized_bfloat16')\n",
    "tokenizer.save_pretrained('model/llama_405b_quantized_bfloat16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce02cd5-92e0-46e6-b655-c6ed55f9ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face ê³„ì •ì—ì„œ ë°›ì€ API í† í°ì„ ì…ë ¥\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296a122-8d25-4dd3-b49b-0ce2ad6ffaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d60dee-9ee9-491e-a1c2-d007e952edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def instruct_structure(prompt,system_prompt=\n",
    "                       \"\"\"You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
    "    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
    "    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
    "    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English.\"\"\"):\n",
    "    input_text, output_text = prompt.split('### target')\n",
    "    input_text = input_text.replace('### glossaries', '### glossary').replace('\\n* ', '\\nâ€¢ ')\n",
    "    input_text = re.sub(r\"\\[[^\\]]+\\] \", \"[UNK] \", input_text)\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{input_text.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f79e1fd4-374f-444a-9318-b86cfd6afa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:00<00:00, 17618.12it/s]\n"
     ]
    }
   ],
   "source": [
    "project_id = \"prod-ai-project\"\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project_id)\n",
    "sql = \"\"\"select series_id, episode_id, org_input_text, org_output_text, prompt \n",
    "        from webtoon_translation.structured_240820_ep_line\n",
    "        where data_split = 'romance_valid'\"\"\"\n",
    "df = client.query(sql).result().to_dataframe()\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['prompt'] = df['prompt'].progress_apply(lambda x: instruct_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc15e8b-2031-42cb-b97a-14399b8c986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
      "    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
      "    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
      "    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "### glossary\n",
      "â€¢ ì•„ë²¨ í—¤ì¼ë¡  (M): abel heylon\n",
      "â€¢ ì•„ë²¨ (M): abel\n",
      "â€¢ í”¼ì˜¤ë‚˜ (F): fiona\n",
      "â€¢ ë§ˆë²•: magic / spell\n",
      "\n",
      "### source\n",
      "000\t[UNK] ë¶ë¶€ ìµœì „ë°© í—¤ì¼ë¡ \n",
      "001\t[UNK] ì—¬ê¸°ê°€ í—¤ì¼ë¡  ê³µì‘ ì„±â€¦.\n",
      "002\t[UNK] ì—„ì²­ë‚œ ìœ„ì••ê°ì´ì•¼â€¦\n",
      "003\t[UNK] ë‚˜ëŠ” ê²°êµ­ ê°€ì¡±ì—ê²Œ ë– ë°€ë ¤ ë¬´ìë¹„í•œ ê³µì‘ì´ ë‹¤ìŠ¤ë¦°ë‹¤ëŠ” ë¶ë¶€ ìµœì „ë°©ì— ì™”ë‹¤.\n",
      "004\t[UNK] ë”± ë´ë„ ë§ˆì™•ì„± ê°™ì€ ê²Œ ì‚¬ëŒ ì—„ì²­ êµ´ë¦´ ê²ƒ ê°™ë‹¤.\n",
      "005\t[UNK] ë‚´ ë¬´ë¤ì„ ë‚´ê°€ íŒ ì§€...\n",
      "006\t[UNK] ì´ìª½ìœ¼ë¡œ.\n",
      "007\t[UNK] ë¬¸ì„ ì—´ì–´ë¼.\n",
      "008\t[UNK] ë¬¸ì´, ì—„ì²­ ì»¤â€¦!!\n",
      "009\t[UNK] ë¶ë¶€ì˜ ê³µì‘, ì•„ë²¨ í—¤ì¼ë¡ \n",
      "010\t[UNK] ë„ˆ ë‚˜ë¦„ëŒ€ë¡œëŠ” ê°ì˜¤í–ˆê² ì§€. í•˜ì§€ë§Œâ€¦\n",
      "011\t[UNK] ë„ˆ ê°™ì€ ì–´ë¦°ì• ëŠ” ì—¬ê¸°ì— í•„ìš” ì—†ë‹¤.\n",
      "012\t[UNK] ì§‘ìœ¼ë¡œ ëŒì•„ê°€ë„ë¡.\n",
      "013\t[UNK] ì“¸ëª¨ì—†ëŠ” ìì—ê²Œ íˆ¬ìí•  ì‹œê°„ì€ ì—†ë‹¤ ì´ê±´ê°€.\n",
      "014\t[UNK] ì•„ë²¨ì€ ëª¨ë¥´ê² ì§€ë§Œ, ì§€ê¸ˆì˜ ë‚˜ëŠ” ë§ˆë•…íˆ ëŒì•„ê°ˆ ê³³ë„ ì—†ì–´.\n",
      "015\t[UNK] ì›ì‘ ì†ì˜, ìì‹ ì˜ ë§ˆë²•ì  ì¬ëŠ¥ì„ ëª¨ë¥´ë˜ í”¼ì˜¤ë‚˜ëŠ”-\n",
      "016\t[UNK] ì´ë ‡ê²Œ ê·¸ë¦° ê°€ë¬¸ì—ì„œë„ í—¤ì¼ë¡ ì—ì„œë„ ì«“ê²¨ë‚œë‹¤.\n",
      "017\t[UNK] ê·¸ë¦¬ê³  ì´ë•Œ ì„¸ìƒì— ë²„ë¦¼ë°›ì€ ìƒì²˜ê°€\n",
      "018\t[UNK] í›„ì— í”¼ì˜¤ë‚˜ê°€ ì•…ì—­ì´ ë˜ëŠ” ê²°ì •ì ì¸ ê³„ê¸°ê°€ ëœë‹¤.\n",
      "019\t[UNK] í•˜ì§€ë§Œ ì§€ê¸ˆì˜ ë‚˜ëŠ” ì•Œê³  ìˆì–´.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(df['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d405a225-add5-439e-9437-6b438c522625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
      "    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
      "    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
      "    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "### glossary\n",
      "â€¢ ì•„ë²¨ í—¤ì¼ë¡  (M): abel heylon\n",
      "â€¢ ì•„ë²¨ (M): abel\n",
      "â€¢ í”¼ì˜¤ë‚˜ (F): fiona\n",
      "â€¢ ë§ˆë²•: magic / spell\n",
      "\n",
      "### source\n",
      "000\t[UNK] ë¶ë¶€ ìµœì „ë°© í—¤ì¼ë¡ \n",
      "001\t[UNK] ì—¬ê¸°ê°€ í—¤ì¼ë¡  ê³µì‘ ì„±â€¦.\n",
      "002\t[UNK] ì—„ì²­ë‚œ ìœ„ì••ê°ì´ì•¼â€¦\n",
      "003\t[UNK] ë‚˜ëŠ” ê²°êµ­ ê°€ì¡±ì—ê²Œ ë– ë°€ë ¤ ë¬´ìë¹„í•œ ê³µì‘ì´ ë‹¤ìŠ¤ë¦°ë‹¤ëŠ” ë¶ë¶€ ìµœì „ë°©ì— ì™”ë‹¤.\n",
      "004\t[UNK] ë”± ë´ë„ ë§ˆì™•ì„± ê°™ì€ ê²Œ ì‚¬ëŒ ì—„ì²­ êµ´ë¦´ ê²ƒ ê°™ë‹¤.\n",
      "005\t[UNK] ë‚´ ë¬´ë¤ì„ ë‚´ê°€ íŒ ì§€...\n",
      "006\t[UNK] ì´ìª½ìœ¼ë¡œ.\n",
      "007\t[UNK] ë¬¸ì„ ì—´ì–´ë¼.\n",
      "008\t[UNK] ë¬¸ì´, ì—„ì²­ ì»¤â€¦!!\n",
      "009\t[UNK] ë¶ë¶€ì˜ ê³µì‘, ì•„ë²¨ í—¤ì¼ë¡ \n",
      "010\t[UNK] ë„ˆ ë‚˜ë¦„ëŒ€ë¡œëŠ” ê°ì˜¤í–ˆê² ì§€. í•˜ì§€ë§Œâ€¦\n",
      "011\t[UNK] ë„ˆ ê°™ì€ ì–´ë¦°ì• ëŠ” ì—¬ê¸°ì— í•„ìš” ì—†ë‹¤.\n",
      "012\t[UNK] ì§‘ìœ¼ë¡œ ëŒì•„ê°€ë„ë¡.\n",
      "013\t[UNK] ì“¸ëª¨ì—†ëŠ” ìì—ê²Œ íˆ¬ìí•  ì‹œê°„ì€ ì—†ë‹¤ ì´ê±´ê°€.\n",
      "014\t[UNK] ì•„ë²¨ì€ ëª¨ë¥´ê² ì§€ë§Œ, ì§€ê¸ˆì˜ ë‚˜ëŠ” ë§ˆë•…íˆ ëŒì•„ê°ˆ ê³³ë„ ì—†ì–´.\n",
      "015\t[UNK] ì›ì‘ ì†ì˜, ìì‹ ì˜ ë§ˆë²•ì  ì¬ëŠ¥ì„ ëª¨ë¥´ë˜ í”¼ì˜¤ë‚˜ëŠ”-\n",
      "016\t[UNK] ì´ë ‡ê²Œ ê·¸ë¦° ê°€ë¬¸ì—ì„œë„ í—¤ì¼ë¡ ì—ì„œë„ ì«“ê²¨ë‚œë‹¤.\n",
      "017\t[UNK] ê·¸ë¦¬ê³  ì´ë•Œ ì„¸ìƒì— ë²„ë¦¼ë°›ì€ ìƒì²˜ê°€\n",
      "018\t[UNK] í›„ì— í”¼ì˜¤ë‚˜ê°€ ì•…ì—­ì´ ë˜ëŠ” ê²°ì •ì ì¸ ê³„ê¸°ê°€ ëœë‹¤.\n",
      "019\t[UNK] í•˜ì§€ë§Œ ì§€ê¸ˆì˜ ë‚˜ëŠ” ì•Œê³  ìˆì–´.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "000\tthe northernmost region  haelion territory\n",
      "001\tthis is house haelionâ€™s castle...\n",
      "002\tit looks so intimidating...\n",
      "003\tIâ€™ve been forced by my family to come all this way up north where Iâ€™ll have to live under the rule of duke haelion, known for his ruthlessness.\n",
      "004\tthat place looks like a demon king's lair... theyâ€™re probably going to treat me like dirt there.\n",
      "005\thave I just dug myself into my own grave?\n",
      "006\tthis way.\n",
      "007\topen the gates!\n",
      "008\tth-the gate is huge...!!!\n",
      "009\tduke of the north abel haelion\n",
      "010\tIâ€™m guessing you came prepared,\n",
      "011\tbut we donâ€™t need someone as young as you here.\n",
      "012\tgo back home.\n",
      "013\tis that because he doesnâ€™t want to waste time on useless people? well, if only he knew...\n",
      "014\the may not know yet, but right now, I donâ€™t even have anywhere else to go.\n",
      "015\tin the original story, since fiona didnâ€™t realize she had magical powers,\n",
      "016\tshe was kicked out from both her family and house haelion.\n",
      "017\tand at that moment, when she felt abandoned by everyone around her,\n",
      "018\tfiona decided to become a villainess.\n",
      "019\tbut things are different now. I already know about my abilities. I can change everything! I wonâ€™t let anyone look down on me anymore! no one will ever call me â€œuselessâ€ again!!! I shall show them what I am truly capable of--! oh, hello there.\n",
      "020\tno more being treated like trash or getting bullied by others!\n",
      "021\tnow, I finally get to start fresh--\n",
      "022\twait, why does he seem so surprised?\n",
      "023\tdidnâ€™t he expect me to refuse him?\n",
      "024\twas he expecting me to run away crying after hearing those harsh words?\n",
      "025\tor did he think Iâ€™d try to beg him to stay?\n",
      "026\teither way, I guess I wasnâ€™t supposed to say yes.\n",
      "027\tmaybe I shouldnâ€™t have said anything until he finished talking first.\n",
      "028\toh, dear lord... please help me!\n",
      "029\twhat do you mean by â€œnoâ€? how could you possibly turn your sister away?! you must take responsibility for her!\n",
      "030\tyou were always such a good boy, abel. I thought you would understand us parents better than anyone else.\n",
      "031\twe trusted you enough to send our daughter over to you without any doubts whatsoever.\n",
      "032\thow dare you betray our trust like this?!\n",
      "033\tare you trying to make us feel guilty?!\n",
      "034\twhy arenâ€™t you saying anything? answer me, abel!\n",
      "035\thaa\n",
      "036\tahem\n",
      "037\tmy lady, welcome to haeleon manor.\n",
      "038\tplease follow me.\n",
      "039\tyes, thank you very much.\n",
      "040\tso, this is where I'll be living from now on...\n",
      "041\twhoops!\n",
      "042\toops!\n",
      "043\tthere goes another vase...\n",
      "044\tgeez, these maids really suck at their jobs.\n",
      "045\tthey keep breaking stuff every day.\n",
      "046\tI wonder if theyâ€™re doing it on purpose.\n",
      "047\tcome to think of it, maybe they hate working here too.\n",
      "048\tanyway, I'm glad I got to see some new faces today.\n",
      "049\tI hope they last longer than the previous ones.\n",
      "050\tgood afternoon, my lady.\n",
      "051\tafternoon.\n",
      "052\tyour grace has requested your presence.\n",
      "053\tthank you for letting me know.\n",
      "054\there you are.\n",
      "055\tyou wanted to speak with me, your grace?\n",
      "056\tyes, sit down.\n",
      "057\tso, tell me...\n",
      "058\thow long do you plan on staying here?\n",
      "059\tas long as possible.\n",
      "060\twhat?\n",
      "061\tuntil I die, preferably.\n",
      "062\twh-what?!\n",
      "063\tjust kidding.\n",
      "064\tphew...\n",
      "065\talthough, I wouldnâ€™t mind dying here either.\n",
      "066\twhat kind of joke is that?!\n",
      "067\tseriously though, I'd love to stay here forever.\n",
      "068\tâ€œforever,â€ huh...?\n",
      "069\tyeah. I promise I wonâ€™t cause any trouble.\n",
      "070\tyou might find it hard to believe, but I actually grew quite fond of this place during my short visit.\n",
      "071\talso, I heard rumors that you werenâ€™t exactly happy about having me here.\n",
      "072\tso I thought Iâ€™d ask you personally whether you still wish to kick me out or not.\n",
      "073\tif you do, then Iâ€™ll leave immediately.\n",
      "074\thowever, if you allow me to stay, I swear Iâ€™ll never disappoint you.\n",
      "075\tIâ€™ll work harder than anyone else to prove myself worthy of becoming part of this household.\n",
      "076\tI-I see...\n",
      "077\tyou certainly sound sincere.\n",
      "078\tvery well, then.\n",
      "079\tI accept your offer.\n",
      "080\treally?!\n",
      "081\tyes,\n",
      "514.56\n"
     ]
    }
   ],
   "source": [
    "sample = df['prompt'][0]\n",
    "\n",
    "# ì…ë ¥ ë³€í™˜ ë° í† í°í™”\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\").to(model.device)\n",
    "import time\n",
    "start_time = time.time()\n",
    "# ëª¨ë¸ ì¶”ë¡ \n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, \n",
    "                            max_new_tokens=1000,#ì…ë ¥ ê¸¸ì´ì™€ ë¬´ê´€í•˜ê²Œ ì¶œë ¥ê¸¸ì´ë§Œ ì œí•œ\n",
    "                            do_sample=True,\n",
    "                            temperature=0.1,\n",
    "                            top_p=0.9,\n",
    "                            top_k=30,\n",
    "                            repetition_penalty=1.2,\n",
    "                            use_cache=True #kv cache\n",
    "                           )\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "print(response)\n",
    "print(f\"{time.time()-start_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d42379-512d-404f-836b-bdaa278a1129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8ccdb-d0ce-46c4-b53c-44935c8bd314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4df90-eee0-49d9-b745-0e9adad80a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
