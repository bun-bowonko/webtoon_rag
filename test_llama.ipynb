{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bac7126-1417-48b0-b9df-3066f0c4cc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from accelerate import Accelerator, infer_auto_device_map\n",
    "\n",
    "LLAMA_PATH = 'model/llama_405b_quantized'\n",
    "\n",
    "# 4bit í€€íƒ€ì´ì œì´ì…˜ ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "        llm_int8_enable_fp32_cpu_offload=True  # CPU ì˜¤í”„ë¡œë”© í™œì„±í™”\n",
    ")\n",
    "# ê° GPUì— ìµœëŒ€ ë©”ëª¨ë¦¬ ì„¤ì • (ì˜ˆì‹œë¡œ 40GBì”© í• ë‹¹)\n",
    "max_memory = {i: \"40GB\" for i in range(torch.cuda.device_count())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c68620-1797-4d75-aae5-2e37bcabaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ ì„¤ì •ì„ ë¨¼ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "config = AutoConfig.from_pretrained(LLAMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280cb594-6c13-4290-a887-4b156b8bd1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_utils import init_empty_weights  # âœ… ì˜¬ë°”ë¥¸ import\n",
    "\n",
    "# ì•„ì§ ëª¨ë¸ì„ ì„ ì–¸í•˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ `config`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹ˆ ëª¨ë¸ ìƒì„±\n",
    "# ğŸš€ ê°€ì¤‘ì¹˜ë¥¼ ì „í˜€ ë¡œë“œí•˜ì§€ ì•ŠëŠ” ì™„ì „ ë¹ˆ(empty) ëª¨ë¸ ìƒì„±\n",
    "with init_empty_weights():\n",
    "    empty_model = LlamaForCausalLM(config)\n",
    "print(\"ë¹ˆ ëª¨ë¸ ì„ ì–¸ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8f6bfa-56e7-4497-8aa9-00e2b9cf2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# **ğŸš€ GPU 8ëŒ€ì— ê· ë“±í•˜ê²Œ ë¶„ë°°í•˜ëŠ” ìˆ˜ë™ device_map ìƒì„±**\n",
    "device_map = {}\n",
    "\n",
    "# ì„ë² ë”© & ì¶œë ¥ ë ˆì´ì–´ëŠ” GPU 0ì— ë°°ì¹˜\n",
    "device_map[\"model.embed_tokens\"] = 7\n",
    "device_map[\"lm_head\"] = 7\n",
    "\n",
    "# 126ê°œ LlamaDecoderLayerë¥¼ 8ê°œì˜ GPUì— ê· ë“± ë¶„ë°°\n",
    "for i, layer in enumerate(empty_model.model.layers):\n",
    "    assigned_gpu = i % num_gpus  # GPU ì¸ë±ìŠ¤ 0ë¶€í„° 7ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ í• ë‹¹\n",
    "    device_map[f\"model.layers.{i}\"] = assigned_gpu\n",
    "\n",
    "# ë§ˆì§€ë§‰ RMSNormë„ ë§ˆì§€ë§‰ GPUë¡œ ë°°ì¹˜\n",
    "device_map[\"model.norm\"] = 7\n",
    "device_map[\"model.rotary_emb\"] = 7  # Rotary Embeddingë„ ë§ˆì§€ë§‰ GPUë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3624c46-eecd-44c5-b334-ab007544c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 7, 'lm_head': 7, 'model.layers.0': 0, 'model.layers.1': 1, 'model.layers.2': 2, 'model.layers.3': 3, 'model.layers.4': 4, 'model.layers.5': 5, 'model.layers.6': 6, 'model.layers.7': 7, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 2, 'model.layers.11': 3, 'model.layers.12': 4, 'model.layers.13': 5, 'model.layers.14': 6, 'model.layers.15': 7, 'model.layers.16': 0, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 3, 'model.layers.20': 4, 'model.layers.21': 5, 'model.layers.22': 6, 'model.layers.23': 7, 'model.layers.24': 0, 'model.layers.25': 1, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 4, 'model.layers.29': 5, 'model.layers.30': 6, 'model.layers.31': 7, 'model.layers.32': 0, 'model.layers.33': 1, 'model.layers.34': 2, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 5, 'model.layers.38': 6, 'model.layers.39': 7, 'model.layers.40': 0, 'model.layers.41': 1, 'model.layers.42': 2, 'model.layers.43': 3, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 6, 'model.layers.47': 7, 'model.layers.48': 0, 'model.layers.49': 1, 'model.layers.50': 2, 'model.layers.51': 3, 'model.layers.52': 4, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 7, 'model.layers.56': 0, 'model.layers.57': 1, 'model.layers.58': 2, 'model.layers.59': 3, 'model.layers.60': 4, 'model.layers.61': 5, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 0, 'model.layers.65': 1, 'model.layers.66': 2, 'model.layers.67': 3, 'model.layers.68': 4, 'model.layers.69': 5, 'model.layers.70': 6, 'model.layers.71': 7, 'model.layers.72': 0, 'model.layers.73': 1, 'model.layers.74': 2, 'model.layers.75': 3, 'model.layers.76': 4, 'model.layers.77': 5, 'model.layers.78': 6, 'model.layers.79': 7, 'model.layers.80': 0, 'model.layers.81': 1, 'model.layers.82': 2, 'model.layers.83': 3, 'model.layers.84': 4, 'model.layers.85': 5, 'model.layers.86': 6, 'model.layers.87': 7, 'model.layers.88': 0, 'model.layers.89': 1, 'model.layers.90': 2, 'model.layers.91': 3, 'model.layers.92': 4, 'model.layers.93': 5, 'model.layers.94': 6, 'model.layers.95': 7, 'model.layers.96': 0, 'model.layers.97': 1, 'model.layers.98': 2, 'model.layers.99': 3, 'model.layers.100': 4, 'model.layers.101': 5, 'model.layers.102': 6, 'model.layers.103': 7, 'model.layers.104': 0, 'model.layers.105': 1, 'model.layers.106': 2, 'model.layers.107': 3, 'model.layers.108': 4, 'model.layers.109': 5, 'model.layers.110': 6, 'model.layers.111': 7, 'model.layers.112': 0, 'model.layers.113': 1, 'model.layers.114': 2, 'model.layers.115': 3, 'model.layers.116': 4, 'model.layers.117': 5, 'model.layers.118': 6, 'model.layers.119': 7, 'model.layers.120': 0, 'model.layers.121': 1, 'model.layers.122': 2, 'model.layers.123': 3, 'model.layers.124': 4, 'model.layers.125': 5, 'model.norm': 7, 'model.rotary_emb': 7}\n"
     ]
    }
   ],
   "source": [
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30c0af9-6a75-4bcc-b5dc-318e4e62e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [01:14<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ë¡œë“œ (`device_map` ì ìš©)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    LLAMA_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,  # ìë™ ë¶„ë°°ëœ device_map ì ìš©\n",
    "    #attn_implementation=\"flash_attention_2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8eb4596-942e-4470-956d-821694ff9e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 16384)\n",
      "    (layers): ModuleList(\n",
      "      (0-125): 126 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=16384, out_features=16384, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=16384, out_features=16384, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=16384, out_features=53248, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=16384, out_features=53248, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=53248, out_features=16384, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((16384,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((16384,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((16384,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=16384, out_features=128256, bias=False)\n",
      ")\n",
      "cuda:7\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b9280cf-c6d1-49af-99e2-e7ab69c39369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained('model/llama_405b_quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "895a92bf-2456-4613-a513-0b31badd4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_PATH = 'model/llama_405b_quantized'\n",
    "\n",
    "# Tokenizer ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "#tokenizer.save_pretrained('model/llama_405b_quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d60dee-9ee9-491e-a1c2-d007e952edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def instruct_structure(prompt,system_prompt=\n",
    "                       \"\"\"You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
    "    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
    "    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
    "    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English.\"\"\"):\n",
    "    input_text, output_text = prompt.split('### target')\n",
    "    input_text = input_text.replace('### glossaries', '### glossary').replace('\\n* ', '\\nâ€¢ ')\n",
    "    input_text = re.sub(r\"\\[[^\\]]+\\] \", \"none \", input_text)\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {input_text.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f79e1fd4-374f-444a-9318-b86cfd6afa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:00<00:00, 17682.36it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "project_id = \"prod-ai-project\"\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project_id)\n",
    "sql = \"\"\"select series_id, episode_id, org_input_text, org_output_text, prompt \n",
    "        from webtoon_translation.structured_240820_ep_line\n",
    "        where data_split = 'romance_valid'\"\"\"\n",
    "df = client.query(sql).result().to_dataframe()\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['prompt'] = df['prompt'].progress_apply(lambda x: instruct_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc15e8b-2031-42cb-b97a-14399b8c986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "    You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
      "    â€¢ Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
      "    â€¢ Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
      "    â€¢ Translate with an American audience in mind. This means easy-to-read, conversational English.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "    ### glossary\n",
      "â€¢ ì•„ë²¨ í—¤ì¼ë¡  (M): abel heylon\n",
      "â€¢ ì•„ë²¨ (M): abel\n",
      "â€¢ í”¼ì˜¤ë‚˜ (F): fiona\n",
      "â€¢ ë§ˆë²•: magic / spell\n",
      "\n",
      "### source\n",
      "000\tnone ë¶ë¶€ ìµœì „ë°© í—¤ì¼ë¡ \n",
      "001\tnone ì—¬ê¸°ê°€ í—¤ì¼ë¡  ê³µì‘ ì„±â€¦.\n",
      "002\tnone ì—„ì²­ë‚œ ìœ„ì••ê°ì´ì•¼â€¦\n",
      "003\tnone ë‚˜ëŠ” ê²°êµ­ ê°€ì¡±ì—ê²Œ ë– ë°€ë ¤ ë¬´ìë¹„í•œ ê³µì‘ì´ ë‹¤ìŠ¤ë¦°ë‹¤ëŠ” ë¶ë¶€ ìµœì „ë°©ì— ì™”ë‹¤.\n",
      "004\tnone ë”± ë´ë„ ë§ˆì™•ì„± ê°™ì€ ê²Œ ì‚¬ëŒ ì—„ì²­ êµ´ë¦´ ê²ƒ ê°™ë‹¤.\n",
      "005\tnone ë‚´ ë¬´ë¤ì„ ë‚´ê°€ íŒ ì§€...\n",
      "006\tnone ì´ìª½ìœ¼ë¡œ.\n",
      "007\tnone ë¬¸ì„ ì—´ì–´ë¼.\n",
      "008\tnone ë¬¸ì´, ì—„ì²­ ì»¤â€¦!!\n",
      "009\tnone ë¶ë¶€ì˜ ê³µì‘, ì•„ë²¨ í—¤ì¼ë¡ \n",
      "010\tnone ë„ˆ ë‚˜ë¦„ëŒ€ë¡œëŠ” ê°ì˜¤í–ˆê² ì§€. í•˜ì§€ë§Œâ€¦\n",
      "011\tnone ë„ˆ ê°™ì€ ì–´ë¦°ì• ëŠ” ì—¬ê¸°ì— í•„ìš” ì—†ë‹¤.\n",
      "012\tnone ì§‘ìœ¼ë¡œ ëŒì•„ê°€ë„ë¡.\n",
      "013\tnone ì“¸ëª¨ì—†ëŠ” ìì—ê²Œ íˆ¬ìí•  ì‹œê°„ì€ ì—†ë‹¤ ì´ê±´ê°€.\n",
      "014\tnone ì•„ë²¨ì€ ëª¨ë¥´ê² ì§€ë§Œ, ì§€ê¸ˆì˜ ë‚˜ëŠ” ë§ˆë•…íˆ ëŒì•„ê°ˆ ê³³ë„ ì—†ì–´.\n",
      "015\tnone ì›ì‘ ì†ì˜, ìì‹ ì˜ ë§ˆë²•ì  ì¬ëŠ¥ì„ ëª¨ë¥´ë˜ í”¼ì˜¤ë‚˜ëŠ”-\n",
      "016\tnone ì´ë ‡ê²Œ ê·¸ë¦° ê°€ë¬¸ì—ì„œë„ í—¤ì¼ë¡ ì—ì„œë„ ì«“ê²¨ë‚œë‹¤.\n",
      "017\tnone ê·¸ë¦¬ê³  ì´ë•Œ ì„¸ìƒì— ë²„ë¦¼ë°›ì€ ìƒì²˜ê°€\n",
      "018\tnone í›„ì— í”¼ì˜¤ë‚˜ê°€ ì•…ì—­ì´ ë˜ëŠ” ê²°ì •ì ì¸ ê³„ê¸°ê°€ ëœë‹¤.\n",
      "019\tnone í•˜ì§€ë§Œ ì§€ê¸ˆì˜ ë‚˜ëŠ” ì•Œê³  ìˆì–´.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(df['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "159c1063-064a-4c1e-905e-7ff185a09caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128000, 128006,   9125, 128007,    198,    262,   1472,   2351,\n",
      "            459,   6335,  46588,    889,  48018,  16526,   3566,    998,    263,\n",
      "            304,   6498,     13,   7557,   2771,    279,   1396,    315,   2218,\n",
      "          23719,   9248,    279,   1396,    315,   2592,  23719,     13,    578,\n",
      "           1121,   1288,    387,    350,  18282,  24001,     13,    720,    262,\n",
      "           7436,   7531,    264,   8335,   1990,  19994,    837,    311,    279,\n",
      "          16526,   7438,    323,  10494,    264,   5933,   6530,     13,   4418,\n",
      "            956,    387,  16984,    311,    923,    311,    279,   1495,     13,\n",
      "          30227,    616,    819,    433,     13,    720,    262,   7436,  35106,\n",
      "          67371,   3492,  15548,  38428,     13,  13969,    279,   4689,   8430,\n",
      "            323,  15025,    279,   1495,  28178,     13,    720,    262,   7436,\n",
      "          38840,    449,    459,   3778,  10877,    304,   4059,     13,   1115,\n",
      "           3445,   4228,   4791,  29906,     11,   7669,   1697,   6498,     13,\n",
      "         128009, 128006,    882, 128007,    198,    262,  17010,  36451,    661,\n",
      "            198,   6806,  49508, 103542, 112784,  33177, 103778,    320,     44,\n",
      "           1680,    671,    301,    568,  27095,    198,   6806,  49508, 103542,\n",
      "            320,     44,   1680,    671,    301,    198,   6806, 104064,  58368,\n",
      "          61415,    320,     37,   1680,    282,  42790,    198,   6806, 127388,\n",
      "             25,  11204,    611,  13141,    271,  14711,   2592,    198,    931,\n",
      "            197,   6836, 108772,  64189,  82273,  66965, 101482, 112784,  33177,\n",
      "         103778,    198,   4119,    197,   6836,  84618, 109957, 112784,  33177,\n",
      "         103778, 100994,  68611, 102132,   1981,    627,   6726,    197,   6836,\n",
      "         112190, 102039, 103777,  46810, 116859, 103655, 115242,  90578,   6268,\n",
      "            197,   6836, 109275,  83719, 100654, 120122, 102244, 111519, 105711,\n",
      "         101103, 101480,  26799,  71682,  24486, 100994,  68611,  13094,  50467,\n",
      "          25941, 102423, 105453, 108772,  64189,  82273,  66965, 101482,  19954,\n",
      "          75984, 104828,    627,   8759,    197,   6836,  77597,    109, 122875,\n",
      "          49085,  96677, 108668,  33931, 105718, 100027, 102745, 112190, 102039,\n",
      "         100487,    112, 109883,  72208, 127964,    627,   8504,    197,   6836,\n",
      "          67236, 101480,  69697,     97,  18359, 109723,  46204,    254,  22035,\n",
      "           9522,  11030,    197,   6836,  23955, 105678,  43139,    627,  11194,\n",
      "            197,   6836,  54535,  18359, 105069,  32179,  51440,    627,  11436,\n",
      "            197,   6836,  54535,  13094,     11, 112190, 102039, 108742,   1981,\n",
      "          51447,  13858,    197,   6836, 108772,  64189,  21028, 100994,  68611,\n",
      "             11,  49508, 103542, 112784,  33177, 103778,    198,   7755,    197,\n",
      "           6836, 105250,  74618,  64254, 106687,  16969, 106603,  36092,  45780,\n",
      "            244,    230, 103373,  22035,     13, 110473,  90578,  10731,    197,\n",
      "           6836, 105250, 105718, 101139, 102423, 105851,  16969,  84618, 109509,\n",
      "         108289, 118318,    627,  11531,    197,   6836, 104441,  43139, 110110,\n",
      "          20565, 108438,    627,  16368,    197,   6836, 120657,    116, 101555,\n",
      "         122820,  65677, 102244, 107896,  26799,  48936, 106243,  34804, 118318,\n",
      "          23955, 101868,  20565,    627,  15901,    197,   6836,  49508, 103542,\n",
      "          34804, 117215, 103373, 102077,     11, 109296,  21028, 109275,  96677,\n",
      "            167,    243,    227, 101709, 110110, 111282, 111003,  49085, 126781,\n",
      "            627,  16037,    197,   6836, 102467,  68611, 105220,  21028,     11,\n",
      "         115968, 127388,  82068, 102888,  67119,  18359, 117215, 101954, 104064,\n",
      "          58368, 110955,   7058,  15794,    197,   6836, 116816,  55925, 102423,\n",
      "          36609,  52688, 121048, 112784,  33177, 103778, 121048,   3396,    104,\n",
      "            241, 108381, 103777,  13447,    627,  17248,    197,   6836, 107536,\n",
      "          23955, 106745, 127784,  19954,  87931, 103338, 107094,  34804,  59134,\n",
      "         102657,  20565,    198,  16745,    197,   6836,  95415,  19954, 104064,\n",
      "          58368,  61415,  20565, 115809, 101577,  13094, 116654, 121469, 103684,\n",
      "          95303, 109957, 119068,    627,  18089,    197,   6836, 110473, 109296,\n",
      "          21028, 109275, 115877, 112795,     13, 128009, 128006,  78191, 128007]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "sample = df['prompt'][0]\n",
    "\n",
    "# ì…ë ¥ ë³€í™˜ ë° í† í°í™”\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d405a225-add5-439e-9437-6b438c522625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "../aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ëª¨ë¸ ì¶”ë¡ \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# ê²°ê³¼ ì¶œë ¥\u001b[39;00m\n\u001b[1;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3256\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3257\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3259\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ëª¨ë¸ ì¶”ë¡ \n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, \n",
    "                            max_length=1000,\n",
    "                            repetition_penalty=1.2\n",
    "                           )\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8ccdb-d0ce-46c4-b53c-44935c8bd314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4df90-eee0-49d9-b745-0e9adad80a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
