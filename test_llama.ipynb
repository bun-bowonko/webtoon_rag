{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bac7126-1417-48b0-b9df-3066f0c4cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from accelerate import Accelerator, infer_auto_device_map\n",
    "\n",
    "LLAMA_PATH = 'model/dpo-240820-ep-line-merged'\n",
    "\n",
    "# 4bit 퀀타이제이션 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "        llm_int8_enable_fp32_cpu_offload=True  # CPU 오프로딩 활성화\n",
    ")\n",
    "# 각 GPU에 최대 메모리 설정 (예시로 40GB씩 할당)\n",
    "max_memory = {i: \"40GB\" for i in range(torch.cuda.device_count())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45c68620-1797-4d75-aae5-2e37bcabaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 설정을 먼저 불러오기\n",
    "config = AutoConfig.from_pretrained(LLAMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "280cb594-6c13-4290-a887-4b156b8bd1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈 모델 선언 완료\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_utils import init_empty_weights  # ✅ 올바른 import\n",
    "\n",
    "# 아직 모델을 선언하지 않은 상태에서 `config`를 기반으로 빈 모델 생성\n",
    "# 🚀 가중치를 전혀 로드하지 않는 완전 빈(empty) 모델 생성\n",
    "with init_empty_weights():\n",
    "    empty_model = LlamaForCausalLM(config)\n",
    "print(\"빈 모델 선언 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8f6bfa-56e7-4497-8aa9-00e2b9cf2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "# **🚀 GPU 8대에 균등하게 분배하는 수동 device_map 생성**\n",
    "device_map = {}\n",
    "\n",
    "# 임베딩 & 출력 레이어는 GPU 0에 배치\n",
    "device_map[\"model.embed_tokens\"] = 0\n",
    "device_map[\"lm_head\"] = 0\n",
    "\n",
    "# 126개 LlamaDecoderLayer를 8개의 GPU에 균등 분배\n",
    "for i, layer in enumerate(empty_model.model.layers):\n",
    "    assigned_gpu = i % num_gpus  # GPU 인덱스 0부터 7까지 순차적으로 할당\n",
    "    device_map[f\"model.layers.{i}\"] = assigned_gpu\n",
    "\n",
    "# 마지막 RMSNorm도 마지막 GPU로 배치\n",
    "device_map[\"model.norm\"] = 7\n",
    "device_map[\"model.rotary_emb\"] = 7  # Rotary Embedding도 마지막 GPU로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30c0af9-6a75-4bcc-b5dc-318e4e62e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████| 191/191 [20:57<00:00,  6.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드 (`device_map` 적용)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    LLAMA_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,  # 자동 분배된 device_map 적용\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "895a92bf-2456-4613-a513-0b31badd4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_PATH = 'model/dpo-240820-ep-line-merged'\n",
    "\n",
    "# Tokenizer 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6fd4eea-6d14-4a3e-91b1-04e4bd90a94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-24 01:25:38,301] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/opt/conda/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/opt/conda/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/opt/conda/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/opt/conda/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('model/llama_405b_quantized_uint8/tokenizer_config.json',\n",
       " 'model/llama_405b_quantized_uint8/special_tokens_map.json',\n",
       " 'model/llama_405b_quantized_uint8/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('model/llama_405b_quantized_bfloat16')\n",
    "tokenizer.save_pretrained('model/llama_405b_quantized_bfloat16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce02cd5-92e0-46e6-b655-c6ed55f9ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face 계정에서 받은 API 토큰을 입력\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296a122-8d25-4dd3-b49b-0ce2ad6ffaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d60dee-9ee9-491e-a1c2-d007e952edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def instruct_structure(prompt,system_prompt=\n",
    "                       \"\"\"You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
    "    • Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
    "    • Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
    "    • Translate with an American audience in mind. This means easy-to-read, conversational English.\"\"\"):\n",
    "    input_text, output_text = prompt.split('### target')\n",
    "    input_text = input_text.replace('### glossaries', '### glossary').replace('\\n* ', '\\n• ')\n",
    "    input_text = re.sub(r\"\\[[^\\]]+\\] \", \"[UNK] \", input_text)\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{input_text.strip()}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f79e1fd4-374f-444a-9318-b86cfd6afa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bun.2/.cache/pypoetry/virtualenvs/poetry-env-eDEwtiIl-py3.10/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:00<00:00, 17618.12it/s]\n"
     ]
    }
   ],
   "source": [
    "project_id = \"prod-ai-project\"\n",
    "\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(project=project_id)\n",
    "sql = \"\"\"select series_id, episode_id, org_input_text, org_output_text, prompt \n",
    "        from webtoon_translation.structured_240820_ep_line\n",
    "        where data_split = 'romance_valid'\"\"\"\n",
    "df = client.query(sql).result().to_dataframe()\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['prompt'] = df['prompt'].progress_apply(lambda x: instruct_structure(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc15e8b-2031-42cb-b97a-14399b8c986f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
      "    • Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
      "    • Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
      "    • Translate with an American audience in mind. This means easy-to-read, conversational English.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "### glossary\n",
      "• 아벨 헤일론 (M): abel heylon\n",
      "• 아벨 (M): abel\n",
      "• 피오나 (F): fiona\n",
      "• 마법: magic / spell\n",
      "\n",
      "### source\n",
      "000\t[UNK] 북부 최전방 헤일론\n",
      "001\t[UNK] 여기가 헤일론 공작 성….\n",
      "002\t[UNK] 엄청난 위압감이야…\n",
      "003\t[UNK] 나는 결국 가족에게 떠밀려 무자비한 공작이 다스린다는 북부 최전방에 왔다.\n",
      "004\t[UNK] 딱 봐도 마왕성 같은 게 사람 엄청 굴릴 것 같다.\n",
      "005\t[UNK] 내 무덤을 내가 팠지...\n",
      "006\t[UNK] 이쪽으로.\n",
      "007\t[UNK] 문을 열어라.\n",
      "008\t[UNK] 문이, 엄청 커…!!\n",
      "009\t[UNK] 북부의 공작, 아벨 헤일론\n",
      "010\t[UNK] 너 나름대로는 각오했겠지. 하지만…\n",
      "011\t[UNK] 너 같은 어린애는 여기에 필요 없다.\n",
      "012\t[UNK] 집으로 돌아가도록.\n",
      "013\t[UNK] 쓸모없는 자에게 투자할 시간은 없다 이건가.\n",
      "014\t[UNK] 아벨은 모르겠지만, 지금의 나는 마땅히 돌아갈 곳도 없어.\n",
      "015\t[UNK] 원작 속의, 자신의 마법적 재능을 모르던 피오나는-\n",
      "016\t[UNK] 이렇게 그린 가문에서도 헤일론에서도 쫓겨난다.\n",
      "017\t[UNK] 그리고 이때 세상에 버림받은 상처가\n",
      "018\t[UNK] 후에 피오나가 악역이 되는 결정적인 계기가 된다.\n",
      "019\t[UNK] 하지만 지금의 나는 알고 있어.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(df['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d405a225-add5-439e-9437-6b438c522625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You're an expert translator who translates Korean webtoon in English. Make sure the number of target sentences matches the number of source sentences. The result should be TSV formatted. \n",
      "    • Find a balance between staying true to the Korean meaning and keeping a natural flow. Don't be afraid to add to the text. Embellish it. \n",
      "    • Avoid translating word-for-word. Keep the general feeling and translate the text accordingly. \n",
      "    • Translate with an American audience in mind. This means easy-to-read, conversational English.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "### glossary\n",
      "• 아벨 헤일론 (M): abel heylon\n",
      "• 아벨 (M): abel\n",
      "• 피오나 (F): fiona\n",
      "• 마법: magic / spell\n",
      "\n",
      "### source\n",
      "000\t[UNK] 북부 최전방 헤일론\n",
      "001\t[UNK] 여기가 헤일론 공작 성….\n",
      "002\t[UNK] 엄청난 위압감이야…\n",
      "003\t[UNK] 나는 결국 가족에게 떠밀려 무자비한 공작이 다스린다는 북부 최전방에 왔다.\n",
      "004\t[UNK] 딱 봐도 마왕성 같은 게 사람 엄청 굴릴 것 같다.\n",
      "005\t[UNK] 내 무덤을 내가 팠지...\n",
      "006\t[UNK] 이쪽으로.\n",
      "007\t[UNK] 문을 열어라.\n",
      "008\t[UNK] 문이, 엄청 커…!!\n",
      "009\t[UNK] 북부의 공작, 아벨 헤일론\n",
      "010\t[UNK] 너 나름대로는 각오했겠지. 하지만…\n",
      "011\t[UNK] 너 같은 어린애는 여기에 필요 없다.\n",
      "012\t[UNK] 집으로 돌아가도록.\n",
      "013\t[UNK] 쓸모없는 자에게 투자할 시간은 없다 이건가.\n",
      "014\t[UNK] 아벨은 모르겠지만, 지금의 나는 마땅히 돌아갈 곳도 없어.\n",
      "015\t[UNK] 원작 속의, 자신의 마법적 재능을 모르던 피오나는-\n",
      "016\t[UNK] 이렇게 그린 가문에서도 헤일론에서도 쫓겨난다.\n",
      "017\t[UNK] 그리고 이때 세상에 버림받은 상처가\n",
      "018\t[UNK] 후에 피오나가 악역이 되는 결정적인 계기가 된다.\n",
      "019\t[UNK] 하지만 지금의 나는 알고 있어.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "000\tthe northernmost region  haelion territory\n",
      "001\tthis is house haelion’s castle...\n",
      "002\tit looks so intimidating...\n",
      "003\tI’ve been forced by my family to come all this way up north where I’ll have to live under the rule of duke haelion, known for his ruthlessness.\n",
      "004\tthat place looks like a demon king's lair... they’re probably going to treat me like dirt there.\n",
      "005\thave I just dug myself into my own grave?\n",
      "006\tthis way.\n",
      "007\topen the gates!\n",
      "008\tth-the gate is huge...!!!\n",
      "009\tduke of the north abel haelion\n",
      "010\tI’m guessing you came prepared,\n",
      "011\tbut we don’t need someone as young as you here.\n",
      "012\tgo back home.\n",
      "013\tis that because he doesn’t want to waste time on useless people? well, if only he knew...\n",
      "014\the may not know yet, but right now, I don’t even have anywhere else to go.\n",
      "015\tin the original story, since fiona didn’t realize she had magical powers,\n",
      "016\tshe was kicked out from both her family and house haelion.\n",
      "017\tand at that moment, when she felt abandoned by everyone around her,\n",
      "018\tfiona decided to become a villainess.\n",
      "019\tbut things are different now. I already know about my abilities. I can change everything! I won’t let anyone look down on me anymore! no one will ever call me “useless” again!!! I shall show them what I am truly capable of--! oh, hello there.\n",
      "020\tno more being treated like trash or getting bullied by others!\n",
      "021\tnow, I finally get to start fresh--\n",
      "022\twait, why does he seem so surprised?\n",
      "023\tdidn’t he expect me to refuse him?\n",
      "024\twas he expecting me to run away crying after hearing those harsh words?\n",
      "025\tor did he think I’d try to beg him to stay?\n",
      "026\teither way, I guess I wasn’t supposed to say yes.\n",
      "027\tmaybe I shouldn’t have said anything until he finished talking first.\n",
      "028\toh, dear lord... please help me!\n",
      "029\twhat do you mean by “no”? how could you possibly turn your sister away?! you must take responsibility for her!\n",
      "030\tyou were always such a good boy, abel. I thought you would understand us parents better than anyone else.\n",
      "031\twe trusted you enough to send our daughter over to you without any doubts whatsoever.\n",
      "032\thow dare you betray our trust like this?!\n",
      "033\tare you trying to make us feel guilty?!\n",
      "034\twhy aren’t you saying anything? answer me, abel!\n",
      "035\thaa\n",
      "036\tahem\n",
      "037\tmy lady, welcome to haeleon manor.\n",
      "038\tplease follow me.\n",
      "039\tyes, thank you very much.\n",
      "040\tso, this is where I'll be living from now on...\n",
      "041\twhoops!\n",
      "042\toops!\n",
      "043\tthere goes another vase...\n",
      "044\tgeez, these maids really suck at their jobs.\n",
      "045\tthey keep breaking stuff every day.\n",
      "046\tI wonder if they’re doing it on purpose.\n",
      "047\tcome to think of it, maybe they hate working here too.\n",
      "048\tanyway, I'm glad I got to see some new faces today.\n",
      "049\tI hope they last longer than the previous ones.\n",
      "050\tgood afternoon, my lady.\n",
      "051\tafternoon.\n",
      "052\tyour grace has requested your presence.\n",
      "053\tthank you for letting me know.\n",
      "054\there you are.\n",
      "055\tyou wanted to speak with me, your grace?\n",
      "056\tyes, sit down.\n",
      "057\tso, tell me...\n",
      "058\thow long do you plan on staying here?\n",
      "059\tas long as possible.\n",
      "060\twhat?\n",
      "061\tuntil I die, preferably.\n",
      "062\twh-what?!\n",
      "063\tjust kidding.\n",
      "064\tphew...\n",
      "065\talthough, I wouldn’t mind dying here either.\n",
      "066\twhat kind of joke is that?!\n",
      "067\tseriously though, I'd love to stay here forever.\n",
      "068\t“forever,” huh...?\n",
      "069\tyeah. I promise I won’t cause any trouble.\n",
      "070\tyou might find it hard to believe, but I actually grew quite fond of this place during my short visit.\n",
      "071\talso, I heard rumors that you weren’t exactly happy about having me here.\n",
      "072\tso I thought I’d ask you personally whether you still wish to kick me out or not.\n",
      "073\tif you do, then I’ll leave immediately.\n",
      "074\thowever, if you allow me to stay, I swear I’ll never disappoint you.\n",
      "075\tI’ll work harder than anyone else to prove myself worthy of becoming part of this household.\n",
      "076\tI-I see...\n",
      "077\tyou certainly sound sincere.\n",
      "078\tvery well, then.\n",
      "079\tI accept your offer.\n",
      "080\treally?!\n",
      "081\tyes,\n",
      "514.56\n"
     ]
    }
   ],
   "source": [
    "sample = df['prompt'][0]\n",
    "\n",
    "# 입력 변환 및 토큰화\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\").to(model.device)\n",
    "import time\n",
    "start_time = time.time()\n",
    "# 모델 추론\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, \n",
    "                            max_new_tokens=1000,#입력 길이와 무관하게 출력길이만 제한\n",
    "                            do_sample=True,\n",
    "                            temperature=0.1,\n",
    "                            top_p=0.9,\n",
    "                            top_k=30,\n",
    "                            repetition_penalty=1.2,\n",
    "                            use_cache=True #kv cache\n",
    "                           )\n",
    "\n",
    "# 결과 출력\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "print(response)\n",
    "print(f\"{time.time()-start_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d42379-512d-404f-836b-bdaa278a1129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8ccdb-d0ce-46c4-b53c-44935c8bd314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4df90-eee0-49d9-b745-0e9adad80a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
